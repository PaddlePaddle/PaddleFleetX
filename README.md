
<h1 align="center">PaddleFleetX</h1>



<p align="center">
    <br>
    <img alt="Fork" src="https://img.shields.io/github/forks/PaddlePaddle/PaddleFleetX">
    <img alt="Issues" src="https://img.shields.io/github/issues/PaddlePaddle/PaddleFleetX">
    <img alt="License" src="https://img.shields.io/github/license/PaddlePaddle/PaddleFleetX">
    <img alt="Star" src="https://img.shields.io/github/stars/PaddlePaddle/PaddleFleetX">
    <br>
<p>


<p align="center"> Fully utilize your GPU Clusters with PaddleFleetX for your model pre-training. </p>

<h2 align="center">What is it?</h2>

- **PaddleFleetX** is an out-of-the-box pre-trained model training toolkit for cloud users. It can be viewed as an extension package for `Paddle's` High-Level Distributed Training API `paddle.distributed.fleet`. 
- [中文文档](https://fleet-x.readthedocs.io/en/latest/) | [详细介绍](https://fleet-x.readthedocs.io/en/latest/paddle_fleet_rst/distributed_introduction.html)

<h2 align="center">Key Features</h2>

- **Pre-defined Models for Training**
    - define a Bert-Large or GPT-2 with one line code, which is commonly used self-supervised training model.
- **Friendly to User-defined Dataset**
    - plugin user-defined dataset and do training without much effort.
- **Distributed Training Best Practices**
    - the most efficient way to do distributed training is provided.

<h2 align="center">Quick start</h2>


- **[Hybrid Parallelism](examples/hybrid_parallelism/README.md#quick-start)**

<h2 align="center">Community</h2>

### Slack

To connect with other users and contributors, welcome to join our [Slack channel](https://fleetx.slack.com/archives/CUBPKHKMJ)

### Feedback

For any feedback or to report a bug, please propose a [GitHub Issue](https://github.com/PaddlePaddle/PaddleFleetX/issues).

### License

[Apache 2.0 License](https://github.com/PaddlePaddle/PaddleFleetX/blob/old_develop/LICENSE)
