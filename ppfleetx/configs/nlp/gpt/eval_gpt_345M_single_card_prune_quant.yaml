_base_: ./pretrain_gpt_345M_single_card.yaml

Engine:
  save_load:
    ckpt_dir: output/epoch_0_step_55000/

Model:
  module: GPTEvalModule
  hidden_dropout_prob: 0.1
  attention_probs_dropout_prob: 0.1
  use_skip_quant: True

Quantization:
  enable: True
  weight_quantize_type: 'abs_max'
  activation_quantize_type: 'moving_average_abs_max'
  weight_bits: 8
  activation_bits: 8
  quantizable_layer_type: ['Conv2D', 'Linear', 'Conv2DTranspose', 'ColumnParallelLinear', 'RowParallelLinear']
  for_tensorrt: False
  is_full_quantize: False
  onnx_format: False

Prune:
  enable: True
  criterion: l1_norm
  ratio: 0.125

Offline_Eval:
  eval_path: ./lambada_test.jsonl
  cloze_eval: True
  overlapping_eval: 32
  batch_size: 8
  max_seq_len: 1024
  logging_freq: 10
