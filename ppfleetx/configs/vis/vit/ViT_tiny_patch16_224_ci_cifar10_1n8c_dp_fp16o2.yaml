_base_: ../base.yaml

Global:
  device: gpu
  seed: 2021

Engine:
  run_mode: 'epoch'
  num_train_epochs: 1
  eval_freq: 1
  accumulate_steps: 1
  logging_freq: 10
  mix_precision:
    enable: True
    scale_loss: 32768.0
    custom_black_list: ["reduce_sum", "elementwise_div"]
    custom_white_list: []
  save_load:
    save_epoch: 1
    output_dir: ./output
    ckpt_dir:

Distributed:
  dp_degree:

Model:
  module: "GeneralClsModule"
  model:
    name: "ViT_tiny_patch16_224"
    class_num: 10
    drop_rate: 0.1
  loss:
    train:
      name: 'ViTCELoss'
      epsilon: 0.0001
    eval:
      name: 'CELoss'
  metric:
    train:
      name: 'TopkAcc'
      topk: [1, 5]
    eval:
      name: 'TopkAcc'
      topk: [1, 5]

Optimizer:
  name: AdamW
  weight_decay: 0.3
  beta1: 0.9
  beta2: 0.999
  epsilon: 1.0e-8
  lr:
    name: ViTLRScheduler
    learning_rate: 0.003
    decay_type: cosine
    warmup_steps: 10000
  grad_clip:
    name: "ClipGradByGlobalNorm"
    clip_norm: 1.0

Data:
  Train:
    dataset:
      name: CIFAR10
      root: ./dataset/cifar-10-batches-py/
      mode: train
      transform_ops:
        - RandCropImage:
            size: 224
            scale: [0.05, 1.0]
            interpolation: bicubic
            backend: pil
        - RandFlipImage:
            flip_code: 1
        - NormalizeImage:
            scale: 1.0/255.0
            mean: [0.5, 0.5, 0.5]
            std: [0.5, 0.5, 0.5]
            order: ''
        - ToCHWImage:

    sampler:
      name: DistributedBatchSampler
      batch_size: 256
      drop_last: True
      shuffle: True
    loader:
      num_workers: 8
      use_shared_memory: True

  Eval:
    dataset: 
      name: CIFAR10
      root: ./dataset/cifar-10-batches-py/
      mode: test
      transform_ops:
        - ResizeImage:
            resize_short: 256
            interpolation: bicubic
            backend: pil
        - CenterCropImage:
            size: 224
        - NormalizeImage:
            scale: 1.0/255.0
            mean: [0.5, 0.5, 0.5]
            std: [0.5, 0.5, 0.5]
            order: ''
        - ToCHWImage:
        
    sampler:
      name: DistributedBatchSampler
      batch_size: 256
      drop_last: False
      shuffle: False
    loader:
      num_workers: 8
      use_shared_memory: True
