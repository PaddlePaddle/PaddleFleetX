{ // block 0
    var tokens : LOD_TENSOR.shape(-1, 512).dtype(int64).stop_gradient(True)
    var position_ids : LOD_TENSOR.shape(-1, 512).dtype(int64).stop_gradient(True)
    var attention_mask : LOD_TENSOR.shape(-1, 1, 512, 512).dtype(float32).stop_gradient(True)
    var labels : LOD_TENSOR.shape(-1, 512).dtype(int64).stop_gradient(True)
    var loss_mask : LOD_TENSOR.shape(-1, 512).dtype(float32).stop_gradient(True)
    persist var create_py_reader_0 : READER)
    persist var double_buffer_0 : READER)
    persist trainable param word_embeddings : LOD_TENSOR.shape(50304, 768).dtype(float32).stop_gradient(False)
    persist trainable param pos_embeddings : LOD_TENSOR.shape(1024, 768).dtype(float32).stop_gradient(False)
    persist trainable param linear_0.w_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist trainable param linear_0.b_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist trainable param linear_1.w_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist trainable param linear_1.b_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist trainable param linear_2.w_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist trainable param linear_2.b_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist trainable param linear_3.w_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist trainable param linear_3.b_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist trainable param linear_4.w_0 : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    persist trainable param linear_4.b_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist trainable param linear_5.w_0 : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    persist trainable param linear_5.b_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist trainable param layer_norm_0.w_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist trainable param layer_norm_0.b_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist trainable param layer_norm_1.w_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist trainable param layer_norm_1.b_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist trainable param linear_6.w_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist trainable param linear_6.b_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist trainable param linear_7.w_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist trainable param linear_7.b_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist trainable param linear_8.w_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist trainable param linear_8.b_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist trainable param linear_9.w_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist trainable param linear_9.b_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist trainable param linear_10.w_0 : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    persist trainable param linear_10.b_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist trainable param linear_11.w_0 : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    persist trainable param linear_11.b_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist trainable param layer_norm_2.w_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist trainable param layer_norm_2.b_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist trainable param layer_norm_3.w_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist trainable param layer_norm_3.b_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist trainable param layer_norm_4.w_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist trainable param layer_norm_4.b_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist trainable param output_embeddings : LOD_TENSOR.shape(50304, 768).dtype(float32).stop_gradient(False)
    var embedding_3.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var embedding_4.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var layer_norm_5.tmp_0 : LOD_TENSOR.shape(-512,).dtype(float32).stop_gradient(True)
    var layer_norm_5.tmp_1 : LOD_TENSOR.shape(-512,).dtype(float32).stop_gradient(True)
    var layer_norm_5.tmp_2 : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var linear_12.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var linear_12.tmp_1 : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var reshape2_0.tmp_0 : LOD_TENSOR.shape(-1, 512, 12, 64).dtype(float32).stop_gradient(False)
    var reshape2_0.tmp_1 : LOD_TENSOR.shape(0, -1, 512, 768).dtype(float32).stop_gradient(False)
    var transpose_0.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float32).stop_gradient(False)
    var transpose_0.tmp_1 : LOD_TENSOR.shape(0, -1, 512, 12, 64).dtype(float32).stop_gradient(False)
    var linear_13.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var linear_13.tmp_1 : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var linear_14.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var linear_14.tmp_1 : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var reshape2_1.tmp_0 : LOD_TENSOR.shape(-1, 512, 12, 64).dtype(float32).stop_gradient(False)
    var reshape2_1.tmp_1 : LOD_TENSOR.shape(0, -1, 512, 768).dtype(float32).stop_gradient(False)
    var transpose_1.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float32).stop_gradient(False)
    var transpose_1.tmp_1 : LOD_TENSOR.shape(0, -1, 512, 12, 64).dtype(float32).stop_gradient(False)
    var reshape2_2.tmp_0 : LOD_TENSOR.shape(-1, 512, 12, 64).dtype(float32).stop_gradient(False)
    var reshape2_2.tmp_1 : LOD_TENSOR.shape(0, -1, 512, 768).dtype(float32).stop_gradient(False)
    var transpose_2.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float32).stop_gradient(False)
    var transpose_2.tmp_1 : LOD_TENSOR.shape(0, -1, 512, 12, 64).dtype(float32).stop_gradient(False)
    var matmul_0.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float32).stop_gradient(False)
    var tmp_1 : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float32).stop_gradient(False)
    var softmax_0.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float32).stop_gradient(False)
    var matmul_v2_0.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float32).stop_gradient(False)
    var transpose_3.tmp_0 : LOD_TENSOR.shape(-1, 512, 12, 64).dtype(float32).stop_gradient(False)
    var transpose_3.tmp_1 : LOD_TENSOR.shape(0, -1, 12, 512, 64).dtype(float32).stop_gradient(False)
    var reshape2_3.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var reshape2_3.tmp_1 : LOD_TENSOR.shape(0, -1, 512, 12, 64).dtype(float32).stop_gradient(False)
    var linear_15.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var linear_15.tmp_1 : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var tmp_2 : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var layer_norm_6.tmp_0 : LOD_TENSOR.shape(-512,).dtype(float32).stop_gradient(True)
    var layer_norm_6.tmp_1 : LOD_TENSOR.shape(-512,).dtype(float32).stop_gradient(True)
    var layer_norm_6.tmp_2 : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var linear_16.tmp_0 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var linear_16.tmp_1 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var gelu_0.tmp_0 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var linear_17.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var linear_17.tmp_1 : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var tmp_3 : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var layer_norm_7.tmp_0 : LOD_TENSOR.shape(-512,).dtype(float32).stop_gradient(True)
    var layer_norm_7.tmp_1 : LOD_TENSOR.shape(-512,).dtype(float32).stop_gradient(True)
    var layer_norm_7.tmp_2 : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var linear_18.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var linear_18.tmp_1 : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var reshape2_4.tmp_0 : LOD_TENSOR.shape(-1, 512, 12, 64).dtype(float32).stop_gradient(False)
    var reshape2_4.tmp_1 : LOD_TENSOR.shape(0, -1, 512, 768).dtype(float32).stop_gradient(False)
    var transpose_4.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float32).stop_gradient(False)
    var transpose_4.tmp_1 : LOD_TENSOR.shape(0, -1, 512, 12, 64).dtype(float32).stop_gradient(False)
    var linear_19.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var linear_19.tmp_1 : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var linear_20.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var linear_20.tmp_1 : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var reshape2_5.tmp_0 : LOD_TENSOR.shape(-1, 512, 12, 64).dtype(float32).stop_gradient(False)
    var reshape2_5.tmp_1 : LOD_TENSOR.shape(0, -1, 512, 768).dtype(float32).stop_gradient(False)
    var transpose_5.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float32).stop_gradient(False)
    var transpose_5.tmp_1 : LOD_TENSOR.shape(0, -1, 512, 12, 64).dtype(float32).stop_gradient(False)
    var reshape2_6.tmp_0 : LOD_TENSOR.shape(-1, 512, 12, 64).dtype(float32).stop_gradient(False)
    var reshape2_6.tmp_1 : LOD_TENSOR.shape(0, -1, 512, 768).dtype(float32).stop_gradient(False)
    var transpose_6.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float32).stop_gradient(False)
    var transpose_6.tmp_1 : LOD_TENSOR.shape(0, -1, 512, 12, 64).dtype(float32).stop_gradient(False)
    var matmul_1.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float32).stop_gradient(False)
    var tmp_4 : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float32).stop_gradient(False)
    var softmax_1.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float32).stop_gradient(False)
    var matmul_v2_1.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float32).stop_gradient(False)
    var transpose_7.tmp_0 : LOD_TENSOR.shape(-1, 512, 12, 64).dtype(float32).stop_gradient(False)
    var transpose_7.tmp_1 : LOD_TENSOR.shape(0, -1, 12, 512, 64).dtype(float32).stop_gradient(False)
    var reshape2_7.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var reshape2_7.tmp_1 : LOD_TENSOR.shape(0, -1, 512, 12, 64).dtype(float32).stop_gradient(False)
    var linear_21.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var linear_21.tmp_1 : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var tmp_5 : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var layer_norm_8.tmp_0 : LOD_TENSOR.shape(-512,).dtype(float32).stop_gradient(True)
    var layer_norm_8.tmp_1 : LOD_TENSOR.shape(-512,).dtype(float32).stop_gradient(True)
    var layer_norm_8.tmp_2 : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var linear_22.tmp_0 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var linear_22.tmp_1 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var gelu_1.tmp_0 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var linear_23.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var linear_23.tmp_1 : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var tmp_6 : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var layer_norm_9.tmp_0 : LOD_TENSOR.shape(-512,).dtype(float32).stop_gradient(True)
    var layer_norm_9.tmp_1 : LOD_TENSOR.shape(-512,).dtype(float32).stop_gradient(True)
    var layer_norm_9.tmp_2 : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var matmul_v2_2.tmp_0 : LOD_TENSOR.shape(-1, 512, 50304).dtype(float32).stop_gradient(False)
    var unsqueeze2_0.tmp_0 : LOD_TENSOR.shape(-1, 512, 1).dtype(int64).stop_gradient(False)
    var unsqueeze2_0.tmp_1 : LOD_TENSOR.shape(0, -1, 512).dtype(int64).stop_gradient(False)
    var softmax_with_cross_entropy_0.tmp_0 : LOD_TENSOR.shape(-1, 512, 50304).dtype(float32).stop_gradient(False)
    var softmax_with_cross_entropy_0.tmp_1 : LOD_TENSOR.shape(-1, 512, 1).dtype(float32).stop_gradient(False)
    var reshape2_8.tmp_0 : LOD_TENSOR.shape(-1,).dtype(float32).stop_gradient(False)
    var reshape2_8.tmp_1 : LOD_TENSOR.shape(0, -1, 512).dtype(float32).stop_gradient(False)
    var reshape2_9.tmp_0 : LOD_TENSOR.shape(-1,).dtype(float32).stop_gradient(False)
    var reshape2_9.tmp_1 : LOD_TENSOR.shape(0, -1, 512, 1).dtype(float32).stop_gradient(False)
    var tmp_7 : LOD_TENSOR.shape(-1,).dtype(float32).stop_gradient(False)
    var sum_0.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var sum_1.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var tmp_8 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var word_embeddings@GRAD : LOD_TENSOR.shape(50304, 768).dtype(float32).stop_gradient(False)
    var embedding_4.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var tmp_0@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var layer_norm_5.tmp_2@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var layer_norm_5.tmp_2@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var linear_0.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reshape2_0.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 12, 64).dtype(float32).stop_gradient(False)
    var linear_1.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var linear_1.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var linear_2.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var layer_norm_5.tmp_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var linear_13.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var linear_14.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var transpose_1.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float32).stop_gradient(False)
    var transpose_0.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float32).stop_gradient(False)
    var matmul_0.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float32).stop_gradient(False)
    var transpose_2.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float32).stop_gradient(False)
    var softmax_0.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float32).stop_gradient(False)
    var matmul_v2_0.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float32).stop_gradient(False)
    var transpose_3.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 12, 64).dtype(float32).stop_gradient(False)
    var reshape2_3.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var linear_3.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var linear_15.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var layer_norm_1.w_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var linear_4.w_0@GRAD : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    var layer_norm_6.tmp_2@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var linear_4.b_0@GRAD : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    var linear_16.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var linear_16.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var linear_5.w_0@GRAD : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    var linear_5.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var linear_17.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var linear_17.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var layer_norm_2.w_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var layer_norm_2.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reshape2_1.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 12, 64).dtype(float32).stop_gradient(False)
    var linear_6.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var linear_18.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var reshape2_4.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 12, 64).dtype(float32).stop_gradient(False)
    var layer_norm_7.tmp_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var linear_7.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var linear_19.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var layer_norm_7.tmp_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var linear_20.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var reshape2_5.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 12, 64).dtype(float32).stop_gradient(False)
    var reshape2_6.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 12, 64).dtype(float32).stop_gradient(False)
    var transpose_5.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float32).stop_gradient(False)
    var linear_18.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var transpose_4.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float32).stop_gradient(False)
    var transpose_6.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float32).stop_gradient(False)
    var softmax_1.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float32).stop_gradient(False)
    var tmp_3@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var matmul_v2_1.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float32).stop_gradient(False)
    var transpose_7.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 12, 64).dtype(float32).stop_gradient(False)
    var embedding_3.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var linear_9.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var linear_9.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var tmp_3@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var tmp_5@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var tmp_5@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var layer_norm_3.w_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var linear_14.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var linear_8.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var layer_norm_3.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var linear_8.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var linear_10.w_0@GRAD : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    var linear_22.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_1@GRAD : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float32).stop_gradient(False)
    var linear_13.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var matmul_1.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float32).stop_gradient(False)
    var linear_23.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var linear_0.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var linear_15.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var layer_norm_8.tmp_2@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var tmp_3@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var tmp_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var linear_21.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var linear_23.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var gelu_0.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var layer_norm_1.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var tmp_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var linear_6.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var linear_12.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var reshape2_2.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 12, 64).dtype(float32).stop_gradient(False)
    var linear_7.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var tmp_4@GRAD : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float32).stop_gradient(False)
    var linear_22.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_2@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var tmp_7@GRAD : LOD_TENSOR.shape(-1,).dtype(float32).stop_gradient(False)
    var linear_21.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var linear_10.b_0@GRAD : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    var layer_norm_5.tmp_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var linear_2.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reshape2_8.tmp_0@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1,).dtype(float32).stop_gradient(False)
    var linear_12.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var layer_norm_7.tmp_2@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var sum_1.tmp_0@GRAD : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var linear_20.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var output_embeddings@GRAD : LOD_TENSOR.shape(50304, 768).dtype(float32).stop_gradient(False)
    var reshape2_9.tmp_0@GRAD : LOD_TENSOR.shape(-1,).dtype(float32).stop_gradient(False)
    var layer_norm_7.tmp_2@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var linear_11.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var layer_norm_0.w_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var pos_embeddings@GRAD : LOD_TENSOR.shape(1024, 768).dtype(float32).stop_gradient(False)
    var matmul_v2_2.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 50304).dtype(float32).stop_gradient(False)
    var reshape2_7.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var layer_norm_9.tmp_2@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var linear_19.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var tmp_8@GRAD : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var sum_0.tmp_0@GRAD : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var linear_3.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var reshape2_8.tmp_0@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1,).dtype(float32).stop_gradient(False)
    var layer_norm_0.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var softmax_with_cross_entropy_0.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 1).dtype(float32).stop_gradient(False)
    var reshape2_8.tmp_0@GRAD : LOD_TENSOR.shape(-1,).dtype(float32).stop_gradient(False)
    var tmp_0@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var layer_norm_4.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var layer_norm_4.w_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var tmp_6@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var tmp_5@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var gelu_1.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var linear_11.w_0@GRAD : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    persist var layer_norm_0.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var layer_norm_0.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var layer_norm_0.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_0.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_0.w_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var layer_norm_0.w_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var layer_norm_0.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_0.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_1.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var layer_norm_1.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var layer_norm_1.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_1.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_1.w_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var layer_norm_1.w_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var layer_norm_1.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_1.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_2.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var layer_norm_2.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var layer_norm_2.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_2.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_2.w_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var layer_norm_2.w_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var layer_norm_2.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_2.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_3.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var layer_norm_3.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var layer_norm_3.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_3.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_3.w_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var layer_norm_3.w_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var layer_norm_3.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_3.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_4.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var layer_norm_4.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var layer_norm_4.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_4.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_4.w_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var layer_norm_4.w_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var layer_norm_4.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_4.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_0.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var linear_0.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var linear_0.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_0.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_0.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var linear_0.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var linear_0.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_0.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_1.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var linear_1.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var linear_1.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_1.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_1.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var linear_1.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var linear_1.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_1.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_10.b_0_moment1_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var linear_10.b_0_moment2_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var linear_10.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_10.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_10.w_0_moment1_0 : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    persist var linear_10.w_0_moment2_0 : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    persist var linear_10.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_10.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_11.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var linear_11.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var linear_11.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_11.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_11.w_0_moment1_0 : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    persist var linear_11.w_0_moment2_0 : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    persist var linear_11.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_11.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_2.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var linear_2.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var linear_2.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_2.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_2.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var linear_2.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var linear_2.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_2.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_3.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var linear_3.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var linear_3.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_3.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_3.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var linear_3.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var linear_3.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_3.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_4.b_0_moment1_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var linear_4.b_0_moment2_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var linear_4.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_4.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_4.w_0_moment1_0 : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    persist var linear_4.w_0_moment2_0 : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    persist var linear_4.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_4.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_5.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var linear_5.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var linear_5.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_5.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_5.w_0_moment1_0 : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    persist var linear_5.w_0_moment2_0 : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    persist var linear_5.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_5.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_6.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var linear_6.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var linear_6.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_6.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_6.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var linear_6.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var linear_6.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_6.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_7.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var linear_7.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var linear_7.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_7.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_7.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var linear_7.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var linear_7.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_7.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_8.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var linear_8.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var linear_8.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_8.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_8.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var linear_8.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var linear_8.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_8.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_9.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var linear_9.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var linear_9.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_9.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_9.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var linear_9.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var linear_9.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_9.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var output_embeddings_moment1_0 : LOD_TENSOR.shape(50304, 768).dtype(float32).stop_gradient(False)
    persist var output_embeddings_moment2_0 : LOD_TENSOR.shape(50304, 768).dtype(float32).stop_gradient(False)
    persist var output_embeddings_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var output_embeddings_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var pos_embeddings_moment1_0 : LOD_TENSOR.shape(1024, 768).dtype(float32).stop_gradient(False)
    persist var pos_embeddings_moment2_0 : LOD_TENSOR.shape(1024, 768).dtype(float32).stop_gradient(False)
    persist var pos_embeddings_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var pos_embeddings_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var word_embeddings_moment1_0 : LOD_TENSOR.shape(50304, 768).dtype(float32).stop_gradient(False)
    persist var word_embeddings_moment2_0 : LOD_TENSOR.shape(50304, 768).dtype(float32).stop_gradient(False)
    persist var word_embeddings_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var word_embeddings_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var learning_rate_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(True)

    {Out=['create_py_reader_0']} = create_py_reader(inputs={blocking_queue=['lod_tensor_blocking_queue_0']}, device_count = 1, device_index = 0, dtypes = [3, 5, 5, 3, 3], lod_levels = [0, 0, 0, 0, 0], need_check_feed = [1, 1, 1, 1, 1], op_device = gpu:0, op_namescope = /, op_role = 0, op_role_var = [], ranks = [2, 2, 4, 2, 2], shape_concat = [-1, 512, -1, 512, -1, 1, 512, 512, -1, 512, -1, 512], use_data_config = True, with_quant_attr = False)
    {Out=['double_buffer_0']} = create_double_buffer_reader(inputs={UnderlyingReader=['create_py_reader_0']}, op_device = gpu:0, op_namescope = /, op_role = 0, op_role_var = [], place = AUTO, with_quant_attr = False)
    {Out=['tokens', 'loss_mask', 'attention_mask', 'position_ids', 'labels']} = read(inputs={Reader=['double_buffer_0']}, drop_last = True, infer_out = True, op_device = gpu:0, op_namescope = /, op_role = 0, op_role_var = [], throw_eof_exp = True, with_quant_attr = False)
    {Out=['embedding_3.tmp_0']} = lookup_table_v2(inputs={Ids=['tokens'], W=['word_embeddings']}, epmap = [], height_sections = [], is_distributed = False, is_sparse = False, op_device = gpu:0, op_namescope = /, op_role = 0, op_role_var = [], padding_idx = -1, remote_prefetch = False, table_names = [], trainer_id = 0, with_quant_attr = False)
    {Out=['embedding_4.tmp_0']} = lookup_table_v2(inputs={Ids=['position_ids'], W=['pos_embeddings']}, epmap = [], height_sections = [], is_distributed = False, is_sparse = False, op_device = gpu:0, op_namescope = /, op_role = 0, op_role_var = [], padding_idx = -1, remote_prefetch = False, table_names = [], trainer_id = 0, with_quant_attr = False)
    {Out=['tmp_0']} = elementwise_add(inputs={X=['embedding_3.tmp_0'], Y=['embedding_4.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, with_quant_attr = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_5.tmp_0'], Variance=['layer_norm_5.tmp_1'], Y=['layer_norm_5.tmp_2']} = layer_norm(inputs={Bias=['layer_norm_0.b_0'], Scale=['layer_norm_0.w_0'], X=['tmp_0']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, with_quant_attr = False)
    {Out=['linear_12.tmp_0']} = matmul_v2(inputs={X=['layer_norm_5.tmp_2'], Y=['linear_0.w_0']}, fused_reshape_Out = [], fused_transpose_Out = [], mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, use_mkldnn = False, with_quant_attr = False)
    {Out=['linear_12.tmp_1']} = elementwise_add(inputs={X=['linear_12.tmp_0'], Y=['linear_0.b_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, with_quant_attr = False, x_data_format = , y_data_format = )
    {Out=['reshape2_0.tmp_0'], XShape=['reshape2_0.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_12.tmp_1']}, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_mkldnn = False, use_quantizer = False, with_quant_attr = False)
    {Out=['transpose_0.tmp_0'], XShape=['transpose_0.tmp_1']} = transpose2(inputs={X=['reshape2_0.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, with_quant_attr = False)
    {Out=['linear_13.tmp_0']} = matmul_v2(inputs={X=['layer_norm_5.tmp_2'], Y=['linear_1.w_0']}, fused_reshape_Out = [], fused_transpose_Out = [], mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, use_mkldnn = False, with_quant_attr = False)
    {Out=['linear_13.tmp_1']} = elementwise_add(inputs={X=['linear_13.tmp_0'], Y=['linear_1.b_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, with_quant_attr = False, x_data_format = , y_data_format = )
    {Out=['linear_14.tmp_0']} = matmul_v2(inputs={X=['layer_norm_5.tmp_2'], Y=['linear_2.w_0']}, fused_reshape_Out = [], fused_transpose_Out = [], mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, use_mkldnn = False, with_quant_attr = False)
    {Out=['linear_14.tmp_1']} = elementwise_add(inputs={X=['linear_14.tmp_0'], Y=['linear_2.b_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, with_quant_attr = False, x_data_format = , y_data_format = )
    {Out=['reshape2_1.tmp_0'], XShape=['reshape2_1.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_13.tmp_1']}, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_mkldnn = False, use_quantizer = False, with_quant_attr = False)
    {Out=['transpose_1.tmp_0'], XShape=['transpose_1.tmp_1']} = transpose2(inputs={X=['reshape2_1.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, with_quant_attr = False)
    {Out=['reshape2_2.tmp_0'], XShape=['reshape2_2.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_14.tmp_1']}, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_mkldnn = False, use_quantizer = False, with_quant_attr = False)
    {Out=['transpose_2.tmp_0'], XShape=['transpose_2.tmp_1']} = transpose2(inputs={X=['reshape2_2.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, with_quant_attr = False)
    {Out=['matmul_0.tmp_0']} = matmul(inputs={X=['transpose_0.tmp_0'], Y=['transpose_1.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 0.125, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = True, use_mkldnn = False, use_quantizer = False, with_quant_attr = False)
    {Out=['tmp_1']} = elementwise_add(inputs={X=['matmul_0.tmp_0'], Y=['attention_mask']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, with_quant_attr = False, x_data_format = , y_data_format = )
    {Out=['softmax_0.tmp_0']} = softmax(inputs={X=['tmp_1']}, axis = -1, data_format = AnyLayout, is_test = False, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = True, use_mkldnn = False, with_quant_attr = False)
    {Out=['matmul_v2_0.tmp_0']} = matmul_v2(inputs={X=['softmax_0.tmp_0'], Y=['transpose_2.tmp_0']}, fused_reshape_Out = [], fused_transpose_Out = [], mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, use_mkldnn = False, with_quant_attr = False)
    {Out=['transpose_3.tmp_0'], XShape=['transpose_3.tmp_1']} = transpose2(inputs={X=['matmul_v2_0.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, with_quant_attr = False)
    {Out=['reshape2_3.tmp_0'], XShape=['reshape2_3.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_3.tmp_0']}, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 768], use_mkldnn = False, use_quantizer = False, with_quant_attr = False)
    {Out=['linear_15.tmp_0']} = matmul_v2(inputs={X=['reshape2_3.tmp_0'], Y=['linear_3.w_0']}, fused_reshape_Out = [], fused_transpose_Out = [], mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, use_mkldnn = False, with_quant_attr = False)
    {Out=['linear_15.tmp_1']} = elementwise_add(inputs={X=['linear_15.tmp_0'], Y=['linear_3.b_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, with_quant_attr = False, x_data_format = , y_data_format = )
    {Out=['tmp_2']} = elementwise_add(inputs={X=['tmp_0'], Y=['linear_15.tmp_1']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, with_quant_attr = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_6.tmp_0'], Variance=['layer_norm_6.tmp_1'], Y=['layer_norm_6.tmp_2']} = layer_norm(inputs={Bias=['layer_norm_1.b_0'], Scale=['layer_norm_1.w_0'], X=['tmp_2']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, with_quant_attr = False)
    {Out=['linear_16.tmp_0']} = matmul_v2(inputs={X=['layer_norm_6.tmp_2'], Y=['linear_4.w_0']}, fused_reshape_Out = [], fused_transpose_Out = [], mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, use_mkldnn = False, with_quant_attr = False)
    {Out=['linear_16.tmp_1']} = elementwise_add(inputs={X=['linear_16.tmp_0'], Y=['linear_4.b_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, with_quant_attr = False, x_data_format = , y_data_format = )
    {Out=['gelu_0.tmp_0']} = gelu(inputs={X=['linear_16.tmp_1']}, approximate = True, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = False, use_mkldnn = False, with_quant_attr = False)
    {Out=['linear_17.tmp_0']} = matmul_v2(inputs={X=['gelu_0.tmp_0'], Y=['linear_5.w_0']}, fused_reshape_Out = [], fused_transpose_Out = [], mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, use_mkldnn = False, with_quant_attr = False)
    {Out=['linear_17.tmp_1']} = elementwise_add(inputs={X=['linear_17.tmp_0'], Y=['linear_5.b_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, with_quant_attr = False, x_data_format = , y_data_format = )
    {Out=['tmp_3']} = elementwise_add(inputs={X=['tmp_2'], Y=['linear_17.tmp_1']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, with_quant_attr = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_7.tmp_0'], Variance=['layer_norm_7.tmp_1'], Y=['layer_norm_7.tmp_2']} = layer_norm(inputs={Bias=['layer_norm_2.b_0'], Scale=['layer_norm_2.w_0'], X=['tmp_3']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, with_quant_attr = False)
    {Out=['linear_18.tmp_0']} = matmul_v2(inputs={X=['layer_norm_7.tmp_2'], Y=['linear_6.w_0']}, fused_reshape_Out = [], fused_transpose_Out = [], mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, use_mkldnn = False, with_quant_attr = False)
    {Out=['linear_18.tmp_1']} = elementwise_add(inputs={X=['linear_18.tmp_0'], Y=['linear_6.b_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, with_quant_attr = False, x_data_format = , y_data_format = )
    {Out=['reshape2_4.tmp_0'], XShape=['reshape2_4.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_18.tmp_1']}, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_mkldnn = False, use_quantizer = False, with_quant_attr = False)
    {Out=['transpose_4.tmp_0'], XShape=['transpose_4.tmp_1']} = transpose2(inputs={X=['reshape2_4.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, with_quant_attr = False)
    {Out=['linear_19.tmp_0']} = matmul_v2(inputs={X=['layer_norm_7.tmp_2'], Y=['linear_7.w_0']}, fused_reshape_Out = [], fused_transpose_Out = [], mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, use_mkldnn = False, with_quant_attr = False)
    {Out=['linear_19.tmp_1']} = elementwise_add(inputs={X=['linear_19.tmp_0'], Y=['linear_7.b_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, with_quant_attr = False, x_data_format = , y_data_format = )
    {Out=['linear_20.tmp_0']} = matmul_v2(inputs={X=['layer_norm_7.tmp_2'], Y=['linear_8.w_0']}, fused_reshape_Out = [], fused_transpose_Out = [], mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, use_mkldnn = False, with_quant_attr = False)
    {Out=['linear_20.tmp_1']} = elementwise_add(inputs={X=['linear_20.tmp_0'], Y=['linear_8.b_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, with_quant_attr = False, x_data_format = , y_data_format = )
    {Out=['reshape2_5.tmp_0'], XShape=['reshape2_5.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_19.tmp_1']}, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_mkldnn = False, use_quantizer = False, with_quant_attr = False)
    {Out=['transpose_5.tmp_0'], XShape=['transpose_5.tmp_1']} = transpose2(inputs={X=['reshape2_5.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, with_quant_attr = False)
    {Out=['reshape2_6.tmp_0'], XShape=['reshape2_6.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_20.tmp_1']}, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_mkldnn = False, use_quantizer = False, with_quant_attr = False)
    {Out=['transpose_6.tmp_0'], XShape=['transpose_6.tmp_1']} = transpose2(inputs={X=['reshape2_6.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, with_quant_attr = False)
    {Out=['matmul_1.tmp_0']} = matmul(inputs={X=['transpose_4.tmp_0'], Y=['transpose_5.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 0.125, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = True, use_mkldnn = False, use_quantizer = False, with_quant_attr = False)
    {Out=['tmp_4']} = elementwise_add(inputs={X=['matmul_1.tmp_0'], Y=['attention_mask']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, with_quant_attr = False, x_data_format = , y_data_format = )
    {Out=['softmax_1.tmp_0']} = softmax(inputs={X=['tmp_4']}, axis = -1, data_format = AnyLayout, is_test = False, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = True, use_mkldnn = False, with_quant_attr = False)
    {Out=['matmul_v2_1.tmp_0']} = matmul_v2(inputs={X=['softmax_1.tmp_0'], Y=['transpose_6.tmp_0']}, fused_reshape_Out = [], fused_transpose_Out = [], mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, use_mkldnn = False, with_quant_attr = False)
    {Out=['transpose_7.tmp_0'], XShape=['transpose_7.tmp_1']} = transpose2(inputs={X=['matmul_v2_1.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, with_quant_attr = False)
    {Out=['reshape2_7.tmp_0'], XShape=['reshape2_7.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_7.tmp_0']}, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 768], use_mkldnn = False, use_quantizer = False, with_quant_attr = False)
    {Out=['linear_21.tmp_0']} = matmul_v2(inputs={X=['reshape2_7.tmp_0'], Y=['linear_9.w_0']}, fused_reshape_Out = [], fused_transpose_Out = [], mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, use_mkldnn = False, with_quant_attr = False)
    {Out=['linear_21.tmp_1']} = elementwise_add(inputs={X=['linear_21.tmp_0'], Y=['linear_9.b_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, with_quant_attr = False, x_data_format = , y_data_format = )
    {Out=['tmp_5']} = elementwise_add(inputs={X=['tmp_3'], Y=['linear_21.tmp_1']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, with_quant_attr = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_8.tmp_0'], Variance=['layer_norm_8.tmp_1'], Y=['layer_norm_8.tmp_2']} = layer_norm(inputs={Bias=['layer_norm_3.b_0'], Scale=['layer_norm_3.w_0'], X=['tmp_5']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, with_quant_attr = False)
    {Out=['linear_22.tmp_0']} = matmul_v2(inputs={X=['layer_norm_8.tmp_2'], Y=['linear_10.w_0']}, fused_reshape_Out = [], fused_transpose_Out = [], mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, use_mkldnn = False, with_quant_attr = False)
    {Out=['linear_22.tmp_1']} = elementwise_add(inputs={X=['linear_22.tmp_0'], Y=['linear_10.b_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, with_quant_attr = False, x_data_format = , y_data_format = )
    {Out=['gelu_1.tmp_0']} = gelu(inputs={X=['linear_22.tmp_1']}, approximate = True, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = False, use_mkldnn = False, with_quant_attr = False)
    {Out=['linear_23.tmp_0']} = matmul_v2(inputs={X=['gelu_1.tmp_0'], Y=['linear_11.w_0']}, fused_reshape_Out = [], fused_transpose_Out = [], mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, use_mkldnn = False, with_quant_attr = False)
    {Out=['linear_23.tmp_1']} = elementwise_add(inputs={X=['linear_23.tmp_0'], Y=['linear_11.b_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, with_quant_attr = False, x_data_format = , y_data_format = )
    {Out=['tmp_6']} = elementwise_add(inputs={X=['tmp_5'], Y=['linear_23.tmp_1']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, with_quant_attr = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_9.tmp_0'], Variance=['layer_norm_9.tmp_1'], Y=['layer_norm_9.tmp_2']} = layer_norm(inputs={Bias=['layer_norm_4.b_0'], Scale=['layer_norm_4.w_0'], X=['tmp_6']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, with_quant_attr = False)
    {Out=['matmul_v2_2.tmp_0']} = matmul_v2(inputs={X=['layer_norm_9.tmp_2'], Y=['output_embeddings']}, fused_reshape_Out = [], fused_transpose_Out = [], mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 0, op_role_var = [], trans_x = False, trans_y = True, use_mkldnn = False, with_quant_attr = False)
    {Out=['unsqueeze2_0.tmp_0'], XShape=['unsqueeze2_0.tmp_1']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['labels']}, axes = [2], op_device = gpu:0, op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Loss=['softmax_with_cross_entropy_0.tmp_1'], Softmax=['softmax_with_cross_entropy_0.tmp_0']} = softmax_with_cross_entropy(inputs={Label=['unsqueeze2_0.tmp_0'], Logits=['matmul_v2_2.tmp_0']}, axis = -1, ignore_index = -100, numeric_stable_mode = True, op_device = gpu:0, op_namescope = /, op_role = 0, op_role_var = [], soft_label = False, use_softmax = True, with_quant_attr = False)
    {Out=['reshape2_8.tmp_0'], XShape=['reshape2_8.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['loss_mask']}, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 0, op_role_var = [], shape = [-1], use_mkldnn = False, use_quantizer = False, with_quant_attr = False)
    {Out=['reshape2_9.tmp_0'], XShape=['reshape2_9.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['softmax_with_cross_entropy_0.tmp_1']}, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 0, op_role_var = [], shape = [-1], use_mkldnn = False, use_quantizer = False, with_quant_attr = False)
    {Out=['tmp_7']} = elementwise_mul(inputs={X=['reshape2_9.tmp_0'], Y=['reshape2_8.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, with_quant_attr = False, x_data_format = , y_data_format = )
    {Out=['sum_0.tmp_0']} = reduce_sum(inputs={X=['tmp_7']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = gpu:0, op_namescope = /, op_role = 0, op_role_var = [], out_dtype = -1, reduce_all = True, use_mkldnn = False, with_quant_attr = False)
    {Out=['sum_1.tmp_0']} = reduce_sum(inputs={X=['reshape2_8.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = gpu:0, op_namescope = /, op_role = 0, op_role_var = [], out_dtype = -1, reduce_all = True, use_mkldnn = False, with_quant_attr = False)
    {Out=['tmp_8']} = elementwise_div(inputs={X=['sum_0.tmp_0'], Y=['sum_1.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 256, op_role_var = [], use_mkldnn = False, use_quantizer = False, with_quant_attr = False, x_data_format = , y_data_format = )
    {Out=['tmp_8@GRAD']} = fill_constant(inputs={}, dtype = 5, force_cpu = False, op_device = gpu:0, op_role = 257, shape = [1], value = 1.0)
    {X@GRAD=['sum_0.tmp_0@GRAD'], Y@GRAD=['sum_1.tmp_0@GRAD']} = elementwise_div_grad(inputs={Out=['tmp_8'], Out@GRAD=['tmp_8@GRAD'], X=['sum_0.tmp_0'], Y=['sum_1.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, with_quant_attr = False, x_data_format = , y_data_format = )
    {X@GRAD=['reshape2_8.tmp_0@GRAD@RENAME@block0@0']} = reduce_sum_grad(inputs={Out@GRAD=['sum_1.tmp_0@GRAD'], X=['reshape2_8.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = gpu:0, op_namescope = /, op_role = 1, op_role_var = [], out_dtype = -1, reduce_all = True, use_mkldnn = False, with_quant_attr = False)
    {X@GRAD=['tmp_7@GRAD']} = reduce_sum_grad(inputs={Out@GRAD=['sum_0.tmp_0@GRAD'], X=['tmp_7']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = gpu:0, op_namescope = /, op_role = 1, op_role_var = [], out_dtype = -1, reduce_all = True, use_mkldnn = False, with_quant_attr = False)
    {X@GRAD=['reshape2_9.tmp_0@GRAD'], Y@GRAD=['reshape2_8.tmp_0@GRAD@RENAME@block0@1']} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_7@GRAD'], X=['reshape2_9.tmp_0'], Y=['reshape2_8.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, with_quant_attr = False, x_data_format = , y_data_format = )
    {X@GRAD=['softmax_with_cross_entropy_0.tmp_1@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_9.tmp_0@GRAD'], XShape=['reshape2_9.tmp_1']}, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 1, op_role_var = [], shape = [-1], use_mkldnn = False, use_quantizer = False, with_quant_attr = False)
    {Out=['reshape2_8.tmp_0@GRAD']} = sum(inputs={X=['reshape2_8.tmp_0@GRAD@RENAME@block0@0', 'reshape2_8.tmp_0@GRAD@RENAME@block0@1']}, op_device = gpu:0, op_role = 1, use_mkldnn = False)
    {Logits@GRAD=['matmul_v2_2.tmp_0@GRAD']} = softmax_with_cross_entropy_grad(inputs={Label=['unsqueeze2_0.tmp_0'], Loss@GRAD=['softmax_with_cross_entropy_0.tmp_1@GRAD'], Softmax=['softmax_with_cross_entropy_0.tmp_0']}, axis = -1, ignore_index = -100, numeric_stable_mode = True, op_device = gpu:0, op_namescope = /, op_role = 1, op_role_var = [], soft_label = False, use_softmax = True, with_quant_attr = False)
    {X@GRAD=['layer_norm_9.tmp_2@GRAD'], Y@GRAD=['output_embeddings@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_2.tmp_0@GRAD'], X=['layer_norm_9.tmp_2'], Y=['output_embeddings']}, fused_reshape_Out = [], fused_transpose_Out = [], mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 1, op_role_var = ['output_embeddings', 'output_embeddings@GRAD'], trans_x = False, trans_y = True, use_mkldnn = False, with_quant_attr = False)
    {Bias@GRAD=['layer_norm_4.b_0@GRAD'], Scale@GRAD=['layer_norm_4.w_0@GRAD'], X@GRAD=['tmp_6@GRAD']} = layer_norm_grad(inputs={Bias=['layer_norm_4.b_0'], Mean=['layer_norm_9.tmp_0'], Scale=['layer_norm_4.w_0'], Variance=['layer_norm_9.tmp_1'], X=['tmp_6'], Y@GRAD=['layer_norm_9.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 1, op_role_var = ['layer_norm_4.b_0', 'layer_norm_4.b_0@GRAD', 'layer_norm_4.w_0', 'layer_norm_4.w_0@GRAD'], use_mkldnn = False, with_quant_attr = False)
    {X@GRAD=['tmp_5@GRAD@RENAME@block0@0'], Y@GRAD=['linear_23.tmp_1@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_6@GRAD'], X=['tmp_5'], Y=['linear_23.tmp_1']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, with_quant_attr = False, x_data_format = , y_data_format = )
    {X@GRAD=['linear_23.tmp_0@GRAD'], Y@GRAD=['linear_11.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_23.tmp_1@GRAD'], X=['linear_23.tmp_0'], Y=['linear_11.b_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 1, op_role_var = ['linear_11.b_0', 'linear_11.b_0@GRAD'], use_mkldnn = False, use_quantizer = False, with_quant_attr = False, x_data_format = , y_data_format = )
    {X@GRAD=['gelu_1.tmp_0@GRAD'], Y@GRAD=['linear_11.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_23.tmp_0@GRAD'], X=['gelu_1.tmp_0'], Y=['linear_11.w_0']}, fused_reshape_Out = [], fused_transpose_Out = [], mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 1, op_role_var = ['linear_11.w_0', 'linear_11.w_0@GRAD'], trans_x = False, trans_y = False, use_mkldnn = False, with_quant_attr = False)
    {X@GRAD=['linear_22.tmp_1@GRAD']} = gelu_grad(inputs={Out@GRAD=['gelu_1.tmp_0@GRAD'], X=['linear_22.tmp_1']}, approximate = True, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = False, use_mkldnn = False, with_quant_attr = False)
    {X@GRAD=['linear_22.tmp_0@GRAD'], Y@GRAD=['linear_10.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_22.tmp_1@GRAD'], X=['linear_22.tmp_0'], Y=['linear_10.b_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 1, op_role_var = ['linear_10.b_0', 'linear_10.b_0@GRAD'], use_mkldnn = False, use_quantizer = False, with_quant_attr = False, x_data_format = , y_data_format = )
    {X@GRAD=['layer_norm_8.tmp_2@GRAD'], Y@GRAD=['linear_10.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_22.tmp_0@GRAD'], X=['layer_norm_8.tmp_2'], Y=['linear_10.w_0']}, fused_reshape_Out = [], fused_transpose_Out = [], mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 1, op_role_var = ['linear_10.w_0', 'linear_10.w_0@GRAD'], trans_x = False, trans_y = False, use_mkldnn = False, with_quant_attr = False)
    {Bias@GRAD=['layer_norm_3.b_0@GRAD'], Scale@GRAD=['layer_norm_3.w_0@GRAD'], X@GRAD=['tmp_5@GRAD@RENAME@block0@1']} = layer_norm_grad(inputs={Bias=['layer_norm_3.b_0'], Mean=['layer_norm_8.tmp_0'], Scale=['layer_norm_3.w_0'], Variance=['layer_norm_8.tmp_1'], X=['tmp_5'], Y@GRAD=['layer_norm_8.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 1, op_role_var = ['layer_norm_3.b_0', 'layer_norm_3.b_0@GRAD', 'layer_norm_3.w_0', 'layer_norm_3.w_0@GRAD'], use_mkldnn = False, with_quant_attr = False)
    {Out=['tmp_5@GRAD']} = sum(inputs={X=['tmp_5@GRAD@RENAME@block0@0', 'tmp_5@GRAD@RENAME@block0@1']}, op_device = gpu:0, op_role = 1, use_mkldnn = False)
    {X@GRAD=['tmp_3@GRAD@RENAME@block0@0'], Y@GRAD=['linear_21.tmp_1@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_5@GRAD'], X=['tmp_3'], Y=['linear_21.tmp_1']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, with_quant_attr = False, x_data_format = , y_data_format = )
    {X@GRAD=['linear_21.tmp_0@GRAD'], Y@GRAD=['linear_9.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_21.tmp_1@GRAD'], X=['linear_21.tmp_0'], Y=['linear_9.b_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 1, op_role_var = ['linear_9.b_0', 'linear_9.b_0@GRAD'], use_mkldnn = False, use_quantizer = False, with_quant_attr = False, x_data_format = , y_data_format = )
    {X@GRAD=['reshape2_7.tmp_0@GRAD'], Y@GRAD=['linear_9.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_21.tmp_0@GRAD'], X=['reshape2_7.tmp_0'], Y=['linear_9.w_0']}, fused_reshape_Out = [], fused_transpose_Out = [], mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 1, op_role_var = ['linear_9.w_0', 'linear_9.w_0@GRAD'], trans_x = False, trans_y = False, use_mkldnn = False, with_quant_attr = False)
    {X@GRAD=['transpose_7.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_7.tmp_0@GRAD'], XShape=['reshape2_7.tmp_1']}, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 768], use_mkldnn = False, use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['matmul_v2_1.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_7.tmp_0@GRAD'], XShape=['transpose_7.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['softmax_1.tmp_0@GRAD'], Y@GRAD=['transpose_6.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_1.tmp_0@GRAD'], X=['softmax_1.tmp_0'], Y=['transpose_6.tmp_0']}, fused_reshape_Out = [], fused_transpose_Out = [], mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, use_mkldnn = False, with_quant_attr = False)
    {X@GRAD=['tmp_4@GRAD']} = softmax_grad(inputs={Out=['softmax_1.tmp_0'], Out@GRAD=['softmax_1.tmp_0@GRAD']}, axis = -1, data_format = AnyLayout, is_test = False, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = True, use_mkldnn = False, with_quant_attr = False)
    {X@GRAD=['matmul_1.tmp_0@GRAD'], Y@GRAD=[]} = elementwise_add_grad(inputs={Out@GRAD=['tmp_4@GRAD'], X=['matmul_1.tmp_0'], Y=['attention_mask']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, with_quant_attr = False, x_data_format = , y_data_format = )
    {X@GRAD=['transpose_4.tmp_0@GRAD'], Y@GRAD=['transpose_5.tmp_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['matmul_1.tmp_0@GRAD'], X=['transpose_4.tmp_0'], Y=['transpose_5.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 0.125, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = True, use_mkldnn = False, use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['reshape2_6.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_6.tmp_0@GRAD'], XShape=['transpose_6.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['linear_20.tmp_1@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_6.tmp_0@GRAD'], XShape=['reshape2_6.tmp_1']}, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_mkldnn = False, use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['reshape2_5.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_5.tmp_0@GRAD'], XShape=['transpose_5.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['linear_19.tmp_1@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_5.tmp_0@GRAD'], XShape=['reshape2_5.tmp_1']}, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_mkldnn = False, use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['linear_20.tmp_0@GRAD'], Y@GRAD=['linear_8.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_20.tmp_1@GRAD'], X=['linear_20.tmp_0'], Y=['linear_8.b_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 1, op_role_var = ['linear_8.b_0', 'linear_8.b_0@GRAD'], use_mkldnn = False, use_quantizer = False, with_quant_attr = False, x_data_format = , y_data_format = )
    {X@GRAD=['layer_norm_7.tmp_2@GRAD@RENAME@block0@0'], Y@GRAD=['linear_8.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_20.tmp_0@GRAD'], X=['layer_norm_7.tmp_2'], Y=['linear_8.w_0']}, fused_reshape_Out = [], fused_transpose_Out = [], mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 1, op_role_var = ['linear_8.w_0', 'linear_8.w_0@GRAD'], trans_x = False, trans_y = False, use_mkldnn = False, with_quant_attr = False)
    {X@GRAD=['linear_19.tmp_0@GRAD'], Y@GRAD=['linear_7.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_19.tmp_1@GRAD'], X=['linear_19.tmp_0'], Y=['linear_7.b_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 1, op_role_var = ['linear_7.b_0', 'linear_7.b_0@GRAD'], use_mkldnn = False, use_quantizer = False, with_quant_attr = False, x_data_format = , y_data_format = )
    {X@GRAD=['layer_norm_7.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['linear_7.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_19.tmp_0@GRAD'], X=['layer_norm_7.tmp_2'], Y=['linear_7.w_0']}, fused_reshape_Out = [], fused_transpose_Out = [], mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 1, op_role_var = ['linear_7.w_0', 'linear_7.w_0@GRAD'], trans_x = False, trans_y = False, use_mkldnn = False, with_quant_attr = False)
    {X@GRAD=['reshape2_4.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_4.tmp_0@GRAD'], XShape=['transpose_4.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['linear_18.tmp_1@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_4.tmp_0@GRAD'], XShape=['reshape2_4.tmp_1']}, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_mkldnn = False, use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['linear_18.tmp_0@GRAD'], Y@GRAD=['linear_6.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_18.tmp_1@GRAD'], X=['linear_18.tmp_0'], Y=['linear_6.b_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 1, op_role_var = ['linear_6.b_0', 'linear_6.b_0@GRAD'], use_mkldnn = False, use_quantizer = False, with_quant_attr = False, x_data_format = , y_data_format = )
    {X@GRAD=['layer_norm_7.tmp_2@GRAD@RENAME@block0@2'], Y@GRAD=['linear_6.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_18.tmp_0@GRAD'], X=['layer_norm_7.tmp_2'], Y=['linear_6.w_0']}, fused_reshape_Out = [], fused_transpose_Out = [], mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 1, op_role_var = ['linear_6.w_0', 'linear_6.w_0@GRAD'], trans_x = False, trans_y = False, use_mkldnn = False, with_quant_attr = False)
    {Out=['layer_norm_7.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_7.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_7.tmp_2@GRAD@RENAME@block0@1', 'layer_norm_7.tmp_2@GRAD@RENAME@block0@2']}, op_device = gpu:0, op_role = 1, use_mkldnn = False)
    {Bias@GRAD=['layer_norm_2.b_0@GRAD'], Scale@GRAD=['layer_norm_2.w_0@GRAD'], X@GRAD=['tmp_3@GRAD@RENAME@block0@1']} = layer_norm_grad(inputs={Bias=['layer_norm_2.b_0'], Mean=['layer_norm_7.tmp_0'], Scale=['layer_norm_2.w_0'], Variance=['layer_norm_7.tmp_1'], X=['tmp_3'], Y@GRAD=['layer_norm_7.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 1, op_role_var = ['layer_norm_2.b_0', 'layer_norm_2.b_0@GRAD', 'layer_norm_2.w_0', 'layer_norm_2.w_0@GRAD'], use_mkldnn = False, with_quant_attr = False)
    {Out=['tmp_3@GRAD']} = sum(inputs={X=['tmp_3@GRAD@RENAME@block0@0', 'tmp_3@GRAD@RENAME@block0@1']}, op_device = gpu:0, op_role = 1, use_mkldnn = False)
    {X@GRAD=['tmp_2@GRAD@RENAME@block0@0'], Y@GRAD=['linear_17.tmp_1@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_3@GRAD'], X=['tmp_2'], Y=['linear_17.tmp_1']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, with_quant_attr = False, x_data_format = , y_data_format = )
    {X@GRAD=['linear_17.tmp_0@GRAD'], Y@GRAD=['linear_5.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_17.tmp_1@GRAD'], X=['linear_17.tmp_0'], Y=['linear_5.b_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 1, op_role_var = ['linear_5.b_0', 'linear_5.b_0@GRAD'], use_mkldnn = False, use_quantizer = False, with_quant_attr = False, x_data_format = , y_data_format = )
    {X@GRAD=['gelu_0.tmp_0@GRAD'], Y@GRAD=['linear_5.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_17.tmp_0@GRAD'], X=['gelu_0.tmp_0'], Y=['linear_5.w_0']}, fused_reshape_Out = [], fused_transpose_Out = [], mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 1, op_role_var = ['linear_5.w_0', 'linear_5.w_0@GRAD'], trans_x = False, trans_y = False, use_mkldnn = False, with_quant_attr = False)
    {X@GRAD=['linear_16.tmp_1@GRAD']} = gelu_grad(inputs={Out@GRAD=['gelu_0.tmp_0@GRAD'], X=['linear_16.tmp_1']}, approximate = True, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = False, use_mkldnn = False, with_quant_attr = False)
    {X@GRAD=['linear_16.tmp_0@GRAD'], Y@GRAD=['linear_4.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_16.tmp_1@GRAD'], X=['linear_16.tmp_0'], Y=['linear_4.b_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 1, op_role_var = ['linear_4.b_0', 'linear_4.b_0@GRAD'], use_mkldnn = False, use_quantizer = False, with_quant_attr = False, x_data_format = , y_data_format = )
    {X@GRAD=['layer_norm_6.tmp_2@GRAD'], Y@GRAD=['linear_4.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_16.tmp_0@GRAD'], X=['layer_norm_6.tmp_2'], Y=['linear_4.w_0']}, fused_reshape_Out = [], fused_transpose_Out = [], mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 1, op_role_var = ['linear_4.w_0', 'linear_4.w_0@GRAD'], trans_x = False, trans_y = False, use_mkldnn = False, with_quant_attr = False)
    {Bias@GRAD=['layer_norm_1.b_0@GRAD'], Scale@GRAD=['layer_norm_1.w_0@GRAD'], X@GRAD=['tmp_2@GRAD@RENAME@block0@1']} = layer_norm_grad(inputs={Bias=['layer_norm_1.b_0'], Mean=['layer_norm_6.tmp_0'], Scale=['layer_norm_1.w_0'], Variance=['layer_norm_6.tmp_1'], X=['tmp_2'], Y@GRAD=['layer_norm_6.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 1, op_role_var = ['layer_norm_1.b_0', 'layer_norm_1.b_0@GRAD', 'layer_norm_1.w_0', 'layer_norm_1.w_0@GRAD'], use_mkldnn = False, with_quant_attr = False)
    {Out=['tmp_2@GRAD']} = sum(inputs={X=['tmp_2@GRAD@RENAME@block0@0', 'tmp_2@GRAD@RENAME@block0@1']}, op_device = gpu:0, op_role = 1, use_mkldnn = False)
    {X@GRAD=['tmp_0@GRAD@RENAME@block0@0'], Y@GRAD=['linear_15.tmp_1@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_2@GRAD'], X=['tmp_0'], Y=['linear_15.tmp_1']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, with_quant_attr = False, x_data_format = , y_data_format = )
    {X@GRAD=['linear_15.tmp_0@GRAD'], Y@GRAD=['linear_3.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_15.tmp_1@GRAD'], X=['linear_15.tmp_0'], Y=['linear_3.b_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 1, op_role_var = ['linear_3.b_0', 'linear_3.b_0@GRAD'], use_mkldnn = False, use_quantizer = False, with_quant_attr = False, x_data_format = , y_data_format = )
    {X@GRAD=['reshape2_3.tmp_0@GRAD'], Y@GRAD=['linear_3.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_15.tmp_0@GRAD'], X=['reshape2_3.tmp_0'], Y=['linear_3.w_0']}, fused_reshape_Out = [], fused_transpose_Out = [], mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 1, op_role_var = ['linear_3.w_0', 'linear_3.w_0@GRAD'], trans_x = False, trans_y = False, use_mkldnn = False, with_quant_attr = False)
    {X@GRAD=['transpose_3.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_3.tmp_0@GRAD'], XShape=['reshape2_3.tmp_1']}, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 768], use_mkldnn = False, use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['matmul_v2_0.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_3.tmp_0@GRAD'], XShape=['transpose_3.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['softmax_0.tmp_0@GRAD'], Y@GRAD=['transpose_2.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_0.tmp_0@GRAD'], X=['softmax_0.tmp_0'], Y=['transpose_2.tmp_0']}, fused_reshape_Out = [], fused_transpose_Out = [], mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, use_mkldnn = False, with_quant_attr = False)
    {X@GRAD=['tmp_1@GRAD']} = softmax_grad(inputs={Out=['softmax_0.tmp_0'], Out@GRAD=['softmax_0.tmp_0@GRAD']}, axis = -1, data_format = AnyLayout, is_test = False, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = True, use_mkldnn = False, with_quant_attr = False)
    {X@GRAD=['matmul_0.tmp_0@GRAD'], Y@GRAD=[]} = elementwise_add_grad(inputs={Out@GRAD=['tmp_1@GRAD'], X=['matmul_0.tmp_0'], Y=['attention_mask']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, with_quant_attr = False, x_data_format = , y_data_format = )
    {X@GRAD=['transpose_0.tmp_0@GRAD'], Y@GRAD=['transpose_1.tmp_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['matmul_0.tmp_0@GRAD'], X=['transpose_0.tmp_0'], Y=['transpose_1.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 0.125, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = True, use_mkldnn = False, use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['reshape2_2.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_2.tmp_0@GRAD'], XShape=['transpose_2.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['linear_14.tmp_1@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_2.tmp_0@GRAD'], XShape=['reshape2_2.tmp_1']}, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_mkldnn = False, use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['reshape2_1.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_1.tmp_0@GRAD'], XShape=['transpose_1.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['linear_13.tmp_1@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_1.tmp_0@GRAD'], XShape=['reshape2_1.tmp_1']}, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_mkldnn = False, use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['linear_14.tmp_0@GRAD'], Y@GRAD=['linear_2.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_14.tmp_1@GRAD'], X=['linear_14.tmp_0'], Y=['linear_2.b_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 1, op_role_var = ['linear_2.b_0', 'linear_2.b_0@GRAD'], use_mkldnn = False, use_quantizer = False, with_quant_attr = False, x_data_format = , y_data_format = )
    {X@GRAD=['layer_norm_5.tmp_2@GRAD@RENAME@block0@0'], Y@GRAD=['linear_2.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_14.tmp_0@GRAD'], X=['layer_norm_5.tmp_2'], Y=['linear_2.w_0']}, fused_reshape_Out = [], fused_transpose_Out = [], mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 1, op_role_var = ['linear_2.w_0', 'linear_2.w_0@GRAD'], trans_x = False, trans_y = False, use_mkldnn = False, with_quant_attr = False)
    {X@GRAD=['linear_13.tmp_0@GRAD'], Y@GRAD=['linear_1.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_13.tmp_1@GRAD'], X=['linear_13.tmp_0'], Y=['linear_1.b_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 1, op_role_var = ['linear_1.b_0', 'linear_1.b_0@GRAD'], use_mkldnn = False, use_quantizer = False, with_quant_attr = False, x_data_format = , y_data_format = )
    {X@GRAD=['layer_norm_5.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['linear_1.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_13.tmp_0@GRAD'], X=['layer_norm_5.tmp_2'], Y=['linear_1.w_0']}, fused_reshape_Out = [], fused_transpose_Out = [], mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 1, op_role_var = ['linear_1.w_0', 'linear_1.w_0@GRAD'], trans_x = False, trans_y = False, use_mkldnn = False, with_quant_attr = False)
    {X@GRAD=['reshape2_0.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_0.tmp_0@GRAD'], XShape=['transpose_0.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['linear_12.tmp_1@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_0.tmp_0@GRAD'], XShape=['reshape2_0.tmp_1']}, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_mkldnn = False, use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['linear_12.tmp_0@GRAD'], Y@GRAD=['linear_0.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_12.tmp_1@GRAD'], X=['linear_12.tmp_0'], Y=['linear_0.b_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 1, op_role_var = ['linear_0.b_0', 'linear_0.b_0@GRAD'], use_mkldnn = False, use_quantizer = False, with_quant_attr = False, x_data_format = , y_data_format = )
    {X@GRAD=['layer_norm_5.tmp_2@GRAD@RENAME@block0@2'], Y@GRAD=['linear_0.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_12.tmp_0@GRAD'], X=['layer_norm_5.tmp_2'], Y=['linear_0.w_0']}, fused_reshape_Out = [], fused_transpose_Out = [], mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 1, op_role_var = ['linear_0.w_0', 'linear_0.w_0@GRAD'], trans_x = False, trans_y = False, use_mkldnn = False, with_quant_attr = False)
    {Out=['layer_norm_5.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_5.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_5.tmp_2@GRAD@RENAME@block0@1', 'layer_norm_5.tmp_2@GRAD@RENAME@block0@2']}, op_device = gpu:0, op_role = 1, use_mkldnn = False)
    {Bias@GRAD=['layer_norm_0.b_0@GRAD'], Scale@GRAD=['layer_norm_0.w_0@GRAD'], X@GRAD=['tmp_0@GRAD@RENAME@block0@1']} = layer_norm_grad(inputs={Bias=['layer_norm_0.b_0'], Mean=['layer_norm_5.tmp_0'], Scale=['layer_norm_0.w_0'], Variance=['layer_norm_5.tmp_1'], X=['tmp_0'], Y@GRAD=['layer_norm_5.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, is_test = False, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 1, op_role_var = ['layer_norm_0.b_0', 'layer_norm_0.b_0@GRAD', 'layer_norm_0.w_0', 'layer_norm_0.w_0@GRAD'], use_mkldnn = False, with_quant_attr = False)
    {Out=['tmp_0@GRAD']} = sum(inputs={X=['tmp_0@GRAD@RENAME@block0@0', 'tmp_0@GRAD@RENAME@block0@1']}, op_device = gpu:0, op_role = 1, use_mkldnn = False)
    {X@GRAD=['embedding_3.tmp_0@GRAD'], Y@GRAD=['embedding_4.tmp_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_0@GRAD'], X=['embedding_3.tmp_0'], Y=['embedding_4.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = gpu:0, op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, with_quant_attr = False, x_data_format = , y_data_format = )
    {W@GRAD=['pos_embeddings@GRAD']} = lookup_table_v2_grad(inputs={Ids=['position_ids'], Out@GRAD=['embedding_4.tmp_0@GRAD'], W=['pos_embeddings']}, epmap = [], height_sections = [], is_distributed = False, is_sparse = False, op_device = gpu:0, op_namescope = /, op_role = 1, op_role_var = ['pos_embeddings', 'pos_embeddings@GRAD'], padding_idx = -1, remote_prefetch = False, table_names = [], trainer_id = 0, with_quant_attr = False)
    {W@GRAD=['word_embeddings@GRAD']} = lookup_table_v2_grad(inputs={Ids=['tokens'], Out@GRAD=['embedding_3.tmp_0@GRAD'], W=['word_embeddings']}, epmap = [], height_sections = [], is_distributed = False, is_sparse = False, op_device = gpu:0, op_namescope = /, op_role = 1, op_role_var = ['word_embeddings', 'word_embeddings@GRAD'], padding_idx = -1, remote_prefetch = False, table_names = [], trainer_id = 0, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_0.b_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_0.b_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_0.b_0_moment1_0'], Moment2Out=['layer_norm_0.b_0_moment2_0'], ParamOut=['layer_norm_0.b_0']} = adam(inputs={Beta1Pow=['layer_norm_0.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_0.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_0.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_0.b_0_moment1_0'], Moment2=['layer_norm_0.b_0_moment2_0'], Param=['layer_norm_0.b_0'], SkipUpdate=[]}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = gpu:0, op_namescope = /optimizer/, op_role = 2, op_role_var = ['layer_norm_0.b_0', 'layer_norm_0.b_0@GRAD'], use_global_beta_pow = False, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_0.w_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_0.w_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_0.w_0_moment1_0'], Moment2Out=['layer_norm_0.w_0_moment2_0'], ParamOut=['layer_norm_0.w_0']} = adam(inputs={Beta1Pow=['layer_norm_0.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_0.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_0.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_0.w_0_moment1_0'], Moment2=['layer_norm_0.w_0_moment2_0'], Param=['layer_norm_0.w_0'], SkipUpdate=[]}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = gpu:0, op_namescope = /optimizer_1/, op_role = 2, op_role_var = ['layer_norm_0.w_0', 'layer_norm_0.w_0@GRAD'], use_global_beta_pow = False, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_1.b_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_1.b_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_1.b_0_moment1_0'], Moment2Out=['layer_norm_1.b_0_moment2_0'], ParamOut=['layer_norm_1.b_0']} = adam(inputs={Beta1Pow=['layer_norm_1.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_1.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_1.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_1.b_0_moment1_0'], Moment2=['layer_norm_1.b_0_moment2_0'], Param=['layer_norm_1.b_0'], SkipUpdate=[]}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = gpu:0, op_namescope = /optimizer_2/, op_role = 2, op_role_var = ['layer_norm_1.b_0', 'layer_norm_1.b_0@GRAD'], use_global_beta_pow = False, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_1.w_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_1.w_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_1.w_0_moment1_0'], Moment2Out=['layer_norm_1.w_0_moment2_0'], ParamOut=['layer_norm_1.w_0']} = adam(inputs={Beta1Pow=['layer_norm_1.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_1.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_1.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_1.w_0_moment1_0'], Moment2=['layer_norm_1.w_0_moment2_0'], Param=['layer_norm_1.w_0'], SkipUpdate=[]}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = gpu:0, op_namescope = /optimizer_3/, op_role = 2, op_role_var = ['layer_norm_1.w_0', 'layer_norm_1.w_0@GRAD'], use_global_beta_pow = False, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_2.b_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_2.b_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_2.b_0_moment1_0'], Moment2Out=['layer_norm_2.b_0_moment2_0'], ParamOut=['layer_norm_2.b_0']} = adam(inputs={Beta1Pow=['layer_norm_2.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_2.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_2.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_2.b_0_moment1_0'], Moment2=['layer_norm_2.b_0_moment2_0'], Param=['layer_norm_2.b_0'], SkipUpdate=[]}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = gpu:0, op_namescope = /optimizer_4/, op_role = 2, op_role_var = ['layer_norm_2.b_0', 'layer_norm_2.b_0@GRAD'], use_global_beta_pow = False, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_2.w_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_2.w_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_2.w_0_moment1_0'], Moment2Out=['layer_norm_2.w_0_moment2_0'], ParamOut=['layer_norm_2.w_0']} = adam(inputs={Beta1Pow=['layer_norm_2.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_2.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_2.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_2.w_0_moment1_0'], Moment2=['layer_norm_2.w_0_moment2_0'], Param=['layer_norm_2.w_0'], SkipUpdate=[]}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = gpu:0, op_namescope = /optimizer_5/, op_role = 2, op_role_var = ['layer_norm_2.w_0', 'layer_norm_2.w_0@GRAD'], use_global_beta_pow = False, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_3.b_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_3.b_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_3.b_0_moment1_0'], Moment2Out=['layer_norm_3.b_0_moment2_0'], ParamOut=['layer_norm_3.b_0']} = adam(inputs={Beta1Pow=['layer_norm_3.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_3.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_3.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_3.b_0_moment1_0'], Moment2=['layer_norm_3.b_0_moment2_0'], Param=['layer_norm_3.b_0'], SkipUpdate=[]}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = gpu:0, op_namescope = /optimizer_6/, op_role = 2, op_role_var = ['layer_norm_3.b_0', 'layer_norm_3.b_0@GRAD'], use_global_beta_pow = False, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_3.w_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_3.w_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_3.w_0_moment1_0'], Moment2Out=['layer_norm_3.w_0_moment2_0'], ParamOut=['layer_norm_3.w_0']} = adam(inputs={Beta1Pow=['layer_norm_3.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_3.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_3.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_3.w_0_moment1_0'], Moment2=['layer_norm_3.w_0_moment2_0'], Param=['layer_norm_3.w_0'], SkipUpdate=[]}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = gpu:0, op_namescope = /optimizer_7/, op_role = 2, op_role_var = ['layer_norm_3.w_0', 'layer_norm_3.w_0@GRAD'], use_global_beta_pow = False, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_4.b_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_4.b_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_4.b_0_moment1_0'], Moment2Out=['layer_norm_4.b_0_moment2_0'], ParamOut=['layer_norm_4.b_0']} = adam(inputs={Beta1Pow=['layer_norm_4.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_4.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_4.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_4.b_0_moment1_0'], Moment2=['layer_norm_4.b_0_moment2_0'], Param=['layer_norm_4.b_0'], SkipUpdate=[]}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = gpu:0, op_namescope = /optimizer_8/, op_role = 2, op_role_var = ['layer_norm_4.b_0', 'layer_norm_4.b_0@GRAD'], use_global_beta_pow = False, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_4.w_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_4.w_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_4.w_0_moment1_0'], Moment2Out=['layer_norm_4.w_0_moment2_0'], ParamOut=['layer_norm_4.w_0']} = adam(inputs={Beta1Pow=['layer_norm_4.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_4.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_4.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_4.w_0_moment1_0'], Moment2=['layer_norm_4.w_0_moment2_0'], Param=['layer_norm_4.w_0'], SkipUpdate=[]}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = gpu:0, op_namescope = /optimizer_9/, op_role = 2, op_role_var = ['layer_norm_4.w_0', 'layer_norm_4.w_0@GRAD'], use_global_beta_pow = False, with_quant_attr = False)
    {Beta1PowOut=['linear_0.b_0_beta1_pow_acc_0'], Beta2PowOut=['linear_0.b_0_beta2_pow_acc_0'], Moment1Out=['linear_0.b_0_moment1_0'], Moment2Out=['linear_0.b_0_moment2_0'], ParamOut=['linear_0.b_0']} = adam(inputs={Beta1Pow=['linear_0.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_0.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_0.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_0.b_0_moment1_0'], Moment2=['linear_0.b_0_moment2_0'], Param=['linear_0.b_0'], SkipUpdate=[]}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = gpu:0, op_namescope = /optimizer_10/, op_role = 2, op_role_var = ['linear_0.b_0', 'linear_0.b_0@GRAD'], use_global_beta_pow = False, with_quant_attr = False)
    {Beta1PowOut=['linear_0.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_0.w_0_beta2_pow_acc_0'], Moment1Out=['linear_0.w_0_moment1_0'], Moment2Out=['linear_0.w_0_moment2_0'], ParamOut=['linear_0.w_0']} = adam(inputs={Beta1Pow=['linear_0.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_0.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_0.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_0.w_0_moment1_0'], Moment2=['linear_0.w_0_moment2_0'], Param=['linear_0.w_0'], SkipUpdate=[]}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = gpu:0, op_namescope = /optimizer_11/, op_role = 2, op_role_var = ['linear_0.w_0', 'linear_0.w_0@GRAD'], use_global_beta_pow = False, with_quant_attr = False)
    {Beta1PowOut=['linear_1.b_0_beta1_pow_acc_0'], Beta2PowOut=['linear_1.b_0_beta2_pow_acc_0'], Moment1Out=['linear_1.b_0_moment1_0'], Moment2Out=['linear_1.b_0_moment2_0'], ParamOut=['linear_1.b_0']} = adam(inputs={Beta1Pow=['linear_1.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_1.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_1.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_1.b_0_moment1_0'], Moment2=['linear_1.b_0_moment2_0'], Param=['linear_1.b_0'], SkipUpdate=[]}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = gpu:0, op_namescope = /optimizer_12/, op_role = 2, op_role_var = ['linear_1.b_0', 'linear_1.b_0@GRAD'], use_global_beta_pow = False, with_quant_attr = False)
    {Beta1PowOut=['linear_1.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_1.w_0_beta2_pow_acc_0'], Moment1Out=['linear_1.w_0_moment1_0'], Moment2Out=['linear_1.w_0_moment2_0'], ParamOut=['linear_1.w_0']} = adam(inputs={Beta1Pow=['linear_1.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_1.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_1.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_1.w_0_moment1_0'], Moment2=['linear_1.w_0_moment2_0'], Param=['linear_1.w_0'], SkipUpdate=[]}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = gpu:0, op_namescope = /optimizer_13/, op_role = 2, op_role_var = ['linear_1.w_0', 'linear_1.w_0@GRAD'], use_global_beta_pow = False, with_quant_attr = False)
    {Beta1PowOut=['linear_10.b_0_beta1_pow_acc_0'], Beta2PowOut=['linear_10.b_0_beta2_pow_acc_0'], Moment1Out=['linear_10.b_0_moment1_0'], Moment2Out=['linear_10.b_0_moment2_0'], ParamOut=['linear_10.b_0']} = adam(inputs={Beta1Pow=['linear_10.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_10.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_10.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_10.b_0_moment1_0'], Moment2=['linear_10.b_0_moment2_0'], Param=['linear_10.b_0'], SkipUpdate=[]}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = gpu:0, op_namescope = /optimizer_14/, op_role = 2, op_role_var = ['linear_10.b_0', 'linear_10.b_0@GRAD'], use_global_beta_pow = False, with_quant_attr = False)
    {Beta1PowOut=['linear_10.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_10.w_0_beta2_pow_acc_0'], Moment1Out=['linear_10.w_0_moment1_0'], Moment2Out=['linear_10.w_0_moment2_0'], ParamOut=['linear_10.w_0']} = adam(inputs={Beta1Pow=['linear_10.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_10.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_10.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_10.w_0_moment1_0'], Moment2=['linear_10.w_0_moment2_0'], Param=['linear_10.w_0'], SkipUpdate=[]}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = gpu:0, op_namescope = /optimizer_15/, op_role = 2, op_role_var = ['linear_10.w_0', 'linear_10.w_0@GRAD'], use_global_beta_pow = False, with_quant_attr = False)
    {Beta1PowOut=['linear_11.b_0_beta1_pow_acc_0'], Beta2PowOut=['linear_11.b_0_beta2_pow_acc_0'], Moment1Out=['linear_11.b_0_moment1_0'], Moment2Out=['linear_11.b_0_moment2_0'], ParamOut=['linear_11.b_0']} = adam(inputs={Beta1Pow=['linear_11.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_11.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_11.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_11.b_0_moment1_0'], Moment2=['linear_11.b_0_moment2_0'], Param=['linear_11.b_0'], SkipUpdate=[]}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = gpu:0, op_namescope = /optimizer_16/, op_role = 2, op_role_var = ['linear_11.b_0', 'linear_11.b_0@GRAD'], use_global_beta_pow = False, with_quant_attr = False)
    {Beta1PowOut=['linear_11.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_11.w_0_beta2_pow_acc_0'], Moment1Out=['linear_11.w_0_moment1_0'], Moment2Out=['linear_11.w_0_moment2_0'], ParamOut=['linear_11.w_0']} = adam(inputs={Beta1Pow=['linear_11.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_11.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_11.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_11.w_0_moment1_0'], Moment2=['linear_11.w_0_moment2_0'], Param=['linear_11.w_0'], SkipUpdate=[]}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = gpu:0, op_namescope = /optimizer_17/, op_role = 2, op_role_var = ['linear_11.w_0', 'linear_11.w_0@GRAD'], use_global_beta_pow = False, with_quant_attr = False)
    {Beta1PowOut=['linear_2.b_0_beta1_pow_acc_0'], Beta2PowOut=['linear_2.b_0_beta2_pow_acc_0'], Moment1Out=['linear_2.b_0_moment1_0'], Moment2Out=['linear_2.b_0_moment2_0'], ParamOut=['linear_2.b_0']} = adam(inputs={Beta1Pow=['linear_2.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_2.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_2.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_2.b_0_moment1_0'], Moment2=['linear_2.b_0_moment2_0'], Param=['linear_2.b_0'], SkipUpdate=[]}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = gpu:0, op_namescope = /optimizer_18/, op_role = 2, op_role_var = ['linear_2.b_0', 'linear_2.b_0@GRAD'], use_global_beta_pow = False, with_quant_attr = False)
    {Beta1PowOut=['linear_2.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_2.w_0_beta2_pow_acc_0'], Moment1Out=['linear_2.w_0_moment1_0'], Moment2Out=['linear_2.w_0_moment2_0'], ParamOut=['linear_2.w_0']} = adam(inputs={Beta1Pow=['linear_2.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_2.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_2.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_2.w_0_moment1_0'], Moment2=['linear_2.w_0_moment2_0'], Param=['linear_2.w_0'], SkipUpdate=[]}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = gpu:0, op_namescope = /optimizer_19/, op_role = 2, op_role_var = ['linear_2.w_0', 'linear_2.w_0@GRAD'], use_global_beta_pow = False, with_quant_attr = False)
    {Beta1PowOut=['linear_3.b_0_beta1_pow_acc_0'], Beta2PowOut=['linear_3.b_0_beta2_pow_acc_0'], Moment1Out=['linear_3.b_0_moment1_0'], Moment2Out=['linear_3.b_0_moment2_0'], ParamOut=['linear_3.b_0']} = adam(inputs={Beta1Pow=['linear_3.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_3.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_3.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_3.b_0_moment1_0'], Moment2=['linear_3.b_0_moment2_0'], Param=['linear_3.b_0'], SkipUpdate=[]}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = gpu:0, op_namescope = /optimizer_20/, op_role = 2, op_role_var = ['linear_3.b_0', 'linear_3.b_0@GRAD'], use_global_beta_pow = False, with_quant_attr = False)
    {Beta1PowOut=['linear_3.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_3.w_0_beta2_pow_acc_0'], Moment1Out=['linear_3.w_0_moment1_0'], Moment2Out=['linear_3.w_0_moment2_0'], ParamOut=['linear_3.w_0']} = adam(inputs={Beta1Pow=['linear_3.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_3.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_3.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_3.w_0_moment1_0'], Moment2=['linear_3.w_0_moment2_0'], Param=['linear_3.w_0'], SkipUpdate=[]}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = gpu:0, op_namescope = /optimizer_21/, op_role = 2, op_role_var = ['linear_3.w_0', 'linear_3.w_0@GRAD'], use_global_beta_pow = False, with_quant_attr = False)
    {Beta1PowOut=['linear_4.b_0_beta1_pow_acc_0'], Beta2PowOut=['linear_4.b_0_beta2_pow_acc_0'], Moment1Out=['linear_4.b_0_moment1_0'], Moment2Out=['linear_4.b_0_moment2_0'], ParamOut=['linear_4.b_0']} = adam(inputs={Beta1Pow=['linear_4.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_4.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_4.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_4.b_0_moment1_0'], Moment2=['linear_4.b_0_moment2_0'], Param=['linear_4.b_0'], SkipUpdate=[]}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = gpu:0, op_namescope = /optimizer_22/, op_role = 2, op_role_var = ['linear_4.b_0', 'linear_4.b_0@GRAD'], use_global_beta_pow = False, with_quant_attr = False)
    {Beta1PowOut=['linear_4.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_4.w_0_beta2_pow_acc_0'], Moment1Out=['linear_4.w_0_moment1_0'], Moment2Out=['linear_4.w_0_moment2_0'], ParamOut=['linear_4.w_0']} = adam(inputs={Beta1Pow=['linear_4.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_4.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_4.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_4.w_0_moment1_0'], Moment2=['linear_4.w_0_moment2_0'], Param=['linear_4.w_0'], SkipUpdate=[]}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = gpu:0, op_namescope = /optimizer_23/, op_role = 2, op_role_var = ['linear_4.w_0', 'linear_4.w_0@GRAD'], use_global_beta_pow = False, with_quant_attr = False)
    {Beta1PowOut=['linear_5.b_0_beta1_pow_acc_0'], Beta2PowOut=['linear_5.b_0_beta2_pow_acc_0'], Moment1Out=['linear_5.b_0_moment1_0'], Moment2Out=['linear_5.b_0_moment2_0'], ParamOut=['linear_5.b_0']} = adam(inputs={Beta1Pow=['linear_5.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_5.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_5.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_5.b_0_moment1_0'], Moment2=['linear_5.b_0_moment2_0'], Param=['linear_5.b_0'], SkipUpdate=[]}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = gpu:0, op_namescope = /optimizer_24/, op_role = 2, op_role_var = ['linear_5.b_0', 'linear_5.b_0@GRAD'], use_global_beta_pow = False, with_quant_attr = False)
    {Beta1PowOut=['linear_5.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_5.w_0_beta2_pow_acc_0'], Moment1Out=['linear_5.w_0_moment1_0'], Moment2Out=['linear_5.w_0_moment2_0'], ParamOut=['linear_5.w_0']} = adam(inputs={Beta1Pow=['linear_5.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_5.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_5.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_5.w_0_moment1_0'], Moment2=['linear_5.w_0_moment2_0'], Param=['linear_5.w_0'], SkipUpdate=[]}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = gpu:0, op_namescope = /optimizer_25/, op_role = 2, op_role_var = ['linear_5.w_0', 'linear_5.w_0@GRAD'], use_global_beta_pow = False, with_quant_attr = False)
    {Beta1PowOut=['linear_6.b_0_beta1_pow_acc_0'], Beta2PowOut=['linear_6.b_0_beta2_pow_acc_0'], Moment1Out=['linear_6.b_0_moment1_0'], Moment2Out=['linear_6.b_0_moment2_0'], ParamOut=['linear_6.b_0']} = adam(inputs={Beta1Pow=['linear_6.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_6.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_6.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_6.b_0_moment1_0'], Moment2=['linear_6.b_0_moment2_0'], Param=['linear_6.b_0'], SkipUpdate=[]}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = gpu:0, op_namescope = /optimizer_26/, op_role = 2, op_role_var = ['linear_6.b_0', 'linear_6.b_0@GRAD'], use_global_beta_pow = False, with_quant_attr = False)
    {Beta1PowOut=['linear_6.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_6.w_0_beta2_pow_acc_0'], Moment1Out=['linear_6.w_0_moment1_0'], Moment2Out=['linear_6.w_0_moment2_0'], ParamOut=['linear_6.w_0']} = adam(inputs={Beta1Pow=['linear_6.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_6.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_6.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_6.w_0_moment1_0'], Moment2=['linear_6.w_0_moment2_0'], Param=['linear_6.w_0'], SkipUpdate=[]}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = gpu:0, op_namescope = /optimizer_27/, op_role = 2, op_role_var = ['linear_6.w_0', 'linear_6.w_0@GRAD'], use_global_beta_pow = False, with_quant_attr = False)
    {Beta1PowOut=['linear_7.b_0_beta1_pow_acc_0'], Beta2PowOut=['linear_7.b_0_beta2_pow_acc_0'], Moment1Out=['linear_7.b_0_moment1_0'], Moment2Out=['linear_7.b_0_moment2_0'], ParamOut=['linear_7.b_0']} = adam(inputs={Beta1Pow=['linear_7.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_7.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_7.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_7.b_0_moment1_0'], Moment2=['linear_7.b_0_moment2_0'], Param=['linear_7.b_0'], SkipUpdate=[]}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = gpu:0, op_namescope = /optimizer_28/, op_role = 2, op_role_var = ['linear_7.b_0', 'linear_7.b_0@GRAD'], use_global_beta_pow = False, with_quant_attr = False)
    {Beta1PowOut=['linear_7.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_7.w_0_beta2_pow_acc_0'], Moment1Out=['linear_7.w_0_moment1_0'], Moment2Out=['linear_7.w_0_moment2_0'], ParamOut=['linear_7.w_0']} = adam(inputs={Beta1Pow=['linear_7.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_7.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_7.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_7.w_0_moment1_0'], Moment2=['linear_7.w_0_moment2_0'], Param=['linear_7.w_0'], SkipUpdate=[]}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = gpu:0, op_namescope = /optimizer_29/, op_role = 2, op_role_var = ['linear_7.w_0', 'linear_7.w_0@GRAD'], use_global_beta_pow = False, with_quant_attr = False)
    {Beta1PowOut=['linear_8.b_0_beta1_pow_acc_0'], Beta2PowOut=['linear_8.b_0_beta2_pow_acc_0'], Moment1Out=['linear_8.b_0_moment1_0'], Moment2Out=['linear_8.b_0_moment2_0'], ParamOut=['linear_8.b_0']} = adam(inputs={Beta1Pow=['linear_8.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_8.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_8.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_8.b_0_moment1_0'], Moment2=['linear_8.b_0_moment2_0'], Param=['linear_8.b_0'], SkipUpdate=[]}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = gpu:0, op_namescope = /optimizer_30/, op_role = 2, op_role_var = ['linear_8.b_0', 'linear_8.b_0@GRAD'], use_global_beta_pow = False, with_quant_attr = False)
    {Beta1PowOut=['linear_8.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_8.w_0_beta2_pow_acc_0'], Moment1Out=['linear_8.w_0_moment1_0'], Moment2Out=['linear_8.w_0_moment2_0'], ParamOut=['linear_8.w_0']} = adam(inputs={Beta1Pow=['linear_8.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_8.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_8.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_8.w_0_moment1_0'], Moment2=['linear_8.w_0_moment2_0'], Param=['linear_8.w_0'], SkipUpdate=[]}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = gpu:0, op_namescope = /optimizer_31/, op_role = 2, op_role_var = ['linear_8.w_0', 'linear_8.w_0@GRAD'], use_global_beta_pow = False, with_quant_attr = False)
    {Beta1PowOut=['linear_9.b_0_beta1_pow_acc_0'], Beta2PowOut=['linear_9.b_0_beta2_pow_acc_0'], Moment1Out=['linear_9.b_0_moment1_0'], Moment2Out=['linear_9.b_0_moment2_0'], ParamOut=['linear_9.b_0']} = adam(inputs={Beta1Pow=['linear_9.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_9.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_9.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_9.b_0_moment1_0'], Moment2=['linear_9.b_0_moment2_0'], Param=['linear_9.b_0'], SkipUpdate=[]}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = gpu:0, op_namescope = /optimizer_32/, op_role = 2, op_role_var = ['linear_9.b_0', 'linear_9.b_0@GRAD'], use_global_beta_pow = False, with_quant_attr = False)
    {Beta1PowOut=['linear_9.w_0_beta1_pow_acc_0'], Beta2PowOut=['linear_9.w_0_beta2_pow_acc_0'], Moment1Out=['linear_9.w_0_moment1_0'], Moment2Out=['linear_9.w_0_moment2_0'], ParamOut=['linear_9.w_0']} = adam(inputs={Beta1Pow=['linear_9.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_9.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_9.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['linear_9.w_0_moment1_0'], Moment2=['linear_9.w_0_moment2_0'], Param=['linear_9.w_0'], SkipUpdate=[]}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = gpu:0, op_namescope = /optimizer_33/, op_role = 2, op_role_var = ['linear_9.w_0', 'linear_9.w_0@GRAD'], use_global_beta_pow = False, with_quant_attr = False)
    {Beta1PowOut=['output_embeddings_beta1_pow_acc_0'], Beta2PowOut=['output_embeddings_beta2_pow_acc_0'], Moment1Out=['output_embeddings_moment1_0'], Moment2Out=['output_embeddings_moment2_0'], ParamOut=['output_embeddings']} = adam(inputs={Beta1Pow=['output_embeddings_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['output_embeddings_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['output_embeddings@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['output_embeddings_moment1_0'], Moment2=['output_embeddings_moment2_0'], Param=['output_embeddings'], SkipUpdate=[]}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = gpu:0, op_namescope = /optimizer_34/, op_role = 2, op_role_var = ['output_embeddings', 'output_embeddings@GRAD'], use_global_beta_pow = False, with_quant_attr = False)
    {Beta1PowOut=['pos_embeddings_beta1_pow_acc_0'], Beta2PowOut=['pos_embeddings_beta2_pow_acc_0'], Moment1Out=['pos_embeddings_moment1_0'], Moment2Out=['pos_embeddings_moment2_0'], ParamOut=['pos_embeddings']} = adam(inputs={Beta1Pow=['pos_embeddings_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['pos_embeddings_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['pos_embeddings@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['pos_embeddings_moment1_0'], Moment2=['pos_embeddings_moment2_0'], Param=['pos_embeddings'], SkipUpdate=[]}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = gpu:0, op_namescope = /optimizer_35/, op_role = 2, op_role_var = ['pos_embeddings', 'pos_embeddings@GRAD'], use_global_beta_pow = False, with_quant_attr = False)
    {Beta1PowOut=['word_embeddings_beta1_pow_acc_0'], Beta2PowOut=['word_embeddings_beta2_pow_acc_0'], Moment1Out=['word_embeddings_moment1_0'], Moment2Out=['word_embeddings_moment2_0'], ParamOut=['word_embeddings']} = adam(inputs={Beta1Pow=['word_embeddings_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['word_embeddings_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['word_embeddings@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['word_embeddings_moment1_0'], Moment2=['word_embeddings_moment2_0'], Param=['word_embeddings'], SkipUpdate=[]}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = gpu:0, op_namescope = /optimizer_36/, op_role = 2, op_role_var = ['word_embeddings', 'word_embeddings@GRAD'], use_global_beta_pow = False, with_quant_attr = False)
}
