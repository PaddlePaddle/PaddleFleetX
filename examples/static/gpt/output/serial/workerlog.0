grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
Hint: Your machine support AVX, but the installed paddlepaddle doesn't have avx core. Hence, no-avx core with worse preformance will be imported.
If you like, you could reinstall paddlepaddle by 'python -m pip install --force-reinstall paddlepaddle-gpu[==version]' to get better performance.
The original error is: No module named 'paddle.fluid.core_avx'
/usr/local/lib/python3.7/dist-packages/pkg_resources/_vendor/pyparsing.py:943: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working
  collections.MutableMapping.register(ParseResults)
/usr/local/lib/python3.7/dist-packages/pkg_resources/_vendor/pyparsing.py:3245: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working
  elif isinstance( exprs, collections.Iterable ):
/usr/local/lib/python3.7/dist-packages/setuptools/depends.py:2: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
W1110 20:03:10.521330 32799 init.cc:202] AVX is available, Please re-compile on local machine
dp_info:
	 GroupInfo(size=1, rank=0, world=[0]), 
pp_info:
	 GroupInfo(size=1, rank=0, world=[0]), 
sharding_info:
	 GroupInfo(size=1, rank=0, world=[0]), 
mp_info:
	 GroupInfo(size=1, rank=0, world=[0])
[32m[2021-11-10 20:03:10,919] [    INFO][0m - Found /root/.paddlenlp/models/gpt2-en/gpt-en-vocab.json[0m
[32m[2021-11-10 20:03:10,920] [    INFO][0m - Found /root/.paddlenlp/models/gpt2-en/gpt-en-merges.txt[0m
[32m[2021-11-10 20:03:11,038] [    INFO][0m - The distributed run, total device num:1[0m
[32m[2021-11-10 20:03:11,038] [    INFO][0m - The distributed run, distinct dataflow num:1[0m
[32m[2021-11-10 20:03:11,038] [    INFO][0m - The distributed run, repeat dataflow times:1[0m
INFO:root:places would be ommited when DataLoader is not iterable
Init GPTModel on gpu:0
Init GPTForPretraining on gpu:0
Forward GPTForPretraining on gpu:0
Forward GPTModel on gpu:0
/usr/local/lib/python3.7/dist-packages/paddle/fluid/layers/math_op_patch.py:341: UserWarning: /workspace/FleetX/examples/static/gpt/modeling.py:576
The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.
  op_type, op_type, EXPRESSION_MAP[method_name]))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/layers/math_op_patch.py:341: UserWarning: /workspace/FleetX/examples/static/gpt/modeling.py:264
The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.
  op_type, op_type, EXPRESSION_MAP[method_name]))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/layers/math_op_patch.py:341: UserWarning: /workspace/FleetX/examples/static/gpt/modeling.py:482
The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.
  op_type, op_type, EXPRESSION_MAP[method_name]))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/layers/math_op_patch.py:341: UserWarning: /workspace/FleetX/examples/static/gpt/modeling.py:494
The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.
  op_type, op_type, EXPRESSION_MAP[method_name]))
Init GPTPretrainingCriterion on gpu:0
Forward GPTPretrainingCriterion on gpu:0
/usr/local/lib/python3.7/dist-packages/paddle/fluid/layers/math_op_patch.py:341: UserWarning: /workspace/FleetX/examples/static/gpt/modeling.py:773
The behavior of expression A * B has been unified with elementwise_mul(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_mul(X, Y, axis=0) instead of A * B. This transitional warning will be dropped in the future.
  op_type, op_type, EXPRESSION_MAP[method_name]))
/usr/local/lib/python3.7/dist-packages/paddle/distributed/fleet/meta_optimizers/graph_execution_optimizer.py:178: DeprecationWarning: The 'warn' function is deprecated, use 'warning' instead
  logging.warn("set nccl_comm_num=1 since you only have 1 node.")
WARNING:root:set nccl_comm_num=1 since you only have 1 node.
    +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    3                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                  False                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                   True                 |
    |                  downpour_table_param                                        |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    2                   |
    |          num_iteration_per_drop_scope                    1                   |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+

The training meta optimizer is/are []
W1110 20:03:27.498257 32799 device_context.cc:451] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W1110 20:03:27.501922 32799 device_context.cc:469] device: 0, cuDNN Version: 8.1.
W1110 20:03:30.822957 32799 operator.cc:1248] Device index is only supported under pipeline parallelism, so it will be ignored.
/usr/local/lib/python3.7/dist-packages/paddle/fluid/reader.py:136: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  if arr.dtype == np.object:
step:  0 loss_print:  [array([10.948218], dtype=float32)]
step:  1 loss_print:  [array([10.926441], dtype=float32)]
step:  2 loss_print:  [array([10.886727], dtype=float32)]
step:  3 loss_print:  [array([10.799146], dtype=float32)]
step:  4 loss_print:  [array([10.735771], dtype=float32)]
step:  5 loss_print:  [array([10.76184], dtype=float32)]
step:  6 loss_print:  [array([10.735151], dtype=float32)]
step:  7 loss_print:  [array([10.63905], dtype=float32)]
step:  8 loss_print:  [array([10.645192], dtype=float32)]
step:  9 loss_print:  [array([10.669403], dtype=float32)]
step:  10 loss_print:  [array([10.514788], dtype=float32)]
step:  11 loss_print:  [array([10.624485], dtype=float32)]
step:  12 loss_print:  [array([10.570334], dtype=float32)]
step:  13 loss_print:  [array([10.523032], dtype=float32)]
step:  14 loss_print:  [array([10.454746], dtype=float32)]
step:  15 loss_print:  [array([10.41185], dtype=float32)]
step:  16 loss_print:  [array([10.6107435], dtype=float32)]
step:  17 loss_print:  [array([10.533065], dtype=float32)]
step:  18 loss_print:  [array([10.52702], dtype=float32)]
step:  19 loss_print:  [array([10.333948], dtype=float32)]
step:  20 loss_print:  [array([10.3254], dtype=float32)]
step:  21 loss_print:  [array([10.291394], dtype=float32)]
step:  22 loss_print:  [array([10.263825], dtype=float32)]
step:  23 loss_print:  [array([10.151881], dtype=float32)]
step:  24 loss_print:  [array([10.294373], dtype=float32)]
step:  25 loss_print:  [array([10.306645], dtype=float32)]
step:  26 loss_print:  [array([10.125299], dtype=float32)]
step:  27 loss_print:  [array([10.213848], dtype=float32)]
step:  28 loss_print:  [array([10.263199], dtype=float32)]
step:  29 loss_print:  [array([10.283932], dtype=float32)]
step:  30 loss_print:  [array([10.201069], dtype=float32)]
step:  31 loss_print:  [array([10.281971], dtype=float32)]
step:  32 loss_print:  [array([10.175202], dtype=float32)]
step:  33 loss_print:  [array([10.05723], dtype=float32)]
step:  34 loss_print:  [array([10.174022], dtype=float32)]
step:  35 loss_print:  [array([10.232189], dtype=float32)]
step:  36 loss_print:  [array([10.200514], dtype=float32)]
step:  37 loss_print:  [array([9.944907], dtype=float32)]
step:  38 loss_print:  [array([10.174416], dtype=float32)]
step:  39 loss_print:  [array([10.098135], dtype=float32)]
step:  40 loss_print:  [array([10.044289], dtype=float32)]
step:  41 loss_print:  [array([10.077901], dtype=float32)]
step:  42 loss_print:  [array([9.942992], dtype=float32)]
step:  43 loss_print:  [array([10.195333], dtype=float32)]
step:  44 loss_print:  [array([10.227403], dtype=float32)]
step:  45 loss_print:  [array([10.032548], dtype=float32)]
step:  46 loss_print:  [array([10.217935], dtype=float32)]
step:  47 loss_print:  [array([10.033681], dtype=float32)]
step:  48 loss_print:  [array([10.1793785], dtype=float32)]
step:  49 loss_print:  [array([10.017375], dtype=float32)]
step:  50 loss_print:  [array([10.132625], dtype=float32)]
step:  51 loss_print:  [array([9.952589], dtype=float32)]
step:  52 loss_print:  [array([9.946114], dtype=float32)]
step:  53 loss_print:  [array([10.025021], dtype=float32)]
step:  54 loss_print:  [array([9.865562], dtype=float32)]
step:  55 loss_print:  [array([9.824236], dtype=float32)]
step:  56 loss_print:  [array([9.87105], dtype=float32)]
step:  57 loss_print:  [array([9.860848], dtype=float32)]
step:  58 loss_print:  [array([9.744232], dtype=float32)]
step:  59 loss_print:  [array([9.78611], dtype=float32)]
step:  60 loss_print:  [array([9.695765], dtype=float32)]
step:  61 loss_print:  [array([9.769169], dtype=float32)]
step:  62 loss_print:  [array([9.75839], dtype=float32)]
step:  63 loss_print:  [array([9.881493], dtype=float32)]
step:  64 loss_print:  [array([9.762363], dtype=float32)]
step:  65 loss_print:  [array([9.700998], dtype=float32)]
step:  66 loss_print:  [array([10.093382], dtype=float32)]
step:  67 loss_print:  [array([9.959647], dtype=float32)]
step:  68 loss_print:  [array([9.8249655], dtype=float32)]
step:  69 loss_print:  [array([9.885042], dtype=float32)]
step:  70 loss_print:  [array([9.801742], dtype=float32)]
step:  71 loss_print:  [array([10.006901], dtype=float32)]
step:  72 loss_print:  [array([9.726671], dtype=float32)]
step:  73 loss_print:  [array([9.786421], dtype=float32)]
step:  74 loss_print:  [array([9.753457], dtype=float32)]
step:  75 loss_print:  [array([9.784088], dtype=float32)]
step:  76 loss_print:  [array([9.831145], dtype=float32)]
step:  77 loss_print:  [array([9.761191], dtype=float32)]
step:  78 loss_print:  [array([9.677542], dtype=float32)]
step:  79 loss_print:  [array([9.704756], dtype=float32)]
step:  80 loss_print:  [array([9.79363], dtype=float32)]
step:  81 loss_print:  [array([9.676271], dtype=float32)]
step:  82 loss_print:  [array([9.828637], dtype=float32)]
step:  83 loss_print:  [array([9.760553], dtype=float32)]
step:  84 loss_print:  [array([9.610724], dtype=float32)]
step:  85 loss_print:  [array([9.654184], dtype=float32)]
step:  86 loss_print:  [array([9.82437], dtype=float32)]
step:  87 loss_print:  [array([9.543932], dtype=float32)]
step:  88 loss_print:  [array([9.589308], dtype=float32)]
step:  89 loss_print:  [array([9.689516], dtype=float32)]
step:  90 loss_print:  [array([9.818676], dtype=float32)]
step:  91 loss_print:  [array([9.621447], dtype=float32)]
step:  92 loss_print:  [array([9.846465], dtype=float32)]
step:  93 loss_print:  [array([9.527768], dtype=float32)]
step:  94 loss_print:  [array([9.681744], dtype=float32)]
step:  95 loss_print:  [array([9.6999445], dtype=float32)]
step:  96 loss_print:  [array([9.722996], dtype=float32)]
step:  97 loss_print:  [array([9.44556], dtype=float32)]
step:  98 loss_print:  [array([9.534186], dtype=float32)]
step:  99 loss_print:  [array([9.538053], dtype=float32)]
step:  100 loss_print:  [array([9.48355], dtype=float32)]
step:  101 loss_print:  [array([9.569818], dtype=float32)]
step:  102 loss_print:  [array([9.816555], dtype=float32)]
step:  103 loss_print:  [array([9.445943], dtype=float32)]
step:  104 loss_print:  [array([9.576906], dtype=float32)]
step:  105 loss_print:  [array([9.505941], dtype=float32)]
step:  106 loss_print:  [array([9.408454], dtype=float32)]
step:  107 loss_print:  [array([9.414432], dtype=float32)]
step:  108 loss_print:  [array([9.598808], dtype=float32)]
step:  109 loss_print:  [array([9.515063], dtype=float32)]
step:  110 loss_print:  [array([9.467216], dtype=float32)]
step:  111 loss_print:  [array([9.62038], dtype=float32)]
step:  112 loss_print:  [array([9.700727], dtype=float32)]
step:  113 loss_print:  [array([9.485332], dtype=float32)]
step:  114 loss_print:  [array([9.58991], dtype=float32)]
step:  115 loss_print:  [array([9.743441], dtype=float32)]
step:  116 loss_print:  [array([9.370408], dtype=float32)]
step:  117 loss_print:  [array([9.562677], dtype=float32)]
step:  118 loss_print:  [array([9.437162], dtype=float32)]
step:  119 loss_print:  [array([9.740506], dtype=float32)]
step:  120 loss_print:  [array([9.680243], dtype=float32)]
step:  121 loss_print:  [array([9.570427], dtype=float32)]
step:  122 loss_print:  [array([9.819236], dtype=float32)]
step:  123 loss_print:  [array([9.268339], dtype=float32)]
step:  124 loss_print:  [array([9.529338], dtype=float32)]
step:  125 loss_print:  [array([9.414149], dtype=float32)]
step:  126 loss_print:  [array([9.5763035], dtype=float32)]
step:  127 loss_print:  [array([9.388963], dtype=float32)]
step:  128 loss_print:  [array([9.513555], dtype=float32)]
step:  129 loss_print:  [array([9.499047], dtype=float32)]
step:  130 loss_print:  [array([9.770594], dtype=float32)]
step:  131 loss_print:  [array([9.825584], dtype=float32)]
step:  132 loss_print:  [array([9.518827], dtype=float32)]
step:  133 loss_print:  [array([9.387245], dtype=float32)]
step:  134 loss_print:  [array([9.455754], dtype=float32)]
step:  135 loss_print:  [array([9.434444], dtype=float32)]
step:  136 loss_print:  [array([10.106791], dtype=float32)]
step:  137 loss_print:  [array([9.323247], dtype=float32)]
step:  138 loss_print:  [array([9.577361], dtype=float32)]
step:  139 loss_print:  [array([9.572628], dtype=float32)]
step:  140 loss_print:  [array([9.248289], dtype=float32)]
step:  141 loss_print:  [array([9.416991], dtype=float32)]
step:  142 loss_print:  [array([9.245158], dtype=float32)]
step:  143 loss_print:  [array([9.5128975], dtype=float32)]
step:  144 loss_print:  [array([9.283234], dtype=float32)]
step:  145 loss_print:  [array([9.248071], dtype=float32)]
step:  146 loss_print:  [array([9.295007], dtype=float32)]
step:  147 loss_print:  [array([9.288839], dtype=float32)]
step:  148 loss_print:  [array([9.3500185], dtype=float32)]
step:  149 loss_print:  [array([9.599671], dtype=float32)]
step:  150 loss_print:  [array([9.434429], dtype=float32)]
step:  151 loss_print:  [array([9.345512], dtype=float32)]
step:  152 loss_print:  [array([9.343892], dtype=float32)]
step:  153 loss_print:  [array([9.231447], dtype=float32)]
step:  154 loss_print:  [array([9.378645], dtype=float32)]
step:  155 loss_print:  [array([9.765992], dtype=float32)]
step:  156 loss_print:  [array([9.389192], dtype=float32)]
step:  157 loss_print:  [array([9.493438], dtype=float32)]
step:  158 loss_print:  [array([9.238982], dtype=float32)]
step:  159 loss_print:  [array([9.447068], dtype=float32)]
step:  160 loss_print:  [array([9.230572], dtype=float32)]
step:  161 loss_print:  [array([9.26857], dtype=float32)]
step:  162 loss_print:  [array([9.291137], dtype=float32)]
step:  163 loss_print:  [array([9.085036], dtype=float32)]
step:  164 loss_print:  [array([9.414706], dtype=float32)]
step:  165 loss_print:  [array([9.225538], dtype=float32)]
step:  166 loss_print:  [array([9.442499], dtype=float32)]
step:  167 loss_print:  [array([9.36529], dtype=float32)]
step:  168 loss_print:  [array([9.198929], dtype=float32)]
step:  169 loss_print:  [array([9.355474], dtype=float32)]
step:  170 loss_print:  [array([9.562629], dtype=float32)]
step:  171 loss_print:  [array([9.293186], dtype=float32)]
step:  172 loss_print:  [array([9.126528], dtype=float32)]
step:  173 loss_print:  [array([9.435165], dtype=float32)]
step:  174 loss_print:  [array([9.357864], dtype=float32)]
step:  175 loss_print:  [array([9.263808], dtype=float32)]
step:  176 loss_print:  [array([9.455989], dtype=float32)]
step:  177 loss_print:  [array([8.990454], dtype=float32)]
step:  178 loss_print:  [array([8.979925], dtype=float32)]
step:  179 loss_print:  [array([9.105763], dtype=float32)]
step:  180 loss_print:  [array([9.073021], dtype=float32)]
step:  181 loss_print:  [array([9.209312], dtype=float32)]
step:  182 loss_print:  [array([9.2786255], dtype=float32)]
step:  183 loss_print:  [array([9.252193], dtype=float32)]
step:  184 loss_print:  [array([9.272327], dtype=float32)]
step:  185 loss_print:  [array([9.210615], dtype=float32)]
step:  186 loss_print:  [array([9.18079], dtype=float32)]
step:  187 loss_print:  [array([9.586661], dtype=float32)]
step:  188 loss_print:  [array([9.516343], dtype=float32)]
step:  189 loss_print:  [array([9.046921], dtype=float32)]
step:  190 loss_print:  [array([9.288654], dtype=float32)]
step:  191 loss_print:  [array([8.957207], dtype=float32)]
step:  192 loss_print:  [array([9.066829], dtype=float32)]
step:  193 loss_print:  [array([9.051848], dtype=float32)]
step:  194 loss_print:  [array([9.079199], dtype=float32)]
step:  195 loss_print:  [array([8.8634205], dtype=float32)]
step:  196 loss_print:  [array([8.796589], dtype=float32)]
step:  197 loss_print:  [array([9.262065], dtype=float32)]
step:  198 loss_print:  [array([8.946136], dtype=float32)]
step:  199 loss_print:  [array([9.157732], dtype=float32)]
step:  200 loss_print:  [array([8.941586], dtype=float32)]
step:  201 loss_print:  [array([9.102812], dtype=float32)]
step:  202 loss_print:  [array([8.99917], dtype=float32)]
step:  203 loss_print:  [array([8.982717], dtype=float32)]
step:  204 loss_print:  [array([9.164152], dtype=float32)]
step:  205 loss_print:  [array([8.957197], dtype=float32)]
step:  206 loss_print:  [array([9.176318], dtype=float32)]
step:  207 loss_print:  [array([8.97079], dtype=float32)]
step:  208 loss_print:  [array([8.99047], dtype=float32)]
step:  209 loss_print:  [array([9.164307], dtype=float32)]
step:  210 loss_print:  [array([8.994517], dtype=float32)]
step:  211 loss_print:  [array([9.217132], dtype=float32)]
step:  212 loss_print:  [array([9.230277], dtype=float32)]
step:  213 loss_print:  [array([8.898227], dtype=float32)]
step:  214 loss_print:  [array([9.24952], dtype=float32)]
step:  215 loss_print:  [array([8.936219], dtype=float32)]
step:  216 loss_print:  [array([9.194212], dtype=float32)]
step:  217 loss_print:  [array([9.04549], dtype=float32)]
step:  218 loss_print:  [array([9.3278055], dtype=float32)]
step:  219 loss_print:  [array([8.822807], dtype=float32)]
step:  220 loss_print:  [array([8.924669], dtype=float32)]
step:  221 loss_print:  [array([8.970063], dtype=float32)]
step:  222 loss_print:  [array([9.3867035], dtype=float32)]
step:  223 loss_print:  [array([9.194589], dtype=float32)]
step:  224 loss_print:  [array([8.995778], dtype=float32)]
step:  225 loss_print:  [array([9.02748], dtype=float32)]
step:  226 loss_print:  [array([9.022065], dtype=float32)]
step:  227 loss_print:  [array([9.448136], dtype=float32)]
step:  228 loss_print:  [array([9.131619], dtype=float32)]
step:  229 loss_print:  [array([8.927511], dtype=float32)]
step:  230 loss_print:  [array([8.995219], dtype=float32)]
step:  231 loss_print:  [array([8.927601], dtype=float32)]
step:  232 loss_print:  [array([9.114659], dtype=float32)]
step:  233 loss_print:  [array([9.429887], dtype=float32)]
step:  234 loss_print:  [array([8.829069], dtype=float32)]
step:  235 loss_print:  [array([8.899199], dtype=float32)]
step:  236 loss_print:  [array([8.853701], dtype=float32)]
step:  237 loss_print:  [array([8.646711], dtype=float32)]
step:  238 loss_print:  [array([9.0631075], dtype=float32)]
step:  239 loss_print:  [array([9.220059], dtype=float32)]
step:  240 loss_print:  [array([8.881758], dtype=float32)]
step:  241 loss_print:  [array([8.904568], dtype=float32)]
step:  242 loss_print:  [array([8.9197645], dtype=float32)]
step:  243 loss_print:  [array([8.701981], dtype=float32)]
step:  244 loss_print:  [array([8.814825], dtype=float32)]
step:  245 loss_print:  [array([9.381395], dtype=float32)]
step:  246 loss_print:  [array([9.244839], dtype=float32)]
step:  247 loss_print:  [array([9.026815], dtype=float32)]
step:  248 loss_print:  [array([8.87732], dtype=float32)]
step:  249 loss_print:  [array([8.999684], dtype=float32)]
step:  250 loss_print:  [array([9.26028], dtype=float32)]
step:  251 loss_print:  [array([9.016437], dtype=float32)]
step:  252 loss_print:  [array([8.980465], dtype=float32)]
step:  253 loss_print:  [array([8.793504], dtype=float32)]
step:  254 loss_print:  [array([9.119377], dtype=float32)]
step:  255 loss_print:  [array([9.073314], dtype=float32)]
step:  256 loss_print:  [array([9.214447], dtype=float32)]
step:  257 loss_print:  [array([8.770886], dtype=float32)]
step:  258 loss_print:  [array([8.900254], dtype=float32)]
step:  259 loss_print:  [array([8.998345], dtype=float32)]
step:  260 loss_print:  [array([9.001175], dtype=float32)]
step:  261 loss_print:  [array([8.969438], dtype=float32)]
step:  262 loss_print:  [array([8.877204], dtype=float32)]
step:  263 loss_print:  [array([8.624278], dtype=float32)]
step:  264 loss_print:  [array([8.754366], dtype=float32)]
step:  265 loss_print:  [array([9.021847], dtype=float32)]
step:  266 loss_print:  [array([8.758114], dtype=float32)]
step:  267 loss_print:  [array([8.622973], dtype=float32)]
step:  268 loss_print:  [array([8.594104], dtype=float32)]
step:  269 loss_print:  [array([9.143184], dtype=float32)]
step:  270 loss_print:  [array([8.675063], dtype=float32)]
step:  271 loss_print:  [array([9.015731], dtype=float32)]
step:  272 loss_print:  [array([8.884342], dtype=float32)]
step:  273 loss_print:  [array([8.95853], dtype=float32)]
step:  274 loss_print:  [array([8.956026], dtype=float32)]
step:  275 loss_print:  [array([9.150691], dtype=float32)]
step:  276 loss_print:  [array([9.254313], dtype=float32)]
step:  277 loss_print:  [array([8.737851], dtype=float32)]
step:  278 loss_print:  [array([8.931541], dtype=float32)]
step:  279 loss_print:  [array([8.939543], dtype=float32)]
step:  280 loss_print:  [array([8.72931], dtype=float32)]
step:  281 loss_print:  [array([8.8316], dtype=float32)]
step:  282 loss_print:  [array([8.864128], dtype=float32)]
step:  283 loss_print:  [array([9.264008], dtype=float32)]
step:  284 loss_print:  [array([8.832067], dtype=float32)]
step:  285 loss_print:  [array([9.1787195], dtype=float32)]
step:  286 loss_print:  [array([9.047214], dtype=float32)]
step:  287 loss_print:  [array([8.842916], dtype=float32)]
step:  288 loss_print:  [array([8.795553], dtype=float32)]
step:  289 loss_print:  [array([8.808148], dtype=float32)]
step:  290 loss_print:  [array([8.711977], dtype=float32)]
step:  291 loss_print:  [array([8.982601], dtype=float32)]
step:  292 loss_print:  [array([8.735425], dtype=float32)]
step:  293 loss_print:  [array([8.721466], dtype=float32)]
step:  294 loss_print:  [array([8.933556], dtype=float32)]
step:  295 loss_print:  [array([9.065833], dtype=float32)]
step:  296 loss_print:  [array([8.772276], dtype=float32)]
step:  297 loss_print:  [array([8.689801], dtype=float32)]
step:  298 loss_print:  [array([9.120382], dtype=float32)]
step:  299 loss_print:  [array([8.786987], dtype=float32)]
step:  300 loss_print:  [array([8.986396], dtype=float32)]
step:  301 loss_print:  [array([9.164496], dtype=float32)]


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::framework::ParallelExecutor::Run(std::vector<std::string, std::allocator<std::string > > const&, bool)
1   paddle::framework::details::ScopeBufferedSSAGraphExecutor::Run(std::vector<std::string, std::allocator<std::string > > const&, bool)
2   paddle::framework::details::ScopeBufferedSSAGraphExecutor::DropLocalExeScopes(bool)
3   paddle::platform::CUDADeviceContext::Wait() const
4   paddle::platform::stream::CUDAStream::Wait() const

----------------------
Error Message Summary:
----------------------
FatalError: `Termination signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1636545829 (unix time) try "date -d @1636545829" if you are using GNU date ***]
  [SignalInfo: *** SIGTERM (@0x7f84) received by PID 32799 (TID 0x7fe094ad7740) from PID 32644 ***]

