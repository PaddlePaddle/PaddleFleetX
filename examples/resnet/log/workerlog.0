Start collective training example:
Run on CUDAPlace(0).
train_fleet_static.py:50: DeprecationWarning: [93m
Warning:
API "paddle.dataset.flowers.train" is deprecated since 2.0.0, and will be removed in future versions. Please use "paddle.vision.datasets.Flowers" instead.
reason: Please use new dataset API which supports paddle.io.DataLoader [0m
  reader_decorator(paddle.dataset.flowers.train(use_xmap=True)),
server not ready, wait 3 sec to retry...
not ready endpoints:['127.0.0.1:12015']
server not ready, wait 3 sec to retry...
not ready endpoints:['127.0.0.1:12015']
Execute startup program.
W0909 12:26:05.553128 33462 device_context.cc:446] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 10.1
W0909 12:26:05.558284 33462 device_context.cc:464] device: 0, cuDNN Version: 7.6.
Start training:
Warning with paddle image module: opencv-python should be imported,
         or paddle image module could NOT work; please install opencv-python first.Warning with paddle image module: opencv-python should be imported,
         or paddle image module could NOT work; please install opencv-python first.Warning with paddle image module: opencv-python should be imported,
         or paddle image module could NOT work; please install opencv-python first.Exception in thread Thread-4:
Traceback (most recent call last):
  File "/opt/conda/envs/py37/lib/python3.7/threading.py", line 926, in _bootstrap_inner
    self.run()
  File "/opt/conda/envs/py37/lib/python3.7/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/opt/conda/envs/py37/lib/python3.7/site-packages/paddle/reader/decorator.py", line 448, in handle_worker
    r = mapper(sample)
  File "/opt/conda/envs/py37/lib/python3.7/site-packages/paddle/dataset/flowers.py", line 75, in default_mapper
    img = load_image_bytes(img)
  File "/opt/conda/envs/py37/lib/python3.7/site-packages/paddle/dataset/image.py", line 159, in load_image_bytes
    assert _check_cv2() is True
AssertionError
Exception in thread Thread-3:
Traceback (most recent call last):
  File "/opt/conda/envs/py37/lib/python3.7/threading.py", line 926, in _bootstrap_inner
    self.run()
  File "/opt/conda/envs/py37/lib/python3.7/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/opt/conda/envs/py37/lib/python3.7/site-packages/paddle/reader/decorator.py", line 448, in handle_worker
    r = mapper(sample)
  File "/opt/conda/envs/py37/lib/python3.7/site-packages/paddle/dataset/flowers.py", line 75, in default_mapper
    img = load_image_bytes(img)
  File "/opt/conda/envs/py37/lib/python3.7/site-packages/paddle/dataset/image.py", line 159, in load_image_bytes
    assert _check_cv2() is True
AssertionError

Exception in thread Thread-5:
Traceback (most recent call last):
  File "/opt/conda/envs/py37/lib/python3.7/threading.py", line 926, in _bootstrap_inner
    self.run()
  File "/opt/conda/envs/py37/lib/python3.7/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/opt/conda/envs/py37/lib/python3.7/site-packages/paddle/reader/decorator.py", line 448, in handle_worker
    r = mapper(sample)
  File "/opt/conda/envs/py37/lib/python3.7/site-packages/paddle/dataset/flowers.py", line 75, in default_mapper
    img = load_image_bytes(img)
  File "/opt/conda/envs/py37/lib/python3.7/site-packages/paddle/dataset/image.py", line 159, in load_image_bytes
    assert _check_cv2() is True
AssertionError


Warning with paddle image module: opencv-python should be imported,
         or paddle image module could NOT work; please install opencv-python first.Exception in thread Thread-6:
Traceback (most recent call last):
  File "/opt/conda/envs/py37/lib/python3.7/threading.py", line 926, in _bootstrap_inner
    self.run()
  File "/opt/conda/envs/py37/lib/python3.7/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/opt/conda/envs/py37/lib/python3.7/site-packages/paddle/reader/decorator.py", line 448, in handle_worker
    r = mapper(sample)
  File "/opt/conda/envs/py37/lib/python3.7/site-packages/paddle/dataset/flowers.py", line 75, in default_mapper
    img = load_image_bytes(img)
  File "/opt/conda/envs/py37/lib/python3.7/site-packages/paddle/dataset/image.py", line 159, in load_image_bytes
    assert _check_cv2() is True
AssertionError

Start collective training example:
Run on CUDAPlace(0).
train_fleet_static.py:50: DeprecationWarning: [93m
Warning:
API "paddle.dataset.flowers.train" is deprecated since 2.0.0, and will be removed in future versions. Please use "paddle.vision.datasets.Flowers" instead.
reason: Please use new dataset API which supports paddle.io.DataLoader [0m
  reader_decorator(paddle.dataset.flowers.train(use_xmap=True)),
server not ready, wait 3 sec to retry...
not ready endpoints:['127.0.0.1:10524']
server not ready, wait 3 sec to retry...
not ready endpoints:['127.0.0.1:10524']
Execute startup program.
W0909 12:27:07.163818 35657 device_context.cc:446] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 10.1
W0909 12:27:07.169871 35657 device_context.cc:464] device: 0, cuDNN Version: 7.6.
Start training:
W0909 12:27:15.064397 35657 fuse_all_reduce_op_pass.cc:76] Find all_reduce operators: 161. To make the speed faster, some all_reduce ops are fused during training, after fusion, the number of all_reduce ops is 5.
[Epoch 0, batch 0] loss: 4.76081, acc1: 0.00000, acc5: 0.03125
[Epoch 0, batch 5] loss: 29.12006, acc1: 0.03125, acc5: 0.09375
[Epoch 0, batch 10] loss: 7.73320, acc1: 0.09375, acc5: 0.09375
[Epoch 0, batch 15] loss: 8.92840, acc1: 0.03125, acc5: 0.03125
[Epoch 0, batch 20] loss: 11.25310, acc1: 0.00000, acc5: 0.06250
[Epoch 0, batch 25] loss: 5.81487, acc1: 0.03125, acc5: 0.12500
[Epoch 0, batch 30] loss: 6.21761, acc1: 0.06250, acc5: 0.06250
[Epoch 0, batch 35] loss: 6.80674, acc1: 0.00000, acc5: 0.15625
[Epoch 0, batch 40] loss: 4.57257, acc1: 0.00000, acc5: 0.09375
[Epoch 0, batch 45] loss: 4.58274, acc1: 0.09375, acc5: 0.25000
[Epoch 0, batch 50] loss: 4.62952, acc1: 0.00000, acc5: 0.06250
[Epoch 0, batch 55] loss: 4.47345, acc1: 0.03125, acc5: 0.18750
[Epoch 0, batch 60] loss: 4.52977, acc1: 0.06250, acc5: 0.09375
[Epoch 0, batch 65] loss: 4.47920, acc1: 0.00000, acc5: 0.09375
[Epoch 0, batch 70] loss: 4.56800, acc1: 0.00000, acc5: 0.18750
[Epoch 0, batch 75] loss: 4.48898, acc1: 0.06250, acc5: 0.15625
[Epoch 0, batch 80] loss: 4.50266, acc1: 0.00000, acc5: 0.12500
[Epoch 0, batch 85] loss: 4.48243, acc1: 0.03125, acc5: 0.12500
[Epoch 0, batch 90] loss: 4.46682, acc1: 0.03125, acc5: 0.12500
[Epoch 0, batch 95] loss: 4.99084, acc1: 0.00000, acc5: 0.06250
[Epoch 0, batch 100] loss: 4.62719, acc1: 0.00000, acc5: 0.06250
[Epoch 0, batch 105] loss: 4.39851, acc1: 0.06250, acc5: 0.21875
[Epoch 0, batch 110] loss: 4.45007, acc1: 0.03125, acc5: 0.09375
[Epoch 0, batch 115] loss: 4.34060, acc1: 0.06250, acc5: 0.21875
[Epoch 0, batch 120] loss: 4.39176, acc1: 0.00000, acc5: 0.15625
[Epoch 0, batch 125] loss: 4.36639, acc1: 0.06250, acc5: 0.21875
[Epoch 0, batch 130] loss: 4.31812, acc1: 0.09375, acc5: 0.25000
[Epoch 0, batch 135] loss: 4.58535, acc1: 0.06250, acc5: 0.09375
[Epoch 0, batch 140] loss: 4.34694, acc1: 0.12500, acc5: 0.21875
[Epoch 0, batch 145] loss: 4.20126, acc1: 0.09375, acc5: 0.28125
[Epoch 0, batch 150] loss: 4.45618, acc1: 0.03125, acc5: 0.15625
[Epoch 0, batch 155] loss: 4.53311, acc1: 0.06250, acc5: 0.12500
[Epoch 0, batch 160] loss: 4.38895, acc1: 0.00000, acc5: 0.12500
[Epoch 0, batch 165] loss: 4.18170, acc1: 0.06250, acc5: 0.15625
[Epoch 0, batch 170] loss: 4.53141, acc1: 0.00000, acc5: 0.06250
[Epoch 0, batch 175] loss: 4.29896, acc1: 0.12500, acc5: 0.25000
[Epoch 0, batch 180] loss: 4.44358, acc1: 0.00000, acc5: 0.15625
[Epoch 0, batch 185] loss: 4.27534, acc1: 0.03125, acc5: 0.21875
[Epoch 0, batch 190] loss: 4.37511, acc1: 0.06250, acc5: 0.21875
[Epoch 1, batch 0] loss: 4.38621, acc1: 0.06250, acc5: 0.12500
[Epoch 1, batch 5] loss: 4.55589, acc1: 0.00000, acc5: 0.12500
[Epoch 1, batch 10] loss: 4.40983, acc1: 0.03125, acc5: 0.15625
[Epoch 1, batch 15] loss: 4.19693, acc1: 0.09375, acc5: 0.28125
[Epoch 1, batch 20] loss: 4.36282, acc1: 0.06250, acc5: 0.18750
[Epoch 1, batch 25] loss: 4.18302, acc1: 0.03125, acc5: 0.21875
[Epoch 1, batch 30] loss: 4.35530, acc1: 0.06250, acc5: 0.25000
[Epoch 1, batch 35] loss: 4.35798, acc1: 0.00000, acc5: 0.25000
[Epoch 1, batch 40] loss: 4.59267, acc1: 0.00000, acc5: 0.12500
[Epoch 1, batch 45] loss: 4.07716, acc1: 0.12500, acc5: 0.34375
[Epoch 1, batch 50] loss: 4.58414, acc1: 0.03125, acc5: 0.06250
[Epoch 1, batch 55] loss: 4.26793, acc1: 0.03125, acc5: 0.15625
[Epoch 1, batch 60] loss: 4.32577, acc1: 0.12500, acc5: 0.21875
[Epoch 1, batch 65] loss: 4.17436, acc1: 0.09375, acc5: 0.18750
[Epoch 1, batch 70] loss: 4.45380, acc1: 0.03125, acc5: 0.21875
[Epoch 1, batch 75] loss: 4.32471, acc1: 0.03125, acc5: 0.21875
[Epoch 1, batch 80] loss: 4.35944, acc1: 0.00000, acc5: 0.18750
[Epoch 1, batch 85] loss: 4.47812, acc1: 0.06250, acc5: 0.12500
[Epoch 1, batch 90] loss: 4.21526, acc1: 0.06250, acc5: 0.12500
[Epoch 1, batch 95] loss: 4.42699, acc1: 0.06250, acc5: 0.15625
[Epoch 1, batch 100] loss: 4.54131, acc1: 0.00000, acc5: 0.09375
[Epoch 1, batch 105] loss: 4.20667, acc1: 0.06250, acc5: 0.21875
[Epoch 1, batch 110] loss: 4.25998, acc1: 0.06250, acc5: 0.18750
[Epoch 1, batch 115] loss: 4.18189, acc1: 0.09375, acc5: 0.25000
[Epoch 1, batch 120] loss: 4.18588, acc1: 0.09375, acc5: 0.15625
[Epoch 1, batch 125] loss: 4.08349, acc1: 0.09375, acc5: 0.37500
[Epoch 1, batch 130] loss: 4.28844, acc1: 0.12500, acc5: 0.25000
[Epoch 1, batch 135] loss: 4.55645, acc1: 0.06250, acc5: 0.09375
[Epoch 1, batch 140] loss: 4.21348, acc1: 0.12500, acc5: 0.28125
[Epoch 1, batch 145] loss: 4.11224, acc1: 0.03125, acc5: 0.25000
[Epoch 1, batch 150] loss: 4.41855, acc1: 0.03125, acc5: 0.15625
[Epoch 1, batch 155] loss: 4.32250, acc1: 0.06250, acc5: 0.18750
[Epoch 1, batch 160] loss: 4.16201, acc1: 0.03125, acc5: 0.34375
[Epoch 1, batch 165] loss: 4.03905, acc1: 0.00000, acc5: 0.28125
[Epoch 1, batch 170] loss: 4.30996, acc1: 0.06250, acc5: 0.18750
[Epoch 1, batch 175] loss: 4.09905, acc1: 0.06250, acc5: 0.28125
[Epoch 1, batch 180] loss: 4.31505, acc1: 0.00000, acc5: 0.15625
[Epoch 1, batch 185] loss: 3.97415, acc1: 0.09375, acc5: 0.28125
[Epoch 1, batch 190] loss: 4.27381, acc1: 0.03125, acc5: 0.18750
[Epoch 2, batch 0] loss: 4.22433, acc1: 0.09375, acc5: 0.31250
[Epoch 2, batch 5] loss: 4.38164, acc1: 0.03125, acc5: 0.09375
[Epoch 2, batch 10] loss: 4.20264, acc1: 0.06250, acc5: 0.21875
[Epoch 2, batch 15] loss: 3.74761, acc1: 0.12500, acc5: 0.40625
[Epoch 2, batch 20] loss: 4.19674, acc1: 0.06250, acc5: 0.21875
[Epoch 2, batch 25] loss: 4.02171, acc1: 0.03125, acc5: 0.31250
[Epoch 2, batch 30] loss: 4.25248, acc1: 0.06250, acc5: 0.25000
[Epoch 2, batch 35] loss: 4.20400, acc1: 0.06250, acc5: 0.25000
[Epoch 2, batch 40] loss: 4.46420, acc1: 0.03125, acc5: 0.12500
[Epoch 2, batch 45] loss: 3.97381, acc1: 0.09375, acc5: 0.34375
[Epoch 2, batch 50] loss: 4.41999, acc1: 0.03125, acc5: 0.09375
[Epoch 2, batch 55] loss: 3.84537, acc1: 0.15625, acc5: 0.34375
[Epoch 2, batch 60] loss: 4.27915, acc1: 0.06250, acc5: 0.15625
[Epoch 2, batch 65] loss: 3.93539, acc1: 0.00000, acc5: 0.28125
[Epoch 2, batch 70] loss: 4.21383, acc1: 0.09375, acc5: 0.31250
[Epoch 2, batch 75] loss: 4.07745, acc1: 0.00000, acc5: 0.28125
[Epoch 2, batch 80] loss: 4.03923, acc1: 0.09375, acc5: 0.25000
[Epoch 2, batch 85] loss: 4.37164, acc1: 0.06250, acc5: 0.12500
[Epoch 2, batch 90] loss: 4.02129, acc1: 0.03125, acc5: 0.15625
[Epoch 2, batch 95] loss: 4.14857, acc1: 0.06250, acc5: 0.21875
[Epoch 2, batch 100] loss: 4.40543, acc1: 0.00000, acc5: 0.12500
[Epoch 2, batch 105] loss: 4.01466, acc1: 0.12500, acc5: 0.25000
[Epoch 2, batch 110] loss: 4.01118, acc1: 0.03125, acc5: 0.25000
[Epoch 2, batch 115] loss: 4.05418, acc1: 0.06250, acc5: 0.25000
[Epoch 2, batch 120] loss: 3.84000, acc1: 0.12500, acc5: 0.25000
[Epoch 2, batch 125] loss: 3.80431, acc1: 0.12500, acc5: 0.40625
[Epoch 2, batch 130] loss: 3.84194, acc1: 0.21875, acc5: 0.28125
[Epoch 2, batch 135] loss: 4.38638, acc1: 0.09375, acc5: 0.12500
[Epoch 2, batch 140] loss: 4.05707, acc1: 0.15625, acc5: 0.25000
[Epoch 2, batch 145] loss: 3.95494, acc1: 0.12500, acc5: 0.28125
[Epoch 2, batch 150] loss: 4.34753, acc1: 0.06250, acc5: 0.28125
[Epoch 2, batch 155] loss: 4.12836, acc1: 0.06250, acc5: 0.18750
[Epoch 2, batch 160] loss: 4.15590, acc1: 0.09375, acc5: 0.21875
[Epoch 2, batch 165] loss: 3.84892, acc1: 0.09375, acc5: 0.40625
[Epoch 2, batch 170] loss: 4.08465, acc1: 0.03125, acc5: 0.25000
[Epoch 2, batch 175] loss: 3.72135, acc1: 0.12500, acc5: 0.37500
[Epoch 2, batch 180] loss: 4.25790, acc1: 0.03125, acc5: 0.21875
[Epoch 2, batch 185] loss: 3.99140, acc1: 0.09375, acc5: 0.37500
[Epoch 2, batch 190] loss: 3.97054, acc1: 0.03125, acc5: 0.25000
[Epoch 3, batch 0] loss: 4.13338, acc1: 0.12500, acc5: 0.37500
[Epoch 3, batch 5] loss: 4.19850, acc1: 0.00000, acc5: 0.12500
[Epoch 3, batch 10] loss: 4.04662, acc1: 0.12500, acc5: 0.25000
[Epoch 3, batch 15] loss: 3.42389, acc1: 0.09375, acc5: 0.56250
[Epoch 3, batch 20] loss: 3.97483, acc1: 0.00000, acc5: 0.21875
[Epoch 3, batch 25] loss: 3.74524, acc1: 0.09375, acc5: 0.34375
[Epoch 3, batch 30] loss: 4.09307, acc1: 0.12500, acc5: 0.34375
[Epoch 3, batch 35] loss: 3.93361, acc1: 0.03125, acc5: 0.34375
[Epoch 3, batch 40] loss: 4.35490, acc1: 0.06250, acc5: 0.18750
[Epoch 3, batch 45] loss: 3.72060, acc1: 0.12500, acc5: 0.40625
[Epoch 3, batch 50] loss: 4.28734, acc1: 0.00000, acc5: 0.12500
[Epoch 3, batch 55] loss: 4.00430, acc1: 0.03125, acc5: 0.28125
[Epoch 3, batch 60] loss: 4.03730, acc1: 0.03125, acc5: 0.21875
[Epoch 3, batch 65] loss: 3.57822, acc1: 0.15625, acc5: 0.46875
[Epoch 3, batch 70] loss: 4.07834, acc1: 0.09375, acc5: 0.31250
[Epoch 3, batch 75] loss: 3.92725, acc1: 0.09375, acc5: 0.21875
[Epoch 3, batch 80] loss: 3.65105, acc1: 0.15625, acc5: 0.37500
[Epoch 3, batch 85] loss: 3.94057, acc1: 0.12500, acc5: 0.18750
[Epoch 3, batch 90] loss: 3.85680, acc1: 0.09375, acc5: 0.40625
[Epoch 3, batch 95] loss: 3.72599, acc1: 0.15625, acc5: 0.37500
[Epoch 3, batch 100] loss: 4.20813, acc1: 0.00000, acc5: 0.18750
[Epoch 3, batch 105] loss: 3.83245, acc1: 0.09375, acc5: 0.31250
[Epoch 3, batch 110] loss: 3.92007, acc1: 0.06250, acc5: 0.21875
[Epoch 3, batch 115] loss: 3.97800, acc1: 0.09375, acc5: 0.25000
[Epoch 3, batch 120] loss: 3.83825, acc1: 0.15625, acc5: 0.21875
[Epoch 3, batch 125] loss: 3.66607, acc1: 0.09375, acc5: 0.34375
[Epoch 3, batch 130] loss: 3.79655, acc1: 0.15625, acc5: 0.31250
[Epoch 3, batch 135] loss: 4.13124, acc1: 0.09375, acc5: 0.25000
[Epoch 3, batch 140] loss: 3.92853, acc1: 0.12500, acc5: 0.28125
[Epoch 3, batch 145] loss: 3.95891, acc1: 0.06250, acc5: 0.28125
[Epoch 3, batch 150] loss: 4.26638, acc1: 0.09375, acc5: 0.18750
[Epoch 3, batch 155] loss: 4.05437, acc1: 0.00000, acc5: 0.21875
[Epoch 3, batch 160] loss: 3.98624, acc1: 0.09375, acc5: 0.31250
[Epoch 3, batch 165] loss: 3.73015, acc1: 0.09375, acc5: 0.40625
[Epoch 3, batch 170] loss: 3.91997, acc1: 0.03125, acc5: 0.25000
[Epoch 3, batch 175] loss: 3.60004, acc1: 0.15625, acc5: 0.43750
[Epoch 3, batch 180] loss: 4.12380, acc1: 0.03125, acc5: 0.18750
[Epoch 3, batch 185] loss: 3.90689, acc1: 0.09375, acc5: 0.37500
[Epoch 3, batch 190] loss: 3.75884, acc1: 0.09375, acc5: 0.31250
[Epoch 4, batch 0] loss: 4.15805, acc1: 0.09375, acc5: 0.53125
[Epoch 4, batch 5] loss: 3.94881, acc1: 0.06250, acc5: 0.31250
[Epoch 4, batch 10] loss: 3.90281, acc1: 0.09375, acc5: 0.28125
[Epoch 4, batch 15] loss: 3.30460, acc1: 0.12500, acc5: 0.62500
[Epoch 4, batch 20] loss: 3.99852, acc1: 0.06250, acc5: 0.15625
[Epoch 4, batch 25] loss: 3.57605, acc1: 0.06250, acc5: 0.43750
[Epoch 4, batch 30] loss: 3.81410, acc1: 0.12500, acc5: 0.31250
[Epoch 4, batch 35] loss: 3.95016, acc1: 0.06250, acc5: 0.37500
[Epoch 4, batch 40] loss: 4.14800, acc1: 0.06250, acc5: 0.18750
[Epoch 4, batch 45] loss: 3.69213, acc1: 0.09375, acc5: 0.40625
[Epoch 4, batch 50] loss: 4.06278, acc1: 0.09375, acc5: 0.21875
[Epoch 4, batch 55] loss: 3.72740, acc1: 0.09375, acc5: 0.40625
[Epoch 4, batch 60] loss: 3.76617, acc1: 0.15625, acc5: 0.21875
[Epoch 4, batch 65] loss: 3.30285, acc1: 0.15625, acc5: 0.50000
[Epoch 4, batch 70] loss: 3.81430, acc1: 0.12500, acc5: 0.34375
[Epoch 4, batch 75] loss: 3.84170, acc1: 0.09375, acc5: 0.34375
[Epoch 4, batch 80] loss: 3.48115, acc1: 0.18750, acc5: 0.40625
[Epoch 4, batch 85] loss: 3.99659, acc1: 0.12500, acc5: 0.21875
[Epoch 4, batch 90] loss: 3.65384, acc1: 0.00000, acc5: 0.43750
[Epoch 4, batch 95] loss: 3.67356, acc1: 0.12500, acc5: 0.37500
[Epoch 4, batch 100] loss: 4.13561, acc1: 0.00000, acc5: 0.25000
[Epoch 4, batch 105] loss: 3.71630, acc1: 0.09375, acc5: 0.34375
[Epoch 4, batch 110] loss: 3.66864, acc1: 0.12500, acc5: 0.31250
[Epoch 4, batch 115] loss: 3.80353, acc1: 0.06250, acc5: 0.37500
[Epoch 4, batch 120] loss: 3.68882, acc1: 0.15625, acc5: 0.34375
[Epoch 4, batch 125] loss: 3.56283, acc1: 0.18750, acc5: 0.46875
[Epoch 4, batch 130] loss: 3.64238, acc1: 0.18750, acc5: 0.37500
[Epoch 4, batch 135] loss: 3.95058, acc1: 0.15625, acc5: 0.28125
[Epoch 4, batch 140] loss: 3.93319, acc1: 0.12500, acc5: 0.25000
[Epoch 4, batch 145] loss: 3.80021, acc1: 0.03125, acc5: 0.31250
[Epoch 4, batch 150] loss: 3.86052, acc1: 0.12500, acc5: 0.37500
[Epoch 4, batch 155] loss: 3.94071, acc1: 0.12500, acc5: 0.21875
[Epoch 4, batch 160] loss: 3.61978, acc1: 0.15625, acc5: 0.43750
[Epoch 4, batch 165] loss: 3.56203, acc1: 0.18750, acc5: 0.31250
[Epoch 4, batch 170] loss: 3.77618, acc1: 0.06250, acc5: 0.37500
[Epoch 4, batch 175] loss: 3.35684, acc1: 0.18750, acc5: 0.53125
[Epoch 4, batch 180] loss: 4.00905, acc1: 0.09375, acc5: 0.25000
[Epoch 4, batch 185] loss: 3.71881, acc1: 0.12500, acc5: 0.37500
[Epoch 4, batch 190] loss: 3.57977, acc1: 0.06250, acc5: 0.31250
[Epoch 5, batch 0] loss: 3.88445, acc1: 0.18750, acc5: 0.46875
[Epoch 5, batch 5] loss: 3.86542, acc1: 0.06250, acc5: 0.34375
[Epoch 5, batch 10] loss: 3.65130, acc1: 0.12500, acc5: 0.34375
[Epoch 5, batch 15] loss: 3.31405, acc1: 0.12500, acc5: 0.53125
[Epoch 5, batch 20] loss: 3.61917, acc1: 0.12500, acc5: 0.40625
[Epoch 5, batch 25] loss: 3.36865, acc1: 0.09375, acc5: 0.43750
[Epoch 5, batch 30] loss: 3.58431, acc1: 0.15625, acc5: 0.46875
[Epoch 5, batch 35] loss: 3.68165, acc1: 0.09375, acc5: 0.28125
[Epoch 5, batch 40] loss: 3.85143, acc1: 0.09375, acc5: 0.25000
[Epoch 5, batch 45] loss: 3.47687, acc1: 0.09375, acc5: 0.46875
[Epoch 5, batch 50] loss: 3.80401, acc1: 0.06250, acc5: 0.21875
[Epoch 5, batch 55] loss: 3.49722, acc1: 0.15625, acc5: 0.37500
[Epoch 5, batch 60] loss: 3.61564, acc1: 0.15625, acc5: 0.34375
[Epoch 5, batch 65] loss: 3.45193, acc1: 0.25000, acc5: 0.40625
[Epoch 5, batch 70] loss: 3.98524, acc1: 0.06250, acc5: 0.21875
[Epoch 5, batch 75] loss: 3.87175, acc1: 0.15625, acc5: 0.31250
[Epoch 5, batch 80] loss: 3.24047, acc1: 0.21875, acc5: 0.46875
[Epoch 5, batch 85] loss: 3.88085, acc1: 0.12500, acc5: 0.21875
[Epoch 5, batch 90] loss: 3.56343, acc1: 0.06250, acc5: 0.50000
[Epoch 5, batch 95] loss: 3.55749, acc1: 0.15625, acc5: 0.37500
[Epoch 5, batch 100] loss: 4.11587, acc1: 0.03125, acc5: 0.31250
[Epoch 5, batch 105] loss: 3.57669, acc1: 0.12500, acc5: 0.43750
[Epoch 5, batch 110] loss: 3.52437, acc1: 0.12500, acc5: 0.34375
[Epoch 5, batch 115] loss: 3.72095, acc1: 0.06250, acc5: 0.31250
[Epoch 5, batch 120] loss: 3.45956, acc1: 0.18750, acc5: 0.28125
[Epoch 5, batch 125] loss: 3.50306, acc1: 0.15625, acc5: 0.43750
[Epoch 5, batch 130] loss: 3.63469, acc1: 0.12500, acc5: 0.46875
[Epoch 5, batch 135] loss: 3.75478, acc1: 0.18750, acc5: 0.31250
[Epoch 5, batch 140] loss: 3.61046, acc1: 0.12500, acc5: 0.34375
[Epoch 5, batch 145] loss: 3.72867, acc1: 0.09375, acc5: 0.31250
[Epoch 5, batch 150] loss: 3.99326, acc1: 0.06250, acc5: 0.28125
[Epoch 5, batch 155] loss: 3.88855, acc1: 0.03125, acc5: 0.28125
[Epoch 5, batch 160] loss: 3.50505, acc1: 0.18750, acc5: 0.43750
[Epoch 5, batch 165] loss: 3.51013, acc1: 0.15625, acc5: 0.37500
[Epoch 5, batch 170] loss: 3.67905, acc1: 0.06250, acc5: 0.34375
[Epoch 5, batch 175] loss: 3.33364, acc1: 0.18750, acc5: 0.46875
[Epoch 5, batch 180] loss: 4.04502, acc1: 0.06250, acc5: 0.25000
[Epoch 5, batch 185] loss: 3.62180, acc1: 0.18750, acc5: 0.43750
[Epoch 5, batch 190] loss: 3.47630, acc1: 0.09375, acc5: 0.28125
[Epoch 6, batch 0] loss: 3.90427, acc1: 0.15625, acc5: 0.40625
[Epoch 6, batch 5] loss: 3.91866, acc1: 0.06250, acc5: 0.25000
[Epoch 6, batch 10] loss: 3.34393, acc1: 0.18750, acc5: 0.46875
[Epoch 6, batch 15] loss: 3.07819, acc1: 0.15625, acc5: 0.53125
[Epoch 6, batch 20] loss: 3.39254, acc1: 0.09375, acc5: 0.46875
[Epoch 6, batch 25] loss: 3.28544, acc1: 0.09375, acc5: 0.43750
[Epoch 6, batch 30] loss: 3.38146, acc1: 0.12500, acc5: 0.46875
[Epoch 6, batch 35] loss: 3.62649, acc1: 0.09375, acc5: 0.34375
[Epoch 6, batch 40] loss: 3.61788, acc1: 0.06250, acc5: 0.37500
[Epoch 6, batch 45] loss: 3.30984, acc1: 0.18750, acc5: 0.50000
[Epoch 6, batch 50] loss: 3.76206, acc1: 0.06250, acc5: 0.18750
[Epoch 6, batch 55] loss: 3.41700, acc1: 0.12500, acc5: 0.43750
[Epoch 6, batch 60] loss: 3.55356, acc1: 0.21875, acc5: 0.34375
[Epoch 6, batch 65] loss: 3.31252, acc1: 0.21875, acc5: 0.46875
[Epoch 6, batch 70] loss: 3.84267, acc1: 0.15625, acc5: 0.31250
[Epoch 6, batch 75] loss: 3.57609, acc1: 0.09375, acc5: 0.31250
[Epoch 6, batch 80] loss: 3.26146, acc1: 0.18750, acc5: 0.46875
[Epoch 6, batch 85] loss: 3.61843, acc1: 0.18750, acc5: 0.31250
[Epoch 6, batch 90] loss: 3.51269, acc1: 0.03125, acc5: 0.37500
[Epoch 6, batch 95] loss: 3.29856, acc1: 0.21875, acc5: 0.43750
[Epoch 6, batch 100] loss: 4.09720, acc1: 0.06250, acc5: 0.28125
[Epoch 6, batch 105] loss: 3.49840, acc1: 0.09375, acc5: 0.31250
[Epoch 6, batch 110] loss: 3.48534, acc1: 0.18750, acc5: 0.43750
[Epoch 6, batch 115] loss: 3.59473, acc1: 0.06250, acc5: 0.37500
[Epoch 6, batch 120] loss: 3.34467, acc1: 0.21875, acc5: 0.37500
[Epoch 6, batch 125] loss: 3.08290, acc1: 0.28125, acc5: 0.56250
[Epoch 6, batch 130] loss: 3.28900, acc1: 0.12500, acc5: 0.53125
[Epoch 6, batch 135] loss: 3.54204, acc1: 0.15625, acc5: 0.31250
[Epoch 6, batch 140] loss: 3.63134, acc1: 0.12500, acc5: 0.31250
[Epoch 6, batch 145] loss: 3.61237, acc1: 0.12500, acc5: 0.37500
[Epoch 6, batch 150] loss: 3.85586, acc1: 0.06250, acc5: 0.28125
[Epoch 6, batch 155] loss: 3.71041, acc1: 0.15625, acc5: 0.31250
[Epoch 6, batch 160] loss: 3.36991, acc1: 0.18750, acc5: 0.50000
[Epoch 6, batch 165] loss: 3.47389, acc1: 0.18750, acc5: 0.34375
[Epoch 6, batch 170] loss: 3.56685, acc1: 0.06250, acc5: 0.31250
[Epoch 6, batch 175] loss: 3.13036, acc1: 0.18750, acc5: 0.50000
[Epoch 6, batch 180] loss: 3.91616, acc1: 0.06250, acc5: 0.21875
[Epoch 6, batch 185] loss: 3.70104, acc1: 0.06250, acc5: 0.40625
[Epoch 6, batch 190] loss: 3.24730, acc1: 0.09375, acc5: 0.40625
[Epoch 7, batch 0] loss: 3.55897, acc1: 0.25000, acc5: 0.50000
[Epoch 7, batch 5] loss: 3.83611, acc1: 0.12500, acc5: 0.25000
[Epoch 7, batch 10] loss: 3.19198, acc1: 0.18750, acc5: 0.46875
[Epoch 7, batch 15] loss: 2.92867, acc1: 0.21875, acc5: 0.62500
[Epoch 7, batch 20] loss: 3.33656, acc1: 0.12500, acc5: 0.34375
[Epoch 7, batch 25] loss: 3.05848, acc1: 0.18750, acc5: 0.59375
[Epoch 7, batch 30] loss: 3.30485, acc1: 0.18750, acc5: 0.53125
[Epoch 7, batch 35] loss: 3.58934, acc1: 0.06250, acc5: 0.43750
[Epoch 7, batch 40] loss: 3.72359, acc1: 0.03125, acc5: 0.34375
[Epoch 7, batch 45] loss: 3.33207, acc1: 0.15625, acc5: 0.46875
[Epoch 7, batch 50] loss: 3.62834, acc1: 0.09375, acc5: 0.25000
[Epoch 7, batch 55] loss: 3.29221, acc1: 0.21875, acc5: 0.53125
[Epoch 7, batch 60] loss: 3.26970, acc1: 0.15625, acc5: 0.40625
[Epoch 7, batch 65] loss: 3.36437, acc1: 0.18750, acc5: 0.40625
[Epoch 7, batch 70] loss: 3.67173, acc1: 0.15625, acc5: 0.37500
[Epoch 7, batch 75] loss: 3.27533, acc1: 0.21875, acc5: 0.43750
[Epoch 7, batch 80] loss: 3.10522, acc1: 0.28125, acc5: 0.50000
[Epoch 7, batch 85] loss: 3.58637, acc1: 0.18750, acc5: 0.37500
[Epoch 7, batch 90] loss: 3.33788, acc1: 0.06250, acc5: 0.46875
[Epoch 7, batch 95] loss: 3.33384, acc1: 0.18750, acc5: 0.46875
[Epoch 7, batch 100] loss: 3.99540, acc1: 0.06250, acc5: 0.34375
[Epoch 7, batch 105] loss: 3.31153, acc1: 0.15625, acc5: 0.37500
[Epoch 7, batch 110] loss: 3.38466, acc1: 0.18750, acc5: 0.34375
[Epoch 7, batch 115] loss: 3.45066, acc1: 0.12500, acc5: 0.37500
[Epoch 7, batch 120] loss: 3.33553, acc1: 0.25000, acc5: 0.43750
[Epoch 7, batch 125] loss: 3.28404, acc1: 0.18750, acc5: 0.50000
[Epoch 7, batch 130] loss: 3.06719, acc1: 0.21875, acc5: 0.62500
[Epoch 7, batch 135] loss: 3.48424, acc1: 0.21875, acc5: 0.37500
[Epoch 7, batch 140] loss: 3.50835, acc1: 0.15625, acc5: 0.37500
[Epoch 7, batch 145] loss: 3.57845, acc1: 0.18750, acc5: 0.37500
[Epoch 7, batch 150] loss: 3.78326, acc1: 0.09375, acc5: 0.34375
[Epoch 7, batch 155] loss: 3.57974, acc1: 0.12500, acc5: 0.31250
[Epoch 7, batch 160] loss: 3.46777, acc1: 0.21875, acc5: 0.56250
[Epoch 7, batch 165] loss: 3.27084, acc1: 0.15625, acc5: 0.50000
[Epoch 7, batch 170] loss: 3.60336, acc1: 0.06250, acc5: 0.34375
[Epoch 7, batch 175] loss: 2.94085, acc1: 0.21875, acc5: 0.53125
[Epoch 7, batch 180] loss: 3.82482, acc1: 0.12500, acc5: 0.28125
[Epoch 7, batch 185] loss: 3.66004, acc1: 0.12500, acc5: 0.50000
[Epoch 7, batch 190] loss: 2.99892, acc1: 0.28125, acc5: 0.59375
[Epoch 8, batch 0] loss: 3.48303, acc1: 0.18750, acc5: 0.53125
[Epoch 8, batch 5] loss: 3.54525, acc1: 0.12500, acc5: 0.31250
[Epoch 8, batch 10] loss: 3.14171, acc1: 0.21875, acc5: 0.46875
[Epoch 8, batch 15] loss: 2.96526, acc1: 0.18750, acc5: 0.59375
[Epoch 8, batch 20] loss: 3.20113, acc1: 0.18750, acc5: 0.50000
[Epoch 8, batch 25] loss: 2.96845, acc1: 0.15625, acc5: 0.59375
[Epoch 8, batch 30] loss: 3.22508, acc1: 0.21875, acc5: 0.56250
[Epoch 8, batch 35] loss: 3.44578, acc1: 0.12500, acc5: 0.43750
[Epoch 8, batch 40] loss: 3.60788, acc1: 0.09375, acc5: 0.31250
[Epoch 8, batch 45] loss: 3.17873, acc1: 0.28125, acc5: 0.50000
[Epoch 8, batch 50] loss: 3.70243, acc1: 0.06250, acc5: 0.28125
[Epoch 8, batch 55] loss: 3.46854, acc1: 0.15625, acc5: 0.40625
[Epoch 8, batch 60] loss: 3.23939, acc1: 0.21875, acc5: 0.31250
[Epoch 8, batch 65] loss: 3.18832, acc1: 0.18750, acc5: 0.43750
[Epoch 8, batch 70] loss: 3.39855, acc1: 0.15625, acc5: 0.37500
[Epoch 8, batch 75] loss: 3.25565, acc1: 0.15625, acc5: 0.50000
[Epoch 8, batch 80] loss: 3.08207, acc1: 0.21875, acc5: 0.50000
[Epoch 8, batch 85] loss: 3.39936, acc1: 0.15625, acc5: 0.40625
[Epoch 8, batch 90] loss: 3.28059, acc1: 0.18750, acc5: 0.43750
[Epoch 8, batch 95] loss: 3.11662, acc1: 0.25000, acc5: 0.50000
[Epoch 8, batch 100] loss: 3.83535, acc1: 0.09375, acc5: 0.34375
[Epoch 8, batch 105] loss: 3.23953, acc1: 0.18750, acc5: 0.37500
[Epoch 8, batch 110] loss: 3.29767, acc1: 0.21875, acc5: 0.34375
[Epoch 8, batch 115] loss: 3.36616, acc1: 0.06250, acc5: 0.46875
[Epoch 8, batch 120] loss: 3.27061, acc1: 0.18750, acc5: 0.46875
[Epoch 8, batch 125] loss: 3.11382, acc1: 0.18750, acc5: 0.50000
[Epoch 8, batch 130] loss: 3.01022, acc1: 0.25000, acc5: 0.59375
[Epoch 8, batch 135] loss: 3.29156, acc1: 0.21875, acc5: 0.43750
[Epoch 8, batch 140] loss: 3.19789, acc1: 0.25000, acc5: 0.40625
[Epoch 8, batch 145] loss: 3.41567, acc1: 0.21875, acc5: 0.43750
[Epoch 8, batch 150] loss: 3.60911, acc1: 0.12500, acc5: 0.31250
[Epoch 8, batch 155] loss: 3.49315, acc1: 0.09375, acc5: 0.40625
[Epoch 8, batch 160] loss: 3.15710, acc1: 0.18750, acc5: 0.56250
[Epoch 8, batch 165] loss: 3.45020, acc1: 0.12500, acc5: 0.46875
[Epoch 8, batch 170] loss: 3.39515, acc1: 0.09375, acc5: 0.50000
[Epoch 8, batch 175] loss: 2.98429, acc1: 0.31250, acc5: 0.53125
[Epoch 8, batch 180] loss: 3.70362, acc1: 0.06250, acc5: 0.40625
[Epoch 8, batch 185] loss: 3.47918, acc1: 0.09375, acc5: 0.53125
[Epoch 8, batch 190] loss: 2.92874, acc1: 0.28125, acc5: 0.62500
[Epoch 9, batch 0] loss: 3.43869, acc1: 0.21875, acc5: 0.50000
[Epoch 9, batch 5] loss: 3.49765, acc1: 0.15625, acc5: 0.37500
[Epoch 9, batch 10] loss: 3.14828, acc1: 0.18750, acc5: 0.46875
[Epoch 9, batch 15] loss: 2.94551, acc1: 0.25000, acc5: 0.50000
[Epoch 9, batch 20] loss: 3.07148, acc1: 0.21875, acc5: 0.59375
[Epoch 9, batch 25] loss: 2.95816, acc1: 0.15625, acc5: 0.59375
[Epoch 9, batch 30] loss: 3.17434, acc1: 0.21875, acc5: 0.56250
[Epoch 9, batch 35] loss: 3.26944, acc1: 0.15625, acc5: 0.43750
[Epoch 9, batch 40] loss: 3.41409, acc1: 0.15625, acc5: 0.40625
[Epoch 9, batch 45] loss: 3.13034, acc1: 0.34375, acc5: 0.43750
[Epoch 9, batch 50] loss: 3.53908, acc1: 0.15625, acc5: 0.34375
[Epoch 9, batch 55] loss: 3.26250, acc1: 0.21875, acc5: 0.50000
[Epoch 9, batch 60] loss: 3.23972, acc1: 0.18750, acc5: 0.34375
[Epoch 9, batch 65] loss: 2.90807, acc1: 0.28125, acc5: 0.56250
[Epoch 9, batch 70] loss: 3.39155, acc1: 0.18750, acc5: 0.40625
[Epoch 9, batch 75] loss: 2.95661, acc1: 0.28125, acc5: 0.53125
[Epoch 9, batch 80] loss: 3.12390, acc1: 0.21875, acc5: 0.50000
[Epoch 9, batch 85] loss: 3.24473, acc1: 0.21875, acc5: 0.46875
[Epoch 9, batch 90] loss: 2.98539, acc1: 0.21875, acc5: 0.59375
[Epoch 9, batch 95] loss: 3.09900, acc1: 0.25000, acc5: 0.50000
[Epoch 9, batch 100] loss: 3.75741, acc1: 0.12500, acc5: 0.34375
[Epoch 9, batch 105] loss: 3.07527, acc1: 0.21875, acc5: 0.43750
[Epoch 9, batch 110] loss: 3.08463, acc1: 0.25000, acc5: 0.46875
[Epoch 9, batch 115] loss: 3.23105, acc1: 0.21875, acc5: 0.40625
[Epoch 9, batch 120] loss: 2.94515, acc1: 0.21875, acc5: 0.50000
[Epoch 9, batch 125] loss: 2.98429, acc1: 0.21875, acc5: 0.53125
[Epoch 9, batch 130] loss: 2.74451, acc1: 0.15625, acc5: 0.65625
[Epoch 9, batch 135] loss: 3.17842, acc1: 0.21875, acc5: 0.40625
[Epoch 9, batch 140] loss: 3.19924, acc1: 0.25000, acc5: 0.40625
[Epoch 9, batch 145] loss: 3.26786, acc1: 0.28125, acc5: 0.43750
[Epoch 9, batch 150] loss: 3.50033, acc1: 0.15625, acc5: 0.31250
[Epoch 9, batch 155] loss: 3.50004, acc1: 0.09375, acc5: 0.40625
[Epoch 9, batch 160] loss: 3.48320, acc1: 0.21875, acc5: 0.43750
[Epoch 9, batch 165] loss: 3.17533, acc1: 0.12500, acc5: 0.43750
[Epoch 9, batch 170] loss: 3.29404, acc1: 0.09375, acc5: 0.53125
[Epoch 9, batch 175] loss: 2.93154, acc1: 0.21875, acc5: 0.59375
[Epoch 9, batch 180] loss: 3.71366, acc1: 0.12500, acc5: 0.37500
[Epoch 9, batch 185] loss: 3.43655, acc1: 0.28125, acc5: 0.53125
[Epoch 9, batch 190] loss: 2.69039, acc1: 0.31250, acc5: 0.71875
Start collective training example:
Run on CUDAPlace(0).
train_fleet_static.py:50: DeprecationWarning: [93m
Warning:
API "paddle.dataset.flowers.train" is deprecated since 2.0.0, and will be removed in future versions. Please use "paddle.vision.datasets.Flowers" instead.
reason: Please use new dataset API which supports paddle.io.DataLoader [0m
  reader_decorator(paddle.dataset.flowers.train(use_xmap=True)),
server not ready, wait 3 sec to retry...
not ready endpoints:['127.0.0.1:34981']
Execute startup program.
W0909 13:57:19.327047 15542 device_context.cc:446] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 10.1
W0909 13:57:19.332624 15542 device_context.cc:464] device: 0, cuDNN Version: 7.6.
Start training:
W0909 13:57:25.818712 15542 fuse_all_reduce_op_pass.cc:76] Find all_reduce operators: 161. To make the speed faster, some all_reduce ops are fused during training, after fusion, the number of all_reduce ops is 5.
[Epoch 0, batch 0] loss: 4.96996, acc1: 0.00000, acc5: 0.00000
[Epoch 0, batch 5] loss: 14.90462, acc1: 0.03125, acc5: 0.06250
[Epoch 0, batch 10] loss: 8.39990, acc1: 0.09375, acc5: 0.12500
[Epoch 0, batch 15] loss: 16.61186, acc1: 0.00000, acc5: 0.09375
[Epoch 0, batch 20] loss: 4.82182, acc1: 0.00000, acc5: 0.00000
[Epoch 0, batch 25] loss: 5.15984, acc1: 0.03125, acc5: 0.06250
[Epoch 0, batch 30] loss: 4.68003, acc1: 0.00000, acc5: 0.00000
[Epoch 0, batch 35] loss: 4.85943, acc1: 0.03125, acc5: 0.18750
[Epoch 0, batch 40] loss: 4.66704, acc1: 0.00000, acc5: 0.06250
[Epoch 0, batch 45] loss: 4.92498, acc1: 0.09375, acc5: 0.18750
[Epoch 0, batch 50] loss: 8.19839, acc1: 0.00000, acc5: 0.03125
[Epoch 0, batch 55] loss: 4.92768, acc1: 0.03125, acc5: 0.09375
[Epoch 0, batch 60] loss: 4.59952, acc1: 0.06250, acc5: 0.12500
[Epoch 0, batch 65] loss: 4.55010, acc1: 0.03125, acc5: 0.09375
[Epoch 0, batch 70] loss: 4.67235, acc1: 0.03125, acc5: 0.18750
[Epoch 0, batch 75] loss: 4.57419, acc1: 0.00000, acc5: 0.09375
[Epoch 0, batch 80] loss: 4.66482, acc1: 0.06250, acc5: 0.09375
[Epoch 0, batch 85] loss: 4.60460, acc1: 0.00000, acc5: 0.09375
[Epoch 0, batch 90] loss: 4.47370, acc1: 0.03125, acc5: 0.15625
[Epoch 0, batch 95] loss: 4.48529, acc1: 0.06250, acc5: 0.12500
[Epoch 0, batch 100] loss: 4.63606, acc1: 0.00000, acc5: 0.06250
[Epoch 0, batch 105] loss: 4.49066, acc1: 0.03125, acc5: 0.15625
[Epoch 0, batch 110] loss: 4.47445, acc1: 0.00000, acc5: 0.06250
[Epoch 0, batch 115] loss: 4.95739, acc1: 0.03125, acc5: 0.15625
[Epoch 0, batch 120] loss: 4.43703, acc1: 0.00000, acc5: 0.15625


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::MultiDeviceFeedReader<paddle::operators::reader::OrderedMultiDeviceLoDTensorBlockingQueue>::ReadNext()
1   paddle::pybind::MultiDeviceFeedReader<paddle::operators::reader::OrderedMultiDeviceLoDTensorBlockingQueue>::CheckNextStatus()
2   paddle::pybind::MultiDeviceFeedReader<paddle::operators::reader::OrderedMultiDeviceLoDTensorBlockingQueue>::WaitFutures(std::__exception_ptr::exception_ptr*)

----------------------
Error Message Summary:
----------------------
FatalError: `Termination signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1631167061 (unix time) try "date -d @1631167061" if you are using GNU date ***]
  [SignalInfo: *** SIGTERM (@0x3c19) received by PID 15542 (TID 0x7f152eeb1700) from PID 15385 ***]

