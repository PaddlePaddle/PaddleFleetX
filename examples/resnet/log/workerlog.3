Traceback (most recent call last):
  File "train_fleet_sharding.py", line 116, in <module>
    train_resnet()
  File "train_fleet_sharding.py", line 94, in train_resnet
    optimizer.minimize(avg_cost)
  File "/opt/conda/envs/optest/lib/python3.7/site-packages/paddle/distributed/fleet/base/fleet_base.py", line 1503, in minimize
    loss, startup_program, parameter_list, no_grad_set=no_grad_set)
  File "/opt/conda/envs/optest/lib/python3.7/site-packages/paddle/distributed/fleet/meta_optimizers/meta_optimizer_base.py", line 95, in minimize
    loss, startup_program, parameter_list, no_grad_set)
  File "/opt/conda/envs/optest/lib/python3.7/site-packages/paddle/distributed/fleet/meta_optimizers/sharding_optimizer.py", line 579, in minimize_impl
    self._get_hybrid_degree()
  File "/opt/conda/envs/optest/lib/python3.7/site-packages/paddle/distributed/fleet/meta_optimizers/sharding_optimizer.py", line 142, in _get_hybrid_degree
    global_world_size, mp_degree, sharding_degree, pp_degree, dp_degree)
AssertionError: global work size [8], mp_degree [1], sharding_degree [2], pp_degree [1], dp_degree [2].


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Termination signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1635236663 (unix time) try "date -d @1635236663" if you are using GNU date ***]
  [SignalInfo: *** SIGTERM (@0x7a71) received by PID 31511 (TID 0x7f16c82b8700) from PID 31345 ***]

2021-10-26 16:25:14 INFO     Hybrid DP mode turn on !
INFO:paddle.distributed.fleet.meta_optimizers.sharding_optimizer:Hybrid DP mode turn on !
2021-10-26 16:25:14 INFO     global word size: 4
INFO:paddle.distributed.fleet.meta_optimizers.sharding_optimizer:global word size: 4
2021-10-26 16:25:14 INFO     global rank: 3
INFO:paddle.distributed.fleet.meta_optimizers.sharding_optimizer:global rank: 3
2021-10-26 16:25:14 INFO     global endpoints: ['127.0.0.1:10151', '127.0.0.1:17370', '127.0.0.1:33542', '127.0.0.1:23943']
INFO:paddle.distributed.fleet.meta_optimizers.sharding_optimizer:global endpoints: ['127.0.0.1:10151', '127.0.0.1:17370', '127.0.0.1:33542', '127.0.0.1:23943']
2021-10-26 16:25:14 INFO     global ring id: 3
INFO:paddle.distributed.fleet.meta_optimizers.sharding_optimizer:global ring id: 3
2021-10-26 16:25:14 INFO     ##############################
INFO:paddle.distributed.fleet.meta_optimizers.sharding_optimizer:##############################
2021-10-26 16:25:14 INFO     mp group size: 1
INFO:paddle.distributed.fleet.meta_optimizers.sharding_optimizer:mp group size: 1
2021-10-26 16:25:14 INFO     mp rank: -1
INFO:paddle.distributed.fleet.meta_optimizers.sharding_optimizer:mp rank: -1
2021-10-26 16:25:14 INFO     mp group id: -1
INFO:paddle.distributed.fleet.meta_optimizers.sharding_optimizer:mp group id: -1
2021-10-26 16:25:14 INFO     mp group endpoints: []
INFO:paddle.distributed.fleet.meta_optimizers.sharding_optimizer:mp group endpoints: []
2021-10-26 16:25:14 INFO     mp ring id: -1
INFO:paddle.distributed.fleet.meta_optimizers.sharding_optimizer:mp ring id: -1
2021-10-26 16:25:14 INFO     ##############################
INFO:paddle.distributed.fleet.meta_optimizers.sharding_optimizer:##############################
2021-10-26 16:25:14 INFO     sharding group size: 2
INFO:paddle.distributed.fleet.meta_optimizers.sharding_optimizer:sharding group size: 2
2021-10-26 16:25:14 INFO     sharding rank: 1
INFO:paddle.distributed.fleet.meta_optimizers.sharding_optimizer:sharding rank: 1
2021-10-26 16:25:14 INFO     sharding group id: 1
INFO:paddle.distributed.fleet.meta_optimizers.sharding_optimizer:sharding group id: 1
2021-10-26 16:25:14 INFO     sharding group endpoints: ['127.0.0.1:33542', '127.0.0.1:23943']
INFO:paddle.distributed.fleet.meta_optimizers.sharding_optimizer:sharding group endpoints: ['127.0.0.1:33542', '127.0.0.1:23943']
2021-10-26 16:25:14 INFO     sharding ring id: 1
INFO:paddle.distributed.fleet.meta_optimizers.sharding_optimizer:sharding ring id: 1
2021-10-26 16:25:14 INFO     ##############################
INFO:paddle.distributed.fleet.meta_optimizers.sharding_optimizer:##############################
2021-10-26 16:25:14 INFO     pp group size: 1
INFO:paddle.distributed.fleet.meta_optimizers.sharding_optimizer:pp group size: 1
2021-10-26 16:25:14 INFO     pp rank: -1
INFO:paddle.distributed.fleet.meta_optimizers.sharding_optimizer:pp rank: -1
2021-10-26 16:25:14 INFO     pp group id: -1
INFO:paddle.distributed.fleet.meta_optimizers.sharding_optimizer:pp group id: -1
2021-10-26 16:25:14 INFO     pp group endpoints: []
INFO:paddle.distributed.fleet.meta_optimizers.sharding_optimizer:pp group endpoints: []
2021-10-26 16:25:14 INFO     pp ring id: -1
INFO:paddle.distributed.fleet.meta_optimizers.sharding_optimizer:pp ring id: -1
2021-10-26 16:25:14 INFO     ##############################
INFO:paddle.distributed.fleet.meta_optimizers.sharding_optimizer:##############################
2021-10-26 16:25:14 INFO     pure dp group size: 2
INFO:paddle.distributed.fleet.meta_optimizers.sharding_optimizer:pure dp group size: 2
2021-10-26 16:25:14 INFO     pure dp rank: 1
INFO:paddle.distributed.fleet.meta_optimizers.sharding_optimizer:pure dp rank: 1
2021-10-26 16:25:14 INFO     pure dp group endpoints: ['127.0.0.1:17370', '127.0.0.1:23943']
INFO:paddle.distributed.fleet.meta_optimizers.sharding_optimizer:pure dp group endpoints: ['127.0.0.1:17370', '127.0.0.1:23943']
2021-10-26 16:25:14 INFO     pure dp ring id: 2
INFO:paddle.distributed.fleet.meta_optimizers.sharding_optimizer:pure dp ring id: 2
2021-10-26 16:25:14 INFO     ##############################
INFO:paddle.distributed.fleet.meta_optimizers.sharding_optimizer:##############################
    +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                      sharding=True <-> sharding_configs                      |
    +------------------------------------------------------------------------------+
    |             sharding_segment_strategy           segment_broadcast_MB         |
    |                  segment_broadcast_MB                   32.0                 |
    |                       sharding_degree                    2                   |
    |                             mp_degree                    1                   |
    |                             dp_degree                    2                   |
    |                             hybrid_dp                  False                 |
    |               gradient_merge_acc_step                    1                   |
    |                      optimize_offload                  False                 |
    |              pp_allreduce_in_optimize                  False                 |
    |                             pp_degree                    1                   |
    |                         optimize_cast                  False                 |
    |             _dp_as_optimizer_sharding                  False                 |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                  False                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+

W1026 16:25:15.578738 31778 device_context.cc:447] Please NOTE: device: 3, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 10.1
W1026 16:25:15.584481 31778 device_context.cc:465] device: 3, cuDNN Version: 7.6.
I1026 16:25:20.320046 31778 gen_comm_id_helper.cc:190] Server listening on: 127.0.0.1:23943 successful.
Warning with paddle image module: opencv-python should be imported,
         or paddle image module could NOT work; please install opencv-python first.Warning with paddle image module: opencv-python should be imported,
         or paddle image module could NOT work; please install opencv-python first.Exception in thread Thread-3:
Traceback (most recent call last):
  File "/opt/conda/envs/optest/lib/python3.7/threading.py", line 926, in _bootstrap_inner
    self.run()
  File "/opt/conda/envs/optest/lib/python3.7/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/opt/conda/envs/optest/lib/python3.7/site-packages/paddle/reader/decorator.py", line 448, in handle_worker
    r = mapper(sample)
  File "/opt/conda/envs/optest/lib/python3.7/site-packages/paddle/dataset/flowers.py", line 75, in default_mapper
    img = load_image_bytes(img)
  File "/opt/conda/envs/optest/lib/python3.7/site-packages/paddle/dataset/image.py", line 161, in load_image_bytes
    assert _check_cv2() is True
AssertionError

Exception in thread Thread-4:
Traceback (most recent call last):
  File "/opt/conda/envs/optest/lib/python3.7/threading.py", line 926, in _bootstrap_inner
    self.run()
  File "/opt/conda/envs/optest/lib/python3.7/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/opt/conda/envs/optest/lib/python3.7/site-packages/paddle/reader/decorator.py", line 448, in handle_worker
    r = mapper(sample)
  File "/opt/conda/envs/optest/lib/python3.7/site-packages/paddle/dataset/flowers.py", line 75, in default_mapper
    img = load_image_bytes(img)
  File "/opt/conda/envs/optest/lib/python3.7/site-packages/paddle/dataset/image.py", line 161, in load_image_bytes
    assert _check_cv2() is True
AssertionError

Warning with paddle image module: opencv-python should be imported,
         or paddle image module could NOT work; please install opencv-python first.Exception in thread Thread-5:
Traceback (most recent call last):
  File "/opt/conda/envs/optest/lib/python3.7/threading.py", line 926, in _bootstrap_inner
    self.run()
  File "/opt/conda/envs/optest/lib/python3.7/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/opt/conda/envs/optest/lib/python3.7/site-packages/paddle/reader/decorator.py", line 448, in handle_worker
    r = mapper(sample)
  File "/opt/conda/envs/optest/lib/python3.7/site-packages/paddle/dataset/flowers.py", line 75, in default_mapper
    img = load_image_bytes(img)
  File "/opt/conda/envs/optest/lib/python3.7/site-packages/paddle/dataset/image.py", line 161, in load_image_bytes
    assert _check_cv2() is True
AssertionError

Warning with paddle image module: opencv-python should be imported,
         or paddle image module could NOT work; please install opencv-python first.Exception in thread Thread-6:
Traceback (most recent call last):
  File "/opt/conda/envs/optest/lib/python3.7/threading.py", line 926, in _bootstrap_inner
    self.run()
  File "/opt/conda/envs/optest/lib/python3.7/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/opt/conda/envs/optest/lib/python3.7/site-packages/paddle/reader/decorator.py", line 448, in handle_worker
    r = mapper(sample)
  File "/opt/conda/envs/optest/lib/python3.7/site-packages/paddle/dataset/flowers.py", line 75, in default_mapper
    img = load_image_bytes(img)
  File "/opt/conda/envs/optest/lib/python3.7/site-packages/paddle/dataset/image.py", line 161, in load_image_bytes
    assert _check_cv2() is True
AssertionError



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::MultiDeviceFeedReader<paddle::operators::reader::OrderedMultiDeviceLoDTensorBlockingQueue>::ReadNext()
1   paddle::pybind::MultiDeviceFeedReader<paddle::operators::reader::OrderedMultiDeviceLoDTensorBlockingQueue>::CheckNextStatus()
2   paddle::pybind::MultiDeviceFeedReader<paddle::operators::reader::OrderedMultiDeviceLoDTensorBlockingQueue>::WaitFutures(std::__exception_ptr::exception_ptr*)

----------------------
Error Message Summary:
----------------------
FatalError: `Termination signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1635236845 (unix time) try "date -d @1635236845" if you are using GNU date ***]
  [SignalInfo: *** SIGTERM (@0x7b7a) received by PID 31778 (TID 0x7fc949dba700) from PID 31610 ***]

