Traceback (most recent call last):
  File "train_fleet_sharding.py", line 116, in <module>
    train_resnet()
  File "train_fleet_sharding.py", line 94, in train_resnet
    optimizer.minimize(avg_cost)
  File "/opt/conda/envs/optest/lib/python3.7/site-packages/paddle/distributed/fleet/base/fleet_base.py", line 1503, in minimize
    loss, startup_program, parameter_list, no_grad_set=no_grad_set)
  File "/opt/conda/envs/optest/lib/python3.7/site-packages/paddle/distributed/fleet/meta_optimizers/meta_optimizer_base.py", line 95, in minimize
    loss, startup_program, parameter_list, no_grad_set)
  File "/opt/conda/envs/optest/lib/python3.7/site-packages/paddle/distributed/fleet/meta_optimizers/sharding_optimizer.py", line 579, in minimize_impl
    self._get_hybrid_degree()
  File "/opt/conda/envs/optest/lib/python3.7/site-packages/paddle/distributed/fleet/meta_optimizers/sharding_optimizer.py", line 142, in _get_hybrid_degree
    global_world_size, mp_degree, sharding_degree, pp_degree, dp_degree)
AssertionError: global work size [8], mp_degree [1], sharding_degree [2], pp_degree [1], dp_degree [2].


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Termination signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1635236663 (unix time) try "date -d @1635236663" if you are using GNU date ***]
  [SignalInfo: *** SIGTERM (@0x7a71) received by PID 31537 (TID 0x7f90f48a0700) from PID 31345 ***]

