Traceback (most recent call last):
  File "train_fleet_sharding.py", line 116, in <module>
    train_resnet()
  File "train_fleet_sharding.py", line 94, in train_resnet
    optimizer.minimize(avg_cost)
  File "/opt/conda/envs/optest/lib/python3.7/site-packages/paddle/distributed/fleet/base/fleet_base.py", line 1503, in minimize
    loss, startup_program, parameter_list, no_grad_set=no_grad_set)
  File "/opt/conda/envs/optest/lib/python3.7/site-packages/paddle/distributed/fleet/meta_optimizers/meta_optimizer_base.py", line 95, in minimize
    loss, startup_program, parameter_list, no_grad_set)
  File "/opt/conda/envs/optest/lib/python3.7/site-packages/paddle/distributed/fleet/meta_optimizers/sharding_optimizer.py", line 579, in minimize_impl
    self._get_hybrid_degree()
  File "/opt/conda/envs/optest/lib/python3.7/site-packages/paddle/distributed/fleet/meta_optimizers/sharding_optimizer.py", line 142, in _get_hybrid_degree
    global_world_size, mp_degree, sharding_degree, pp_degree, dp_degree)
AssertionError: global work size [8], mp_degree [1], sharding_degree [2], pp_degree [1], dp_degree [2].
