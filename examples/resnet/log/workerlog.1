Start collective training example:
Run on CUDAPlace(1).
train_fleet_static.py:50: DeprecationWarning: [93m
Warning:
API "paddle.dataset.flowers.train" is deprecated since 2.0.0, and will be removed in future versions. Please use "paddle.vision.datasets.Flowers" instead.
reason: Please use new dataset API which supports paddle.io.DataLoader [0m
  reader_decorator(paddle.dataset.flowers.train(use_xmap=True)),
Execute startup program.
W0909 12:25:59.575390 33465 device_context.cc:446] Please NOTE: device: 1, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 10.1
W0909 12:25:59.580684 33465 device_context.cc:464] device: 1, cuDNN Version: 7.6.
I0909 12:26:02.730139 33465 gen_comm_id_helper.cc:188] Server listening on: 127.0.0.1:12015 successful.
Start training:
Warning with paddle image module: opencv-python should be imported,
         or paddle image module could NOT work; please install opencv-python first.Warning with paddle image module: opencv-python should be imported,
         or paddle image module could NOT work; please install opencv-python first.Warning with paddle image module: opencv-python should be imported,
         or paddle image module could NOT work; please install opencv-python first.Exception in thread Thread-5:
Traceback (most recent call last):
  File "/opt/conda/envs/py37/lib/python3.7/threading.py", line 926, in _bootstrap_inner
    self.run()
  File "/opt/conda/envs/py37/lib/python3.7/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/opt/conda/envs/py37/lib/python3.7/site-packages/paddle/reader/decorator.py", line 448, in handle_worker
    r = mapper(sample)
  File "/opt/conda/envs/py37/lib/python3.7/site-packages/paddle/dataset/flowers.py", line 75, in default_mapper
    img = load_image_bytes(img)
  File "/opt/conda/envs/py37/lib/python3.7/site-packages/paddle/dataset/image.py", line 159, in load_image_bytes
    assert _check_cv2() is True
AssertionError
Exception in thread Thread-4:
Traceback (most recent call last):
  File "/opt/conda/envs/py37/lib/python3.7/threading.py", line 926, in _bootstrap_inner
    self.run()
  File "/opt/conda/envs/py37/lib/python3.7/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/opt/conda/envs/py37/lib/python3.7/site-packages/paddle/reader/decorator.py", line 448, in handle_worker
    r = mapper(sample)
  File "/opt/conda/envs/py37/lib/python3.7/site-packages/paddle/dataset/flowers.py", line 75, in default_mapper
    img = load_image_bytes(img)
  File "/opt/conda/envs/py37/lib/python3.7/site-packages/paddle/dataset/image.py", line 159, in load_image_bytes
    assert _check_cv2() is True
AssertionError


Exception in thread Thread-3:
Traceback (most recent call last):
  File "/opt/conda/envs/py37/lib/python3.7/threading.py", line 926, in _bootstrap_inner
    self.run()
  File "/opt/conda/envs/py37/lib/python3.7/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/opt/conda/envs/py37/lib/python3.7/site-packages/paddle/reader/decorator.py", line 448, in handle_worker
    r = mapper(sample)
  File "/opt/conda/envs/py37/lib/python3.7/site-packages/paddle/dataset/flowers.py", line 75, in default_mapper
    img = load_image_bytes(img)
  File "/opt/conda/envs/py37/lib/python3.7/site-packages/paddle/dataset/image.py", line 159, in load_image_bytes
    assert _check_cv2() is True
AssertionError

Warning with paddle image module: opencv-python should be imported,
         or paddle image module could NOT work; please install opencv-python first.Exception in thread Thread-6:
Traceback (most recent call last):
  File "/opt/conda/envs/py37/lib/python3.7/threading.py", line 926, in _bootstrap_inner
    self.run()
  File "/opt/conda/envs/py37/lib/python3.7/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/opt/conda/envs/py37/lib/python3.7/site-packages/paddle/reader/decorator.py", line 448, in handle_worker
    r = mapper(sample)
  File "/opt/conda/envs/py37/lib/python3.7/site-packages/paddle/dataset/flowers.py", line 75, in default_mapper
    img = load_image_bytes(img)
  File "/opt/conda/envs/py37/lib/python3.7/site-packages/paddle/dataset/image.py", line 159, in load_image_bytes
    assert _check_cv2() is True
AssertionError

Start collective training example:
Run on CUDAPlace(1).
train_fleet_static.py:50: DeprecationWarning: [93m
Warning:
API "paddle.dataset.flowers.train" is deprecated since 2.0.0, and will be removed in future versions. Please use "paddle.vision.datasets.Flowers" instead.
reason: Please use new dataset API which supports paddle.io.DataLoader [0m
  reader_decorator(paddle.dataset.flowers.train(use_xmap=True)),
Execute startup program.
W0909 12:27:01.171555 35660 device_context.cc:446] Please NOTE: device: 1, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 10.1
W0909 12:27:01.177042 35660 device_context.cc:464] device: 1, cuDNN Version: 7.6.
I0909 12:27:04.214745 35660 gen_comm_id_helper.cc:188] Server listening on: 127.0.0.1:10524 successful.
Start training:
W0909 12:27:15.058800 35660 fuse_all_reduce_op_pass.cc:76] Find all_reduce operators: 161. To make the speed faster, some all_reduce ops are fused during training, after fusion, the number of all_reduce ops is 5.
[Epoch 0, batch 0] loss: 4.75175, acc1: 0.00000, acc5: 0.03125
[Epoch 0, batch 5] loss: 25.96189, acc1: 0.03125, acc5: 0.09375
[Epoch 0, batch 10] loss: 7.89282, acc1: 0.09375, acc5: 0.09375
[Epoch 0, batch 15] loss: 7.48180, acc1: 0.03125, acc5: 0.06250
[Epoch 0, batch 20] loss: 10.62112, acc1: 0.03125, acc5: 0.09375
[Epoch 0, batch 25] loss: 5.76648, acc1: 0.00000, acc5: 0.12500
[Epoch 0, batch 30] loss: 5.76492, acc1: 0.06250, acc5: 0.06250
[Epoch 0, batch 35] loss: 6.66065, acc1: 0.00000, acc5: 0.18750
[Epoch 0, batch 40] loss: 4.58221, acc1: 0.00000, acc5: 0.09375
[Epoch 0, batch 45] loss: 4.77026, acc1: 0.09375, acc5: 0.25000
[Epoch 0, batch 50] loss: 4.59367, acc1: 0.00000, acc5: 0.09375
[Epoch 0, batch 55] loss: 4.47091, acc1: 0.00000, acc5: 0.18750
[Epoch 0, batch 60] loss: 4.53597, acc1: 0.06250, acc5: 0.09375
[Epoch 0, batch 65] loss: 4.48237, acc1: 0.00000, acc5: 0.09375
[Epoch 0, batch 70] loss: 4.56733, acc1: 0.00000, acc5: 0.18750
[Epoch 0, batch 75] loss: 4.50134, acc1: 0.06250, acc5: 0.15625
[Epoch 0, batch 80] loss: 4.48323, acc1: 0.00000, acc5: 0.12500
[Epoch 0, batch 85] loss: 4.56074, acc1: 0.00000, acc5: 0.12500
[Epoch 0, batch 90] loss: 4.46926, acc1: 0.03125, acc5: 0.12500
[Epoch 0, batch 95] loss: 4.89030, acc1: 0.00000, acc5: 0.06250
[Epoch 0, batch 100] loss: 4.61120, acc1: 0.00000, acc5: 0.09375
[Epoch 0, batch 105] loss: 4.40213, acc1: 0.06250, acc5: 0.21875
[Epoch 0, batch 110] loss: 4.44775, acc1: 0.03125, acc5: 0.09375
[Epoch 0, batch 115] loss: 4.38345, acc1: 0.03125, acc5: 0.18750
[Epoch 0, batch 120] loss: 4.38041, acc1: 0.03125, acc5: 0.15625
[Epoch 0, batch 125] loss: 4.39582, acc1: 0.03125, acc5: 0.21875
[Epoch 0, batch 130] loss: 4.35980, acc1: 0.09375, acc5: 0.21875
[Epoch 0, batch 135] loss: 4.60568, acc1: 0.03125, acc5: 0.09375
[Epoch 0, batch 140] loss: 4.31850, acc1: 0.12500, acc5: 0.21875
[Epoch 0, batch 145] loss: 4.21180, acc1: 0.09375, acc5: 0.25000
[Epoch 0, batch 150] loss: 4.45925, acc1: 0.03125, acc5: 0.15625
[Epoch 0, batch 155] loss: 4.55138, acc1: 0.03125, acc5: 0.09375
[Epoch 0, batch 160] loss: 4.37635, acc1: 0.00000, acc5: 0.12500
[Epoch 0, batch 165] loss: 4.16632, acc1: 0.06250, acc5: 0.18750
[Epoch 0, batch 170] loss: 4.53685, acc1: 0.00000, acc5: 0.06250
[Epoch 0, batch 175] loss: 4.34987, acc1: 0.09375, acc5: 0.21875
[Epoch 0, batch 180] loss: 4.45574, acc1: 0.00000, acc5: 0.15625
[Epoch 0, batch 185] loss: 4.28232, acc1: 0.06250, acc5: 0.21875
[Epoch 0, batch 190] loss: 4.37595, acc1: 0.06250, acc5: 0.21875
[Epoch 1, batch 0] loss: 4.36827, acc1: 0.06250, acc5: 0.25000
[Epoch 1, batch 5] loss: 4.55854, acc1: 0.00000, acc5: 0.12500
[Epoch 1, batch 10] loss: 4.40338, acc1: 0.03125, acc5: 0.15625
[Epoch 1, batch 15] loss: 4.18840, acc1: 0.09375, acc5: 0.28125
[Epoch 1, batch 20] loss: 4.39842, acc1: 0.03125, acc5: 0.15625
[Epoch 1, batch 25] loss: 4.15015, acc1: 0.03125, acc5: 0.28125
[Epoch 1, batch 30] loss: 4.35550, acc1: 0.06250, acc5: 0.25000
[Epoch 1, batch 35] loss: 4.36569, acc1: 0.00000, acc5: 0.21875
[Epoch 1, batch 40] loss: 4.60065, acc1: 0.00000, acc5: 0.12500
[Epoch 1, batch 45] loss: 4.13319, acc1: 0.15625, acc5: 0.31250
[Epoch 1, batch 50] loss: 4.59641, acc1: 0.00000, acc5: 0.06250
[Epoch 1, batch 55] loss: 4.25498, acc1: 0.03125, acc5: 0.15625
[Epoch 1, batch 60] loss: 4.33666, acc1: 0.12500, acc5: 0.21875
[Epoch 1, batch 65] loss: 4.18753, acc1: 0.06250, acc5: 0.18750
[Epoch 1, batch 70] loss: 4.51675, acc1: 0.03125, acc5: 0.18750
[Epoch 1, batch 75] loss: 4.32683, acc1: 0.09375, acc5: 0.21875
[Epoch 1, batch 80] loss: 4.32994, acc1: 0.03125, acc5: 0.18750
[Epoch 1, batch 85] loss: 4.48731, acc1: 0.03125, acc5: 0.12500
[Epoch 1, batch 90] loss: 4.21180, acc1: 0.03125, acc5: 0.18750
[Epoch 1, batch 95] loss: 4.37598, acc1: 0.06250, acc5: 0.18750
[Epoch 1, batch 100] loss: 4.56710, acc1: 0.00000, acc5: 0.09375
[Epoch 1, batch 105] loss: 4.20810, acc1: 0.03125, acc5: 0.21875
[Epoch 1, batch 110] loss: 4.23797, acc1: 0.06250, acc5: 0.18750
[Epoch 1, batch 115] loss: 4.25080, acc1: 0.09375, acc5: 0.18750
[Epoch 1, batch 120] loss: 4.09932, acc1: 0.09375, acc5: 0.21875
[Epoch 1, batch 125] loss: 4.12066, acc1: 0.09375, acc5: 0.34375
[Epoch 1, batch 130] loss: 4.27791, acc1: 0.09375, acc5: 0.25000
[Epoch 1, batch 135] loss: 4.58421, acc1: 0.03125, acc5: 0.09375
[Epoch 1, batch 140] loss: 4.19762, acc1: 0.12500, acc5: 0.28125
[Epoch 1, batch 145] loss: 4.13755, acc1: 0.03125, acc5: 0.21875
[Epoch 1, batch 150] loss: 4.49857, acc1: 0.00000, acc5: 0.12500
[Epoch 1, batch 155] loss: 4.33615, acc1: 0.06250, acc5: 0.18750
[Epoch 1, batch 160] loss: 4.23036, acc1: 0.00000, acc5: 0.25000
[Epoch 1, batch 165] loss: 4.05662, acc1: 0.00000, acc5: 0.28125
[Epoch 1, batch 170] loss: 4.33787, acc1: 0.00000, acc5: 0.18750
[Epoch 1, batch 175] loss: 4.11866, acc1: 0.03125, acc5: 0.25000
[Epoch 1, batch 180] loss: 4.29246, acc1: 0.00000, acc5: 0.15625
[Epoch 1, batch 185] loss: 4.08079, acc1: 0.12500, acc5: 0.25000
[Epoch 1, batch 190] loss: 4.27853, acc1: 0.03125, acc5: 0.15625
[Epoch 2, batch 0] loss: 4.23646, acc1: 0.12500, acc5: 0.28125
[Epoch 2, batch 5] loss: 4.35733, acc1: 0.03125, acc5: 0.09375
[Epoch 2, batch 10] loss: 4.18607, acc1: 0.06250, acc5: 0.21875
[Epoch 2, batch 15] loss: 3.75289, acc1: 0.15625, acc5: 0.40625
[Epoch 2, batch 20] loss: 4.06651, acc1: 0.09375, acc5: 0.25000
[Epoch 2, batch 25] loss: 4.00482, acc1: 0.03125, acc5: 0.31250
[Epoch 2, batch 30] loss: 4.28030, acc1: 0.06250, acc5: 0.21875
[Epoch 2, batch 35] loss: 4.19269, acc1: 0.03125, acc5: 0.25000
[Epoch 2, batch 40] loss: 4.45286, acc1: 0.03125, acc5: 0.12500
[Epoch 2, batch 45] loss: 3.97061, acc1: 0.09375, acc5: 0.34375
[Epoch 2, batch 50] loss: 4.45416, acc1: 0.03125, acc5: 0.06250
[Epoch 2, batch 55] loss: 3.83398, acc1: 0.15625, acc5: 0.34375
[Epoch 2, batch 60] loss: 4.30607, acc1: 0.09375, acc5: 0.12500
[Epoch 2, batch 65] loss: 3.96322, acc1: 0.00000, acc5: 0.28125
[Epoch 2, batch 70] loss: 4.26038, acc1: 0.06250, acc5: 0.28125
[Epoch 2, batch 75] loss: 4.09956, acc1: 0.00000, acc5: 0.28125
[Epoch 2, batch 80] loss: 4.05518, acc1: 0.09375, acc5: 0.28125
[Epoch 2, batch 85] loss: 4.28021, acc1: 0.09375, acc5: 0.15625
[Epoch 2, batch 90] loss: 4.04448, acc1: 0.03125, acc5: 0.18750
[Epoch 2, batch 95] loss: 4.15135, acc1: 0.06250, acc5: 0.18750
[Epoch 2, batch 100] loss: 4.41437, acc1: 0.00000, acc5: 0.09375
[Epoch 2, batch 105] loss: 4.03051, acc1: 0.06250, acc5: 0.21875
[Epoch 2, batch 110] loss: 4.06342, acc1: 0.03125, acc5: 0.18750
[Epoch 2, batch 115] loss: 4.07091, acc1: 0.06250, acc5: 0.21875
[Epoch 2, batch 120] loss: 3.92768, acc1: 0.12500, acc5: 0.21875
[Epoch 2, batch 125] loss: 3.81962, acc1: 0.12500, acc5: 0.40625
[Epoch 2, batch 130] loss: 3.85513, acc1: 0.21875, acc5: 0.28125
[Epoch 2, batch 135] loss: 4.38898, acc1: 0.06250, acc5: 0.12500
[Epoch 2, batch 140] loss: 4.03138, acc1: 0.12500, acc5: 0.25000
[Epoch 2, batch 145] loss: 3.99113, acc1: 0.09375, acc5: 0.28125
[Epoch 2, batch 150] loss: 4.14732, acc1: 0.09375, acc5: 0.28125
[Epoch 2, batch 155] loss: 4.11904, acc1: 0.06250, acc5: 0.18750
[Epoch 2, batch 160] loss: 4.13218, acc1: 0.09375, acc5: 0.28125
[Epoch 2, batch 165] loss: 3.86638, acc1: 0.09375, acc5: 0.40625
[Epoch 2, batch 170] loss: 4.08030, acc1: 0.03125, acc5: 0.21875
[Epoch 2, batch 175] loss: 3.74069, acc1: 0.15625, acc5: 0.40625
[Epoch 2, batch 180] loss: 4.26095, acc1: 0.03125, acc5: 0.21875
[Epoch 2, batch 185] loss: 4.06048, acc1: 0.06250, acc5: 0.37500
[Epoch 2, batch 190] loss: 3.97907, acc1: 0.06250, acc5: 0.21875
[Epoch 3, batch 0] loss: 4.18601, acc1: 0.12500, acc5: 0.37500
[Epoch 3, batch 5] loss: 4.13441, acc1: 0.03125, acc5: 0.15625
[Epoch 3, batch 10] loss: 4.10956, acc1: 0.09375, acc5: 0.18750
[Epoch 3, batch 15] loss: 3.43853, acc1: 0.15625, acc5: 0.53125
[Epoch 3, batch 20] loss: 3.96390, acc1: 0.03125, acc5: 0.21875
[Epoch 3, batch 25] loss: 3.76299, acc1: 0.09375, acc5: 0.37500
[Epoch 3, batch 30] loss: 4.16118, acc1: 0.12500, acc5: 0.31250
[Epoch 3, batch 35] loss: 3.96973, acc1: 0.06250, acc5: 0.34375
[Epoch 3, batch 40] loss: 4.40821, acc1: 0.00000, acc5: 0.18750
[Epoch 3, batch 45] loss: 3.70621, acc1: 0.12500, acc5: 0.37500
[Epoch 3, batch 50] loss: 4.31900, acc1: 0.00000, acc5: 0.12500
[Epoch 3, batch 55] loss: 3.93389, acc1: 0.03125, acc5: 0.31250
[Epoch 3, batch 60] loss: 4.06188, acc1: 0.03125, acc5: 0.21875
[Epoch 3, batch 65] loss: 3.54749, acc1: 0.12500, acc5: 0.46875
[Epoch 3, batch 70] loss: 4.05245, acc1: 0.06250, acc5: 0.34375
[Epoch 3, batch 75] loss: 3.98707, acc1: 0.03125, acc5: 0.25000
[Epoch 3, batch 80] loss: 3.64197, acc1: 0.15625, acc5: 0.31250
[Epoch 3, batch 85] loss: 4.04804, acc1: 0.09375, acc5: 0.15625
[Epoch 3, batch 90] loss: 3.92871, acc1: 0.06250, acc5: 0.31250
[Epoch 3, batch 95] loss: 3.86059, acc1: 0.12500, acc5: 0.37500
[Epoch 3, batch 100] loss: 4.19599, acc1: 0.00000, acc5: 0.15625
[Epoch 3, batch 105] loss: 3.82548, acc1: 0.09375, acc5: 0.21875
[Epoch 3, batch 110] loss: 3.85049, acc1: 0.06250, acc5: 0.25000
[Epoch 3, batch 115] loss: 3.96305, acc1: 0.06250, acc5: 0.28125
[Epoch 3, batch 120] loss: 3.81370, acc1: 0.15625, acc5: 0.25000
[Epoch 3, batch 125] loss: 3.64326, acc1: 0.12500, acc5: 0.34375
[Epoch 3, batch 130] loss: 3.74849, acc1: 0.12500, acc5: 0.31250
[Epoch 3, batch 135] loss: 4.09653, acc1: 0.09375, acc5: 0.21875
[Epoch 3, batch 140] loss: 3.96367, acc1: 0.12500, acc5: 0.28125
[Epoch 3, batch 145] loss: 3.94604, acc1: 0.09375, acc5: 0.25000
[Epoch 3, batch 150] loss: 4.26343, acc1: 0.12500, acc5: 0.18750
[Epoch 3, batch 155] loss: 4.01278, acc1: 0.00000, acc5: 0.25000
[Epoch 3, batch 160] loss: 3.88456, acc1: 0.09375, acc5: 0.31250
[Epoch 3, batch 165] loss: 3.67864, acc1: 0.09375, acc5: 0.37500
[Epoch 3, batch 170] loss: 3.93802, acc1: 0.06250, acc5: 0.28125
[Epoch 3, batch 175] loss: 3.59711, acc1: 0.12500, acc5: 0.40625
[Epoch 3, batch 180] loss: 4.15744, acc1: 0.00000, acc5: 0.18750
[Epoch 3, batch 185] loss: 3.94027, acc1: 0.12500, acc5: 0.34375
[Epoch 3, batch 190] loss: 3.70105, acc1: 0.09375, acc5: 0.34375
[Epoch 4, batch 0] loss: 4.11341, acc1: 0.12500, acc5: 0.50000
[Epoch 4, batch 5] loss: 3.98679, acc1: 0.09375, acc5: 0.21875
[Epoch 4, batch 10] loss: 3.91498, acc1: 0.06250, acc5: 0.28125
[Epoch 4, batch 15] loss: 3.26400, acc1: 0.18750, acc5: 0.59375
[Epoch 4, batch 20] loss: 4.00175, acc1: 0.03125, acc5: 0.21875
[Epoch 4, batch 25] loss: 3.61085, acc1: 0.06250, acc5: 0.40625
[Epoch 4, batch 30] loss: 3.89901, acc1: 0.15625, acc5: 0.31250
[Epoch 4, batch 35] loss: 3.94514, acc1: 0.06250, acc5: 0.34375
[Epoch 4, batch 40] loss: 4.16314, acc1: 0.03125, acc5: 0.21875
[Epoch 4, batch 45] loss: 3.73665, acc1: 0.15625, acc5: 0.31250
[Epoch 4, batch 50] loss: 4.06615, acc1: 0.06250, acc5: 0.18750
[Epoch 4, batch 55] loss: 3.63847, acc1: 0.12500, acc5: 0.43750
[Epoch 4, batch 60] loss: 3.77581, acc1: 0.12500, acc5: 0.21875
[Epoch 4, batch 65] loss: 3.32817, acc1: 0.15625, acc5: 0.43750
[Epoch 4, batch 70] loss: 3.94625, acc1: 0.12500, acc5: 0.31250
[Epoch 4, batch 75] loss: 3.82815, acc1: 0.09375, acc5: 0.28125
[Epoch 4, batch 80] loss: 3.53803, acc1: 0.21875, acc5: 0.34375
[Epoch 4, batch 85] loss: 3.97315, acc1: 0.15625, acc5: 0.18750
[Epoch 4, batch 90] loss: 3.70651, acc1: 0.00000, acc5: 0.37500
[Epoch 4, batch 95] loss: 3.71071, acc1: 0.09375, acc5: 0.34375
[Epoch 4, batch 100] loss: 4.12406, acc1: 0.03125, acc5: 0.25000
[Epoch 4, batch 105] loss: 3.70512, acc1: 0.18750, acc5: 0.40625
[Epoch 4, batch 110] loss: 3.69401, acc1: 0.12500, acc5: 0.34375
[Epoch 4, batch 115] loss: 3.85977, acc1: 0.06250, acc5: 0.40625
[Epoch 4, batch 120] loss: 3.70440, acc1: 0.12500, acc5: 0.34375
[Epoch 4, batch 125] loss: 3.58578, acc1: 0.12500, acc5: 0.40625
[Epoch 4, batch 130] loss: 3.77767, acc1: 0.15625, acc5: 0.34375
[Epoch 4, batch 135] loss: 3.97933, acc1: 0.12500, acc5: 0.34375
[Epoch 4, batch 140] loss: 3.86474, acc1: 0.12500, acc5: 0.28125
[Epoch 4, batch 145] loss: 3.82657, acc1: 0.03125, acc5: 0.28125
[Epoch 4, batch 150] loss: 4.02342, acc1: 0.12500, acc5: 0.25000
[Epoch 4, batch 155] loss: 4.10727, acc1: 0.03125, acc5: 0.18750
[Epoch 4, batch 160] loss: 3.70518, acc1: 0.09375, acc5: 0.40625
[Epoch 4, batch 165] loss: 3.57295, acc1: 0.18750, acc5: 0.31250
[Epoch 4, batch 170] loss: 3.69658, acc1: 0.06250, acc5: 0.37500
[Epoch 4, batch 175] loss: 3.44148, acc1: 0.12500, acc5: 0.46875
[Epoch 4, batch 180] loss: 4.06103, acc1: 0.09375, acc5: 0.21875
[Epoch 4, batch 185] loss: 3.71030, acc1: 0.18750, acc5: 0.40625
[Epoch 4, batch 190] loss: 3.62899, acc1: 0.12500, acc5: 0.25000
[Epoch 5, batch 0] loss: 3.81545, acc1: 0.18750, acc5: 0.50000
[Epoch 5, batch 5] loss: 4.02438, acc1: 0.06250, acc5: 0.28125
[Epoch 5, batch 10] loss: 3.62845, acc1: 0.12500, acc5: 0.31250
[Epoch 5, batch 15] loss: 3.27564, acc1: 0.15625, acc5: 0.53125
[Epoch 5, batch 20] loss: 3.59382, acc1: 0.12500, acc5: 0.31250
[Epoch 5, batch 25] loss: 3.26437, acc1: 0.09375, acc5: 0.50000
[Epoch 5, batch 30] loss: 3.51603, acc1: 0.21875, acc5: 0.43750
[Epoch 5, batch 35] loss: 3.83675, acc1: 0.03125, acc5: 0.31250
[Epoch 5, batch 40] loss: 3.85600, acc1: 0.09375, acc5: 0.21875
[Epoch 5, batch 45] loss: 3.43083, acc1: 0.12500, acc5: 0.40625
[Epoch 5, batch 50] loss: 3.75426, acc1: 0.06250, acc5: 0.21875
[Epoch 5, batch 55] loss: 3.64750, acc1: 0.09375, acc5: 0.37500
[Epoch 5, batch 60] loss: 3.63219, acc1: 0.15625, acc5: 0.31250
[Epoch 5, batch 65] loss: 3.25543, acc1: 0.28125, acc5: 0.50000
[Epoch 5, batch 70] loss: 3.86795, acc1: 0.12500, acc5: 0.21875
[Epoch 5, batch 75] loss: 3.57720, acc1: 0.15625, acc5: 0.37500
[Epoch 5, batch 80] loss: 3.36200, acc1: 0.21875, acc5: 0.46875
[Epoch 5, batch 85] loss: 3.84593, acc1: 0.15625, acc5: 0.25000
[Epoch 5, batch 90] loss: 3.49435, acc1: 0.06250, acc5: 0.46875
[Epoch 5, batch 95] loss: 3.55899, acc1: 0.12500, acc5: 0.31250
[Epoch 5, batch 100] loss: 4.18699, acc1: 0.03125, acc5: 0.21875
[Epoch 5, batch 105] loss: 3.60212, acc1: 0.09375, acc5: 0.40625
[Epoch 5, batch 110] loss: 3.54642, acc1: 0.12500, acc5: 0.31250
[Epoch 5, batch 115] loss: 3.74042, acc1: 0.09375, acc5: 0.31250
[Epoch 5, batch 120] loss: 3.47774, acc1: 0.21875, acc5: 0.34375
[Epoch 5, batch 125] loss: 3.36380, acc1: 0.18750, acc5: 0.50000
[Epoch 5, batch 130] loss: 3.57783, acc1: 0.12500, acc5: 0.46875
[Epoch 5, batch 135] loss: 3.72177, acc1: 0.15625, acc5: 0.34375
[Epoch 5, batch 140] loss: 3.63624, acc1: 0.15625, acc5: 0.31250
[Epoch 5, batch 145] loss: 3.74926, acc1: 0.09375, acc5: 0.28125
[Epoch 5, batch 150] loss: 3.91530, acc1: 0.09375, acc5: 0.28125
[Epoch 5, batch 155] loss: 3.94166, acc1: 0.06250, acc5: 0.25000
[Epoch 5, batch 160] loss: 3.55784, acc1: 0.18750, acc5: 0.37500
[Epoch 5, batch 165] loss: 3.57284, acc1: 0.12500, acc5: 0.37500
[Epoch 5, batch 170] loss: 3.62667, acc1: 0.06250, acc5: 0.31250
[Epoch 5, batch 175] loss: 3.35791, acc1: 0.18750, acc5: 0.46875
[Epoch 5, batch 180] loss: 4.11646, acc1: 0.09375, acc5: 0.28125
[Epoch 5, batch 185] loss: 3.58550, acc1: 0.21875, acc5: 0.46875
[Epoch 5, batch 190] loss: 3.37024, acc1: 0.12500, acc5: 0.40625
[Epoch 6, batch 0] loss: 3.86143, acc1: 0.15625, acc5: 0.40625
[Epoch 6, batch 5] loss: 3.81790, acc1: 0.09375, acc5: 0.31250
[Epoch 6, batch 10] loss: 3.46124, acc1: 0.18750, acc5: 0.31250
[Epoch 6, batch 15] loss: 3.19393, acc1: 0.15625, acc5: 0.53125
[Epoch 6, batch 20] loss: 3.50120, acc1: 0.12500, acc5: 0.34375
[Epoch 6, batch 25] loss: 3.26597, acc1: 0.09375, acc5: 0.53125
[Epoch 6, batch 30] loss: 3.53929, acc1: 0.15625, acc5: 0.50000
[Epoch 6, batch 35] loss: 3.74867, acc1: 0.12500, acc5: 0.31250
[Epoch 6, batch 40] loss: 3.74968, acc1: 0.06250, acc5: 0.28125
[Epoch 6, batch 45] loss: 3.41658, acc1: 0.15625, acc5: 0.46875
[Epoch 6, batch 50] loss: 3.80311, acc1: 0.06250, acc5: 0.21875
[Epoch 6, batch 55] loss: 3.39404, acc1: 0.21875, acc5: 0.50000
[Epoch 6, batch 60] loss: 3.47394, acc1: 0.21875, acc5: 0.34375
[Epoch 6, batch 65] loss: 3.26245, acc1: 0.25000, acc5: 0.43750
[Epoch 6, batch 70] loss: 3.82297, acc1: 0.12500, acc5: 0.21875
[Epoch 6, batch 75] loss: 3.42105, acc1: 0.21875, acc5: 0.43750
[Epoch 6, batch 80] loss: 3.24987, acc1: 0.21875, acc5: 0.37500
[Epoch 6, batch 85] loss: 3.57264, acc1: 0.18750, acc5: 0.37500
[Epoch 6, batch 90] loss: 3.48414, acc1: 0.06250, acc5: 0.34375
[Epoch 6, batch 95] loss: 3.39079, acc1: 0.18750, acc5: 0.43750
[Epoch 6, batch 100] loss: 4.00689, acc1: 0.09375, acc5: 0.25000
[Epoch 6, batch 105] loss: 3.42164, acc1: 0.12500, acc5: 0.43750
[Epoch 6, batch 110] loss: 3.46689, acc1: 0.15625, acc5: 0.34375
[Epoch 6, batch 115] loss: 3.51795, acc1: 0.09375, acc5: 0.40625
[Epoch 6, batch 120] loss: 3.39749, acc1: 0.18750, acc5: 0.37500
[Epoch 6, batch 125] loss: 3.28321, acc1: 0.21875, acc5: 0.43750
[Epoch 6, batch 130] loss: 3.37509, acc1: 0.12500, acc5: 0.56250
[Epoch 6, batch 135] loss: 3.58746, acc1: 0.12500, acc5: 0.37500
[Epoch 6, batch 140] loss: 3.71168, acc1: 0.09375, acc5: 0.21875
[Epoch 6, batch 145] loss: 3.56435, acc1: 0.15625, acc5: 0.34375
[Epoch 6, batch 150] loss: 3.86552, acc1: 0.06250, acc5: 0.28125
[Epoch 6, batch 155] loss: 3.87311, acc1: 0.06250, acc5: 0.25000
[Epoch 6, batch 160] loss: 3.47387, acc1: 0.18750, acc5: 0.46875
[Epoch 6, batch 165] loss: 3.53445, acc1: 0.15625, acc5: 0.37500
[Epoch 6, batch 170] loss: 3.56780, acc1: 0.06250, acc5: 0.37500
[Epoch 6, batch 175] loss: 3.13714, acc1: 0.25000, acc5: 0.53125
[Epoch 6, batch 180] loss: 3.86503, acc1: 0.09375, acc5: 0.28125
[Epoch 6, batch 185] loss: 3.72377, acc1: 0.12500, acc5: 0.40625
[Epoch 6, batch 190] loss: 3.21319, acc1: 0.15625, acc5: 0.43750
[Epoch 7, batch 0] loss: 3.57921, acc1: 0.18750, acc5: 0.53125
[Epoch 7, batch 5] loss: 3.81378, acc1: 0.09375, acc5: 0.28125
[Epoch 7, batch 10] loss: 3.09289, acc1: 0.18750, acc5: 0.50000
[Epoch 7, batch 15] loss: 2.86814, acc1: 0.28125, acc5: 0.75000
[Epoch 7, batch 20] loss: 3.24828, acc1: 0.09375, acc5: 0.50000
[Epoch 7, batch 25] loss: 2.96195, acc1: 0.15625, acc5: 0.53125
[Epoch 7, batch 30] loss: 3.26799, acc1: 0.15625, acc5: 0.62500
[Epoch 7, batch 35] loss: 3.59533, acc1: 0.09375, acc5: 0.46875
[Epoch 7, batch 40] loss: 3.66309, acc1: 0.12500, acc5: 0.37500
[Epoch 7, batch 45] loss: 3.29556, acc1: 0.18750, acc5: 0.46875
[Epoch 7, batch 50] loss: 3.54030, acc1: 0.12500, acc5: 0.21875
[Epoch 7, batch 55] loss: 3.25792, acc1: 0.18750, acc5: 0.53125
[Epoch 7, batch 60] loss: 3.29243, acc1: 0.18750, acc5: 0.37500
[Epoch 7, batch 65] loss: 3.12371, acc1: 0.21875, acc5: 0.56250
[Epoch 7, batch 70] loss: 3.72556, acc1: 0.15625, acc5: 0.31250
[Epoch 7, batch 75] loss: 3.49676, acc1: 0.15625, acc5: 0.40625
[Epoch 7, batch 80] loss: 3.17648, acc1: 0.28125, acc5: 0.43750
[Epoch 7, batch 85] loss: 3.55812, acc1: 0.15625, acc5: 0.34375
[Epoch 7, batch 90] loss: 3.44925, acc1: 0.06250, acc5: 0.37500
[Epoch 7, batch 95] loss: 3.27488, acc1: 0.15625, acc5: 0.53125
[Epoch 7, batch 100] loss: 3.91463, acc1: 0.06250, acc5: 0.31250
[Epoch 7, batch 105] loss: 3.38599, acc1: 0.12500, acc5: 0.40625
[Epoch 7, batch 110] loss: 3.31844, acc1: 0.25000, acc5: 0.37500
[Epoch 7, batch 115] loss: 3.49604, acc1: 0.09375, acc5: 0.34375
[Epoch 7, batch 120] loss: 3.31751, acc1: 0.18750, acc5: 0.43750
[Epoch 7, batch 125] loss: 3.27732, acc1: 0.18750, acc5: 0.40625
[Epoch 7, batch 130] loss: 3.16320, acc1: 0.21875, acc5: 0.53125
[Epoch 7, batch 135] loss: 3.48664, acc1: 0.21875, acc5: 0.40625
[Epoch 7, batch 140] loss: 3.45976, acc1: 0.09375, acc5: 0.37500
[Epoch 7, batch 145] loss: 3.45887, acc1: 0.09375, acc5: 0.43750
[Epoch 7, batch 150] loss: 3.85909, acc1: 0.09375, acc5: 0.21875
[Epoch 7, batch 155] loss: 3.61089, acc1: 0.12500, acc5: 0.34375
[Epoch 7, batch 160] loss: 3.45766, acc1: 0.25000, acc5: 0.46875
[Epoch 7, batch 165] loss: 3.35910, acc1: 0.18750, acc5: 0.50000
[Epoch 7, batch 170] loss: 3.54404, acc1: 0.09375, acc5: 0.40625
[Epoch 7, batch 175] loss: 2.91841, acc1: 0.25000, acc5: 0.50000
[Epoch 7, batch 180] loss: 3.75734, acc1: 0.12500, acc5: 0.31250
[Epoch 7, batch 185] loss: 3.76457, acc1: 0.12500, acc5: 0.50000
[Epoch 7, batch 190] loss: 3.03732, acc1: 0.18750, acc5: 0.53125
[Epoch 8, batch 0] loss: 3.40360, acc1: 0.18750, acc5: 0.53125
[Epoch 8, batch 5] loss: 3.63691, acc1: 0.15625, acc5: 0.28125
[Epoch 8, batch 10] loss: 3.17956, acc1: 0.18750, acc5: 0.43750
[Epoch 8, batch 15] loss: 2.91771, acc1: 0.18750, acc5: 0.65625
[Epoch 8, batch 20] loss: 3.25354, acc1: 0.09375, acc5: 0.46875
[Epoch 8, batch 25] loss: 2.91153, acc1: 0.21875, acc5: 0.59375
[Epoch 8, batch 30] loss: 3.28638, acc1: 0.18750, acc5: 0.50000
[Epoch 8, batch 35] loss: 3.41586, acc1: 0.06250, acc5: 0.43750
[Epoch 8, batch 40] loss: 3.63137, acc1: 0.09375, acc5: 0.34375
[Epoch 8, batch 45] loss: 3.03620, acc1: 0.31250, acc5: 0.59375
[Epoch 8, batch 50] loss: 3.64056, acc1: 0.09375, acc5: 0.28125
[Epoch 8, batch 55] loss: 3.44027, acc1: 0.15625, acc5: 0.46875
[Epoch 8, batch 60] loss: 3.24803, acc1: 0.21875, acc5: 0.34375
[Epoch 8, batch 65] loss: 3.07622, acc1: 0.25000, acc5: 0.62500
[Epoch 8, batch 70] loss: 3.27451, acc1: 0.21875, acc5: 0.46875
[Epoch 8, batch 75] loss: 3.22287, acc1: 0.15625, acc5: 0.50000
[Epoch 8, batch 80] loss: 3.06045, acc1: 0.25000, acc5: 0.50000
[Epoch 8, batch 85] loss: 3.51059, acc1: 0.15625, acc5: 0.37500
[Epoch 8, batch 90] loss: 3.21193, acc1: 0.21875, acc5: 0.40625
[Epoch 8, batch 95] loss: 3.26803, acc1: 0.25000, acc5: 0.43750
[Epoch 8, batch 100] loss: 3.84948, acc1: 0.03125, acc5: 0.40625
[Epoch 8, batch 105] loss: 3.17522, acc1: 0.12500, acc5: 0.43750
[Epoch 8, batch 110] loss: 3.25354, acc1: 0.21875, acc5: 0.34375
[Epoch 8, batch 115] loss: 3.33553, acc1: 0.12500, acc5: 0.43750
[Epoch 8, batch 120] loss: 3.28360, acc1: 0.21875, acc5: 0.50000
[Epoch 8, batch 125] loss: 3.11665, acc1: 0.25000, acc5: 0.53125
[Epoch 8, batch 130] loss: 3.01241, acc1: 0.28125, acc5: 0.62500
[Epoch 8, batch 135] loss: 3.23711, acc1: 0.21875, acc5: 0.50000
[Epoch 8, batch 140] loss: 3.26646, acc1: 0.18750, acc5: 0.40625
[Epoch 8, batch 145] loss: 3.36501, acc1: 0.15625, acc5: 0.50000
[Epoch 8, batch 150] loss: 3.76919, acc1: 0.12500, acc5: 0.25000
[Epoch 8, batch 155] loss: 3.58249, acc1: 0.12500, acc5: 0.37500
[Epoch 8, batch 160] loss: 3.25856, acc1: 0.21875, acc5: 0.53125
[Epoch 8, batch 165] loss: 3.32835, acc1: 0.15625, acc5: 0.46875
[Epoch 8, batch 170] loss: 3.48340, acc1: 0.12500, acc5: 0.50000
[Epoch 8, batch 175] loss: 2.95057, acc1: 0.28125, acc5: 0.50000
[Epoch 8, batch 180] loss: 3.64233, acc1: 0.12500, acc5: 0.37500
[Epoch 8, batch 185] loss: 3.58522, acc1: 0.18750, acc5: 0.50000
[Epoch 8, batch 190] loss: 2.94378, acc1: 0.31250, acc5: 0.53125
[Epoch 9, batch 0] loss: 3.38721, acc1: 0.25000, acc5: 0.46875
[Epoch 9, batch 5] loss: 3.49517, acc1: 0.15625, acc5: 0.31250
[Epoch 9, batch 10] loss: 3.06260, acc1: 0.21875, acc5: 0.43750
[Epoch 9, batch 15] loss: 3.02767, acc1: 0.18750, acc5: 0.56250
[Epoch 9, batch 20] loss: 3.09735, acc1: 0.21875, acc5: 0.62500
[Epoch 9, batch 25] loss: 2.95971, acc1: 0.18750, acc5: 0.65625
[Epoch 9, batch 30] loss: 3.20882, acc1: 0.25000, acc5: 0.53125
[Epoch 9, batch 35] loss: 3.22175, acc1: 0.18750, acc5: 0.50000
[Epoch 9, batch 40] loss: 3.41257, acc1: 0.15625, acc5: 0.46875
[Epoch 9, batch 45] loss: 3.25821, acc1: 0.28125, acc5: 0.43750
[Epoch 9, batch 50] loss: 3.42013, acc1: 0.12500, acc5: 0.37500
[Epoch 9, batch 55] loss: 3.29614, acc1: 0.18750, acc5: 0.43750
[Epoch 9, batch 60] loss: 3.11410, acc1: 0.21875, acc5: 0.37500
[Epoch 9, batch 65] loss: 2.85409, acc1: 0.25000, acc5: 0.56250
[Epoch 9, batch 70] loss: 3.21761, acc1: 0.18750, acc5: 0.46875
[Epoch 9, batch 75] loss: 3.00348, acc1: 0.28125, acc5: 0.56250
[Epoch 9, batch 80] loss: 3.01624, acc1: 0.25000, acc5: 0.50000
[Epoch 9, batch 85] loss: 3.13987, acc1: 0.21875, acc5: 0.56250
[Epoch 9, batch 90] loss: 2.93289, acc1: 0.15625, acc5: 0.59375
[Epoch 9, batch 95] loss: 3.13672, acc1: 0.21875, acc5: 0.50000
[Epoch 9, batch 100] loss: 3.75731, acc1: 0.15625, acc5: 0.34375
[Epoch 9, batch 105] loss: 3.03563, acc1: 0.25000, acc5: 0.43750
[Epoch 9, batch 110] loss: 3.22449, acc1: 0.18750, acc5: 0.40625
[Epoch 9, batch 115] loss: 3.29492, acc1: 0.18750, acc5: 0.40625
[Epoch 9, batch 120] loss: 3.04845, acc1: 0.21875, acc5: 0.50000
[Epoch 9, batch 125] loss: 3.03569, acc1: 0.21875, acc5: 0.53125
[Epoch 9, batch 130] loss: 2.65898, acc1: 0.28125, acc5: 0.71875
[Epoch 9, batch 135] loss: 3.23022, acc1: 0.21875, acc5: 0.37500
[Epoch 9, batch 140] loss: 3.14417, acc1: 0.28125, acc5: 0.46875
[Epoch 9, batch 145] loss: 3.26196, acc1: 0.25000, acc5: 0.50000
[Epoch 9, batch 150] loss: 3.62909, acc1: 0.09375, acc5: 0.31250
[Epoch 9, batch 155] loss: 3.52147, acc1: 0.09375, acc5: 0.43750
[Epoch 9, batch 160] loss: 3.23512, acc1: 0.28125, acc5: 0.50000
[Epoch 9, batch 165] loss: 3.07425, acc1: 0.15625, acc5: 0.46875
[Epoch 9, batch 170] loss: 3.24628, acc1: 0.09375, acc5: 0.43750
[Epoch 9, batch 175] loss: 2.92281, acc1: 0.28125, acc5: 0.59375
[Epoch 9, batch 180] loss: 3.73371, acc1: 0.15625, acc5: 0.37500
[Epoch 9, batch 185] loss: 3.62437, acc1: 0.15625, acc5: 0.50000
[Epoch 9, batch 190] loss: 2.60009, acc1: 0.34375, acc5: 0.65625
Start collective training example:
Run on CUDAPlace(1).
train_fleet_static.py:50: DeprecationWarning: [93m
Warning:
API "paddle.dataset.flowers.train" is deprecated since 2.0.0, and will be removed in future versions. Please use "paddle.vision.datasets.Flowers" instead.
reason: Please use new dataset API which supports paddle.io.DataLoader [0m
  reader_decorator(paddle.dataset.flowers.train(use_xmap=True)),
Execute startup program.
W0909 13:57:16.308640 15545 device_context.cc:446] Please NOTE: device: 1, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 10.1
W0909 13:57:16.315934 15545 device_context.cc:464] device: 1, cuDNN Version: 7.6.
I0909 13:57:19.186403 15545 gen_comm_id_helper.cc:190] Server listening on: 127.0.0.1:34981 successful.
Start training:
W0909 13:57:25.818969 15545 fuse_all_reduce_op_pass.cc:76] Find all_reduce operators: 161. To make the speed faster, some all_reduce ops are fused during training, after fusion, the number of all_reduce ops is 5.
[Epoch 0, batch 0] loss: 4.97343, acc1: 0.00000, acc5: 0.00000
[Epoch 0, batch 5] loss: 15.10670, acc1: 0.03125, acc5: 0.06250
[Epoch 0, batch 10] loss: 8.27726, acc1: 0.09375, acc5: 0.12500
[Epoch 0, batch 15] loss: 8.02874, acc1: 0.00000, acc5: 0.06250
[Epoch 0, batch 20] loss: 4.80902, acc1: 0.00000, acc5: 0.00000
[Epoch 0, batch 25] loss: 4.97025, acc1: 0.03125, acc5: 0.06250
[Epoch 0, batch 30] loss: 4.69705, acc1: 0.00000, acc5: 0.00000
[Epoch 0, batch 35] loss: 4.83755, acc1: 0.03125, acc5: 0.18750
[Epoch 0, batch 40] loss: 4.66172, acc1: 0.00000, acc5: 0.06250
[Epoch 0, batch 45] loss: 4.91311, acc1: 0.12500, acc5: 0.25000
[Epoch 0, batch 50] loss: 9.03236, acc1: 0.00000, acc5: 0.00000
[Epoch 0, batch 55] loss: 4.77837, acc1: 0.03125, acc5: 0.09375
[Epoch 0, batch 60] loss: 4.61636, acc1: 0.06250, acc5: 0.09375
[Epoch 0, batch 65] loss: 4.58164, acc1: 0.03125, acc5: 0.06250
[Epoch 0, batch 70] loss: 4.51485, acc1: 0.12500, acc5: 0.21875
[Epoch 0, batch 75] loss: 4.57762, acc1: 0.00000, acc5: 0.12500
[Epoch 0, batch 80] loss: 4.66687, acc1: 0.03125, acc5: 0.09375
[Epoch 0, batch 85] loss: 4.54976, acc1: 0.00000, acc5: 0.12500
[Epoch 0, batch 90] loss: 4.44964, acc1: 0.06250, acc5: 0.18750
[Epoch 0, batch 95] loss: 4.49174, acc1: 0.03125, acc5: 0.12500
[Epoch 0, batch 100] loss: 4.64591, acc1: 0.00000, acc5: 0.06250
[Epoch 0, batch 105] loss: 4.42663, acc1: 0.03125, acc5: 0.18750
[Epoch 0, batch 110] loss: 4.49755, acc1: 0.00000, acc5: 0.12500
[Epoch 0, batch 115] loss: 4.83419, acc1: 0.03125, acc5: 0.12500
[Epoch 0, batch 120] loss: 4.41562, acc1: 0.00000, acc5: 0.15625


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::framework::ParallelExecutor::Run(std::vector<std::string, std::allocator<std::string > > const&, bool)
1   paddle::framework::details::ScopeBufferedSSAGraphExecutor::Run(std::vector<std::string, std::allocator<std::string > > const&, bool)
2   paddle::framework::details::ScopeBufferedMonitor::Apply(std::function<void ()> const&, bool)
3   paddle::framework::details::FastThreadedSSAGraphExecutor::Run(std::vector<std::string, std::allocator<std::string > > const&, bool)

----------------------
Error Message Summary:
----------------------
FatalError: `Termination signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1631167061 (unix time) try "date -d @1631167061" if you are using GNU date ***]
  [SignalInfo: *** SIGTERM (@0x3c19) received by PID 15545 (TID 0x7f17627f8700) from PID 15385 ***]

