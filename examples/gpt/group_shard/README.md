# GPT åˆ†ç»„åˆ‡åˆ†å¹¶è¡Œæ¨¡å‹è®­ç»ƒ

å½“æ¨¡å‹å‚æ•°è¾¾åˆ°ç™¾äº¿æˆ–è€…åƒäº¿æ—¶ï¼Œ ä¼ ç»Ÿçš„æ•°æ®å¹¶è¡Œè®­ç»ƒå¯èƒ½ä¼šé‡åˆ°æ˜¾å­˜ç“¶é¢ˆã€‚ åœ¨æ•°æ®å¹¶è¡Œè®­ç»ƒä¸­ï¼Œæ¯ä¸ªgpu worker éƒ½æœ‰ä¸€ä»½å®Œæ•´æ¨¡å‹å‚æ•°å’Œä¼˜åŒ–å™¨çŠ¶æ€å‰¯æœ¬ã€‚ ã€Š[ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://arxiv.org/abs/1910.02054)ã€‹æŒ‡å‡ºåœ¨æ¯ä¸ªGPU ä¸Šéƒ½ä¿å­˜ä¸€ä»½æ¨¡å‹å‚æ•°å’Œä¼˜åŒ–å™¨çŠ¶æ€å‰¯æœ¬æ˜¯å†—ä½™çš„ã€‚ æˆ‘ä»¬å¯ä»¥é€šè¿‡å°†ä¸Šè¿°å‚æ•°å’Œå‰¯æœ¬åˆ’åˆ†åˆ°ä¸åŒGPU ä¸­ï¼Œ åœ¨æ¯ä¸ªGPU åªä¿å­˜éƒ¨åˆ†å‰¯æœ¬ï¼Œæ¥å‡å°‘æ¯å¼ GPUä¸Šæ˜¾å­˜çš„å ç”¨ï¼Œä»è€Œå¯ä»¥æ”¯æŒæ›´å¤§æ¨¡å‹çš„è®­ç»ƒã€‚å…·ä½“ç­–ç•¥ä»¥åŠç›¸å…³FleetAPIä»‹ç»å¯ä»¥å‚è€ƒä»¥ä¸‹æ•™ç¨‹ï¼š

- [åˆ†ç»„åˆ‡åˆ†å¹¶è¡Œ](https://www.paddlepaddle.org.cn/documentation/docs/zh/develop/guides/06_distributed_training/group_sharded_parallel_cn.html)


## 1.åˆ†ç»„åˆ‡åˆ†å¹¶è¡Œ
å½“å‰GPTæ¨¡å‹å·²é€‚é…åˆ†ç»„åˆ‡åˆ†å¹¶è¡Œï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡é…ç½®æ–‡ä»¶é€‰æ‹©å¹¶è¡Œç»´åº¦å’Œåˆ‡åˆ†ç­–ç•¥ã€‚

```yaml
  Distributed:
    sharding:
      sharding_degree: 8
      sharding_stage: 2
      sharding_offload: False
```

å…¶ä¸­å‚æ•°å«ä¹‰ï¼š
- `sharding_degree` åˆ†ç»„åˆ‡åˆ†å¹¶è¡Œç»´åº¦
- `sharding_stage` åˆ‡åˆ†ç­–ç•¥ã€‚`2`è¡¨ç¤ºåˆ‡åˆ†æ¢¯åº¦å’Œä¼˜åŒ–å™¨çŠ¶æ€ï¼Œ`3`è¡¨ç¤ºåœ¨ä¸Šè¿°ç­–ç•¥åŸºç¡€ä¸Šå†åˆ‡åˆ†å‰å‘å‚æ•°
- `sharding_offload` CPU offloadç­–ç•¥

## 2.è¿è¡Œæ–¹å¼


ä»¥å•æœº8å¡ä¸ºä¾‹ï¼Œé€šè¿‡``paddle.distributed.launch``å¯åŠ¨å¤šè¿›ç¨‹è®­ç»ƒã€‚

```shell
log_dir=sharding8
python -m paddle.distributed.launch --log_dir $log_dir --devices "0,1,2,3,4,5,6,7" run_pretrain.py \
    -c ./configs.yaml
```


æ‰§è¡Œæ—¥å¿—ï¼š

```
[32m[2022-07-27 08:53:11,036] [    INFO][0m - global step 1, epoch: 0, batch: 0, loss: 11.642805099, avg_reader_cost: 0.23430 sec, avg_batch_cost: 11.67036 sec, speed: 0.09 step/s, ips_total: 175 tokens/s, ips: 22 tokens/s, learning rate: 6.25000e-09[0m
[32m[2022-07-27 08:53:13,144] [    INFO][0m - global step 2, epoch: 0, batch: 1, loss: 11.620714188, avg_reader_cost: 0.00040 sec, avg_batch_cost: 2.10704 sec, speed: 0.47 step/s, ips_total: 972 tokens/s, ips: 121 tokens/s, learning rate: 9.37500e-09[0m
[32m[2022-07-27 08:53:13,540] [    INFO][0m - global step 3, epoch: 0, batch: 2, loss: 11.711474419, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.39656 sec, speed: 2.52 step/s, ips_total: 5164 tokens/s, ips: 646 tokens/s, learning rate: 1.25000e-08[0m
[32m[2022-07-27 08:53:13,836] [    INFO][0m - global step 4, epoch: 0, batch: 3, loss: 11.773808479, avg_reader_cost: 0.00019 sec, avg_batch_cost: 0.29522 sec, speed: 3.39 step/s, ips_total: 6937 tokens/s, ips: 867 tokens/s, learning rate: 1.56250e-08[0m
[32m[2022-07-27 08:53:14,150] [    INFO][0m - global step 5, epoch: 0, batch: 4, loss: 11.698161125, avg_reader_cost: 0.00029 sec, avg_batch_cost: 0.31358 sec, speed: 3.19 step/s, ips_total: 6531 tokens/s, ips: 816 tokens/s, learning rate: 1.87500e-08[0m
[32m[2022-07-27 08:53:14,433] [    INFO][0m - global step 6, epoch: 0, batch: 5, loss: 11.689817429, avg_reader_cost: 0.00034 sec, avg_batch_cost: 0.28225 sec, speed: 3.54 step/s, ips_total: 7256 tokens/s, ips: 907 tokens/s, learning rate: 2.18750e-08[0m
[32m[2022-07-27 08:53:14,734] [    INFO][0m - global step 7, epoch: 0, batch: 6, loss: 11.665119171, avg_reader_cost: 0.00023 sec, avg_batch_cost: 0.29825 sec, speed: 3.35 step/s, ips_total: 6867 tokens/s, ips: 858 tokens/s, learning rate: 2.50000e-08[0m
[32m[2022-07-27 08:53:15,015] [    INFO][0m - global step 8, epoch: 0, batch: 7, loss: 11.673336983, avg_reader_cost: 0.00024 sec, avg_batch_cost: 0.28085 sec, speed: 3.56 step/s, ips_total: 7292 tokens/s, ips: 912 tokens/s, learning rate: 2.81250e-08[0m
[32m[2022-07-27 08:53:15,295] [    INFO][0m - global step 9, epoch: 0, batch: 8, loss: 11.724355698, avg_reader_cost: 0.00022 sec, avg_batch_cost: 0.27952 sec, speed: 3.58 step/s, ips_total: 7327 tokens/s, ips: 916 tokens/s, learning rate: 3.12500e-08[0m
[32m[2022-07-27 08:53:15,577] [    INFO][0m - global step 10, epoch: 0, batch: 9, loss: 11.674280167, avg_reader_cost: 0.00021 sec, avg_batch_cost: 0.28149 sec, speed: 3.55 step/s, ips_total: 7276 tokens/s, ips: 909 tokens/s, learning rate: 3.43750e-08[0m

```
