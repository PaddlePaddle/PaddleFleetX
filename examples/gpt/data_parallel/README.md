# GPT æ•°æ®å¹¶è¡Œæ¨¡å‹è®­ç»ƒ

æ•°æ®å¹¶è¡Œæ˜¯å¤§è§„æ¨¡æ·±åº¦å­¦ä¹ è®­ç»ƒä¸­éå¸¸æˆç†Ÿå’Œå¸¸ç”¨çš„å¹¶è¡Œæ¨¡å¼ã€‚åœ¨æ•°æ®å¹¶è¡Œæ¨¡å‹è®­ç»ƒä¸­ï¼Œè®­ç»ƒä»»åŠ¡è¢«åˆ‡åˆ†åˆ°å¤šä¸ªè¿›ç¨‹(è®¾å¤‡)ä¸Š,æ¯ä¸ªè¿›ç¨‹ç»´æŠ¤ç›¸åŒçš„æ¨¡å‹å‚æ•°å’Œç›¸åŒçš„è®¡ç®—ä»»åŠ¡ï¼Œä½†æ˜¯å¤„ç†ä¸åŒçš„æ•°æ®(batch data)ï¼›é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒåŒä¸€å…¨å±€æ•°æ®(global batch)ä¸‹çš„æ•°æ®å’Œè®¡ç®—è¢«åˆ‡åˆ†åˆ°äº†ä¸åŒçš„è¿›ç¨‹ï¼Œä»è€Œå‡è½»äº†å•ä¸ªè®¾å¤‡ä¸Šçš„è®¡ç®—å’Œå­˜å‚¨å‹åŠ›ã€‚å…·ä½“ç­–ç•¥ä»¥åŠç›¸å…³FleetAPIä»‹ç»å¯ä»¥å‚è€ƒä»¥ä¸‹æ•™ç¨‹ï¼š

- [æ•°æ®å¹¶è¡Œ](https://www.paddlepaddle.org.cn/documentation/docs/zh/develop/guides/06_distributed_training/data_parallel/index_cn.html)


## 1.æ•°æ®å¹¶è¡Œ
å½“å‰GPTæ¨¡å‹å·²é€‚é…æ•°æ®å¹¶è¡Œï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡é…ç½®æ–‡ä»¶é€‰æ‹©å¹¶è¡Œçš„ç»´åº¦ã€‚

```yaml
  Distributed:
    dp_degree: 8
```

å…¶ä¸­å‚æ•°å«ä¹‰ï¼š
- `dp_degree` æ•°æ®å¹¶è¡Œç»´åº¦

## 2.è¿è¡Œæ–¹å¼


ä»¥å•æœº8å¡ä¸ºä¾‹ï¼Œé€šè¿‡``paddle.distributed.launch``å¯åŠ¨å¤šè¿›ç¨‹è®­ç»ƒã€‚

```shell
log_dir=dp8
python -m paddle.distributed.launch --log_dir $log_dir --devices "0,1,2,3,4,5,6,7" run_pretrain.py \
    -c ./configs.yaml
```


æ‰§è¡Œæ—¥å¿—ï¼š

```
[32m[2022-07-27 13:17:17,469] [    INFO][0m - global step 1, epoch: 0, batch: 0, loss: 11.266701698, avg_reader_cost: 0.24114 sec, avg_batch_cost: 3.90742 sec, speed: 0.26 step/s, ips_total: 16772 tokens/s, ips: 16772 tokens/s, learning rate: 5.55556e-09[0m
[32m[2022-07-27 13:17:19,467] [    INFO][0m - global step 2, epoch: 0, batch: 1, loss: 11.274262428, avg_reader_cost: 0.00020 sec, avg_batch_cost: 1.99697 sec, speed: 0.50 step/s, ips_total: 32818 tokens/s, ips: 32818 tokens/s, learning rate: 8.33333e-09[0m
[32m[2022-07-27 13:17:21,073] [    INFO][0m - global step 3, epoch: 0, batch: 2, loss: 11.266974449, avg_reader_cost: 0.00029 sec, avg_batch_cost: 1.60637 sec, speed: 0.62 step/s, ips_total: 40798 tokens/s, ips: 40798 tokens/s, learning rate: 1.11111e-08[0m
[32m[2022-07-27 13:17:22,692] [    INFO][0m - global step 4, epoch: 0, batch: 3, loss: 11.261226654, avg_reader_cost: 0.00017 sec, avg_batch_cost: 1.61802 sec, speed: 0.62 step/s, ips_total: 40504 tokens/s, ips: 40504 tokens/s, learning rate: 1.38889e-08[0m
[32m[2022-07-27 13:17:24,303] [    INFO][0m - global step 5, epoch: 0, batch: 4, loss: 11.268389702, avg_reader_cost: 0.00016 sec, avg_batch_cost: 1.61117 sec, speed: 0.62 step/s, ips_total: 40676 tokens/s, ips: 40676 tokens/s, learning rate: 1.66667e-08[0m
[32m[2022-07-27 13:17:25,915] [    INFO][0m - global step 6, epoch: 0, batch: 5, loss: 11.278966904, avg_reader_cost: 0.00016 sec, avg_batch_cost: 1.61185 sec, speed: 0.62 step/s, ips_total: 40659 tokens/s, ips: 40659 tokens/s, learning rate: 1.94444e-08[0m
[32m[2022-07-27 13:17:27,526] [    INFO][0m - global step 7, epoch: 0, batch: 6, loss: 11.280961037, avg_reader_cost: 0.00030 sec, avg_batch_cost: 1.61001 sec, speed: 0.62 step/s, ips_total: 40705 tokens/s, ips: 40705 tokens/s, learning rate: 2.22222e-08[0m
[32m[2022-07-27 13:17:29,127] [    INFO][0m - global step 8, epoch: 0, batch: 7, loss: 11.269421577, avg_reader_cost: 0.00016 sec, avg_batch_cost: 1.60079 sec, speed: 0.62 step/s, ips_total: 40940 tokens/s, ips: 40940 tokens/s, learning rate: 2.50000e-08[0m
[32m[2022-07-27 13:17:30,730] [    INFO][0m - global step 9, epoch: 0, batch: 8, loss: 11.264699936, avg_reader_cost: 0.00016 sec, avg_batch_cost: 1.60254 sec, speed: 0.62 step/s, ips_total: 40895 tokens/s, ips: 40895 tokens/s, learning rate: 2.77778e-08[0m
[32m[2022-07-27 13:17:32,333] [    INFO][0m - global step 10, epoch: 0, batch: 9, loss: 11.262663841, avg_reader_cost: 0.00015 sec, avg_batch_cost: 1.60261 sec, speed: 0.62 step/s, ips_total: 40893 tokens/s, ips: 40893 tokens/s, learning rate: 3.05556e-08[0m

```
